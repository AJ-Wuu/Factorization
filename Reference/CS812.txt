
CS 812
Spring 2017
Course Log

Lecture 1
1/18/17 Wednesday

   Handouts: course blurb, topics list, reserve books

   Why study computations in number theory?

     Computer Algebra -- relies on exact arithmetic

          Example: adding fractions

     Pseudo-Random Number Generation

          Lehmer sequence x[n] = ( a *x[n-1] + b ) mod m
          What's the period?  How can it be maximized?
          Can x[n] be predicted from x[n-1],...,x[0]?

     Communication

        Error Correcting Codes

          BCH decoding: error positions obtained by finding roots
                        of a polynomial

        Cryptography

          Public key systems (1970s)
          Security tied to the difficulty of factoring, discrete log, etc.

     Complexity Theory

        Problems at all difficulty levels

          Hilbert's 10th problem (undecidable)

          Decision problem for factoring (NP n coNP), unknown if in P

          Multiplication of m-bit numbers - unknown if O(m) bit operations
                                            suffice

     Experimental Number Theory

        Example: Testing Goldbach's Conjecture
                 Relies on sieve for generating a list of primes
                 Current record: 4 * 10^14, by Olivera e Silva
                 & others, Math. Comp. 2014.

Lecture 2
1/20/17 Friday

   Complexity of "standard" arithmetic algorithms

   References: Bach/Shallit 3.1-3.2
               Knuth 4.3.1

   Goal: Algorithms with cost bounded by polynomial in input length

   Binary length function

         lg n = 1, if n=0
                floor(log_2 n) + 1, if n >= 1

   Cost = number of Boolean operations (bops).

   Consider positive numbers only.

   Addition

     Standard right-to-left method works in any base b >= 2.
     Cost to add x to y is O( lg x + lg y ) bops.

   Subtraction

      Can be reduced to addition.  If 0 <= x,y < 2^L, to subtract y
      you add 2^L - y = [bitwise complement of y] + 1.  Carry out from
      this addition tells you whether y was less than x or not.

      This was used in mechanical calculating machines -- see 
      Wikipedia article, "Method of Complements"
   
   Multiplication

      Use the identity 

              ( sum_i x_i 2^i ) * y = sum_i x_i (2^i  * y)

      Multiplications by y are just shifts.  

      Cost to multiply: O( (lg x)(lg y) )

   Division: z = qx + r, 0 <= r < x.  (We assume z>0, x >= 1).

      Standard long division algorithm works well in binary.
      Just try the subtraction.  If the result goes negative
      don't use it.

      Number of subtractions = number of quotient bits computed

      So we need O( (lg q) (lg x) ) bops

Lecture 3
1/23/17 Monday

   Asymptotic complexity of arithmetic

   References: Aho, Hopcroft, and Ullman, The Design and Analysis of
                 Computer Algorithms, 8.2
               Karatsuba & Ofman, Soviet Physics Doklady, v. 7, p. 595, 1963.
               Kleinberg-Tardos, Algorithm Design, 5.6 (for the FFT)

   Division complexity = O( multiplication complexity )

      Idea: To divide by a, compute 1/a by the Newton iteration

                 next x := x(2 - a*x)

            which solves f(x) = a - 1/x = 0.

            f is concave so if 0 < x < 1/a, next x will be too.
            This tells you which direction to round.

            Quadratic convergence: if x = (1-eps)/a, the next x is
            (1 - eps^2)/a.  So we can double the working
            precision with each iteration.  

            If mult complexity is O(L^alpha g(L)) with g nicely behaved,
            we can deliver O(L) bits of 1/a with O(L^alpha g(L)) bops

   Karatsuba subquadratic multiplication algorithm

      Divide and conquer: Reduces an L-bit multiplication to three
      L/2-bit multiplications plus O(L) other work.
 
      Cost of algorithm: O(L^log_2(3)) = O(L^1.59...) bops

   Using the FFT

      We need a fact from the undergrad algorithms course.  Suppose
      that f,g are polynomials (in one variable) with degree < L.
      Using O(L log L) arithmetic operations we can form their product
      h = f*g.  For our purposes we can think of this algorithm as
      a bunch of exact formulas, giving each coefficient of h in terms
      of the coefficients of f and g.  The formulas use:

           A primitive N-th root of unity w, with 2L < N := 2^m <= 4L.

           The reciprocal of N. (This is the only division required.)

      Suppose we want to multiply 

           A = a0 + a1 2 + ... + a_{L-1} 2^{L-1}
           B = b0 + b1 2 + ... + b_{L-1} 2^{L-1}

      Replace 2 by the variable X and form

           f(X) = a0 + a1 2 + ... + a_{L-1} 2^{L-1}
           g(X) = b0 + b1 2 + ... + b_{L-1} 2^{L-1}

      Their product is h = sum_k c_k X^k, where

           c_k = sum_i a_i b_j <= L  (since 0 <= a_i, b_j <= 1)

      Then we "release the carries" and get

           C = A*B = h(2).

      Forming h(2) from the coeffs of h costs O(L log L) [details?]

      The computations required for polynomial multiplication can be
      done modulo the prime p, where p satisfied:

          p == 1 mod 2^m   (this guarantees an element of order 2^m)
          p > L (this guarantees we get the c_k's exactly)

      Linnik's theorem (in this case) states that the least prime p
      that is 1 mod 2^m satisfies p = 2^O(m).  Since p > 2^m >= 2L, 
      we have p>L as required.  Also N is a power of 2 so it has
      an inverse (p is odd).

      As we'll see shortly, arithmetic modulo p can be done using
      O(lg p)^2 = O(lg L)^2 bops.  Putting everything together we
      get an L-bit multiplication algorithm with bit complexity

           O(L (log L)^3 ).

      By carefully choosing moduli, and doing this recursively,
      Schonhage and Strassen (1970) achieved O(L log L loglog L).

Lecture 4
1/25/17 Wednesday

   Greatest Common Divisor

   B/S 4.1-4.3

   Any two positive integers have a greatest common divisor (gcd).
   This follows from the unique factorization law.

   We can compute gcd's efficiently using the idea that

         gcd(u,v) = gcd(v,u mod v)                            (*)

   Euclid's algorithm exploits this systematically.  Let u_0 > u_1 > 0.
   Then

           u_0 = q_0 u_1 + u_2,   0 <= u_2 < u_1
           u_1 = q_1 u_2 + u_3,   0 <= u_3 < u_2
           u_2 = q_2 u_3 + u_4,   0 <= u_4 < u_3
              ...
       u_{n-1} = q_{n-1} u_n      (u_{n+1} = 0)


    u_n, the last divisor, is the gcd of u_0, u_1.  (Use (*).)

    Proof of termination: the sum u_i + u_{i+1} decreases.

    By working backwards, we have (since all quotients are >= 1)

          u_n, u_{n-1} >= 1 
               u_{n-2} >= 1 + 1 = 2
               u_{n-3} >= 2 + 1 = 3
               ...
               u_0 >= n-th Fibonacci number F_n

          (Using the indexing F_0 = F_1 = 1.)

     Since F_n >= const*(1.618...)^n, we get

          n = O(log u_0).

   Cubic complexity bound: Suppose 0 < v < u < 2^L.  Then
   Euclid's algorithm uses

       [# divisions] * O( L^2 ) = O(L^3) bops

    Collins bound: The division steps of Euclid's algorithm (applied
    to u,v) use O((lg u) (lg v)) bops.

           Proof: Work for the ith division step (counting from i=0)
                  is <= const * (lg q_i)(lg u_1).  Add these up and
                  use q_0 * q_1 * ... * q_{n-1} <= u_0.

    Extended Euclidean Algorithm:

        We can write each step of the algorithm using 2x2 matrices.
        For example, the first one is

         [u_1]  =  [0    1]  [u_0]
         [u_2]     [1 -q_0]  [u_1]

        Multiplying all the matrices together we get

         [u_n]  =  [alpha  beta]  [u_0]
         [  0]     [gamma delta]  [u_1]

         where alpha, beta, gamma, delta are integers.

        In particular:

              gcd(u,v) = integer (!) linear combination of u,v.

        The Collins bound applies to this too: you need O((lg u)(lv v))
        bops to find the gcd and the four matrix entries.  Analysis
        is very similar.

Lecture 5
1/27/17 Friday

    Congruences, Arithmetic mod n

    B/S 5.1-5.3

    Defn (Gauss) Let n >= 1.  a and b are congruent modulo n if
                 n | (b - a)

                 Written a == b (mod n)

    Unary mod: x mod n means the remainder you get when you divide x
               by n.  Remainders are chosen from {0,...,n-1}.

    Congruences can be added, subtracted, and multiplied as equalities
    are in ordinary algebra.

        Example: Solving a linear equation mod n.

    Let Z_n = {0,...,n-1}.

        Temporarily use +_n and *_n to indicate the operations

           x +_n y = (x + y) mod n
           x *_n y = (x * y) mod n

        That is, we do the operation as usual and then take the remainder.

    The system (Z_n, +_n, *_n, 0, 1) satisfies the "usual" laws of
    algebra.  

        More precisely: this system is a commutative ring.  For the
        commutative ring axioms and some interesting history, look 
        up "Ring" in Wikipedia.

        Exercise: Prove all these axioms for Z_n.

        Example we did: every element x has an additive inverse, and
        this additive inverse (written -x) is unique.

    Warning: Standard math defines arithmetic mod n using equivalence
    classes of the "mod n" congruence relation.  The elements of (our)
    Z_n are "names" for these classes.

    Example: addition and multiplication tables for Z_4.
             Latin square property of addition table

    We'll now just use +,* for addition and multiplication.
    Also (in the course log) Zn for Z_n.

    Defn: a in Zn is invertible if there is a b making ab = 1.

          There can be nonzero elements with no inverse.

    Defn: A field is a commutative ring such that:

               every nonzero element is invertible
               0 != 1

    It can be shown that Zn is a field iff n is prime.

    Thm: If a has an inverse it is unique.

      Pf: suppose ab = ab' = 1.  Then

           b = b(ab') = (ba)b' = b'.

    Zn* is the set of invertible elements within Z_n

      Do some examples, e.g. Z4* = {1,3}.
    
    How do we compute inverses efficiently?

      Use the extended Euclidean algorithm. This gives us integers
      x,y for which nx + ay= 1.  So

        a^{-1} = (y mod n)

    Example: inverting 7 mod 24

                              quotients
          24 = 3 * 7 + 3      3
           7 = 2*3 + 1        2
           3 = 3*1            3

    We know that

        [1] = [0  1] [0  1] [0  1] * [24]
        [0]   [1 -3] [1 -2] [1 -3]   [ 7]

    Let's collect the matrix product left to right (the associative 
    law holds for matrices) and observe how the second matrix changes:

        [0  1] [0  1] = [ 1  -3] <-- copy top row
        [1 -2] [1 -3]   [-2   7] <- top row - quotient * bottom row

        [0  1] [ 1 -3] = [-2   7]
        [1 -3] [-2  7]   [ 7 -24]

    So (-2)*24 + y*7 = 1, making 7^^{-1} mod 24 = 7.

    If we like, we can think of this as a process that generates
    a new row from the last two rows:

            0   1
            1  -3
           -2   7
            7 -24

    You can prove by induction that the signs alternate, and as
    you go down a column, the absolute values will not decrease.

    For computing a^{-1} we can ignore the first column.  This is
    sometimes called the half-gcd algorithm.

    Bounds on bit complexity for Zn.

      Suppose the extended Euclidean algorithm computed, for u,v >= 1,

        [d] = [alpha  beta] [u]
        [0]   [gamma delta] [v]

      Then |gamma| <= u and |delta| <= v (Why? the fraction gamma/delta
      is in lowest terms, and equals -u/v.)

      Also, |alpha| <= |gamma|, |beta| <= |delta|.

      When we use this for computing inverses, |beta| <= n.  So
      we can get beta mod n from beta using one addition or subtraction.

    Upshot: The number of bit operations to do computations in Zn is
    bounded by

              O(lg n)   for +,-
              O(lg n)^2 for *
              O(lg n)^2 for inverse (if possible)

    This assumes "standard" arithmetic is being used.

Lecture 6
1/30/17 Monday

  Topic du jour: Exponentiation

  Reference: D. Gordon, J. Algorithms 27:1, 1998, pp. 129-146.

  Review of arithmetic in Zn.

  Exponentiation is defined by induction:

            1 if r = 0.
     x^r = 
            x^{r-1}*x if r >= 1.

  Repeated squaring reduces the number of multiplications to O(log r).

     Often written as a loop, which processes the binary digits
     of r in either the forward or reverse direction.

  Addition chains

     We want to consider algorithms that compute powers by multiplying
     together previously computed powers.  

     Defn: An addition chain for r is a list a1=1 , a_2, ..., a_l = r
     of positive integers, in which each a_k (k >= 2) is the sum of
     previous entries.

     Example: computing x^11.  Since 11 = 1011 in binary, the right-to-left
     algorithm would produce (starting with x^0 = 1):

          x^1, x^2, x^4, x^5, x^10, x^11.

     So the addition chain has length 6, it is 1, 2, 4, 5, 10, 11.

  Thm: Any addition chain for r has length Omega(log r).

     Pf: Show by induction on r>=1 that any number you can make with
     a length l chain is <= 2^{l-1}.
     
  This is best possible up to constant factors, since the "greedy" chain
  1, 2, 4, 8, ... , 2^{l-1} for 2^{l-1} has length l.

  Defn: L(r) = length of a shortest addition chain for r.

     Exact value of L(r) is not known, except in special cases.

     Computing L exactly is an NP-hard problem.  

  Erdos bound (1960): For almost all r, we have

                L(r) = log r + (1 + o(1)) log r / loglog r.

     Here log = log_2.

     Repeated squaring gives L(r) <= 2 log r

  m-ary methods

     To compute x^r:

          Let r = sum__[i=0}^L c_i m^i

          Precompute x^0, x^1, x^2, x^3, ..., x^{m-1}.

          z = x^{c_L}
          for i from L-1 down to 0 do
                z = z^m
                z = z*x^{c_i}.  (*)
          return z

     For convenience, let m = 2^k.

     The basic tradeoff is the following: When m is large, there is
     a longer precomputation, but fewer executions of (*).

     Analysis: The worst-case number of multiplications is about

           2^k + log_m(log r) + log_m r  
        =  2^k + (log r)/k + log r

     We should pick k to minimize the first two terms.  Consider

        f(x) = 2^x + a/x,  a --> oo.

        Minimizing f in the usual way, we are led to the equation

             2^x = a / (x^2 ln 2),

        which has a unique positive solution.  Taking logs,

             x = log a - 2 log x + O(1).

        This is probably impossible to solve in closed form, but
        we can try plugging the equation into itself ("bootstrapping").
        We get

             x = log a - 2 log( log a - 2 log x + O(1) )

               = log a - 2 log( (log a) (1 - O(log x / log a) ).

        So   x = log a - 2 loglog a + O( (loglog a)/(log a) ).

        Pretend for the moment that k is a continuous variable
        that we can choose to be log a - 2 loglog a.  Then,

            2^k = log r / (loglog r)^2,

         (log r)/k ~ (log r) / (loglog r).

        However, k must be an integer.  So if it is rounded up,
        the first quantity increases by a factor <= 2, and the
        second is never increased.  So we end up with

            2^k + (log r)/k <= (log r)/(loglog r)
                               + O( (log r)/(loglog r)^2 ).

    Taken together, the Erdos bound and the m-ary method (optimized
    as above) give a polynomial-time approximation scheme (PTAS) for
    addition chains.  

Lecture 7
2/1/17 Wednesday

    Chinese Remainder Theorem (CRT)

    B/S 5.5

    Thm: If n1,n2 >= are relatively prime, there is always a
    solution to

             x == x1 (mod n1)
             x == x2 (mod n2)

    Any two solutions differ by a multiple of n (= n1 n2)

    Proof: show that f : Zn --> Z_{n1} x Z_{n2}, given by
           f(x) = (x mod n1, x mod n2) is 1-1, therefore onto.

    The CRT is a very important theorem with lots of clever applications. 
    Here are interpretations and examples of its use.

    1) Counters with relatively prime periods

           Ex: two counters, with periods n1 = 2, n2 = 5 in 
           parallel.  

                     0 0
                     1 1
                     0 2
                     1 3
                     0 4
                     1 0
                     0 1
                     1 2
                     0 3
                     1 4

           Every possible pair appears. The full period is 2*5 = 10.

           This is a way to get very long periods out of simple
           devices.  One WWII era machine (Lorenz) had wheels 
           with periods

               23, 26, 29, 31, 37, 41, 47, 51, 53, 59

           (Reference?) Their product is about 6 x 10^15.

    2) Sequence coding

         Around 1930, Kurt Godel proved that any formal system 
         powerful enough to express a "useful" fragment of
         number theory would have statements that are true but
         not provable within the system.

         First-order means that we can quantify over numbers (but
         not sets of numbers, sets of sets, etc.).

         To prove this, Godel figured out how to express statements
         about computation within the system.  This required a way
         to encode sequences as numbers.  Godel used Chinese remaindering.

    3) An algebraic structure theorem.

         As rings, Zn is isomorphic to Z_{n1} (+) Z_{n2}

         This is the way the theorem is presented in most algebra
         books.  What it means is that we can do ring operations 
         component by component.  Example: with n1=2, n2=5,

               3 is represented by (2,3)
               6        "          (0,1)

         3+6 = 9        "          (2,4)

         This is commonly done with n = p1^e1 * ... * pr^er (prime
         factorization).  For convenience let qi = pi^ei.

         Both directions (coding and decoding) can be done in
         polynomial time.

         Encoding x: just divide x by the various qi's, and take
                     remainders.

         Decoding (x1,...,xr): Let mi satisfy

                     1 (mod qi)
               mi ==
                     0 (mod n/qi).

         An explicit formula for the mi is given by

               _
               mi = n/qi 

                      _                 _
               mi = ( mi^{-1} mod qi) * mi.

         Then x = ( sum_{i=1}^r mi xi ) mod n

         Collins bound: Using standard arithmetic, coding and decoding
         can be done with O( (lg n)^2 ) bops, using standard arithmetic.

         You should verify that this works on small examples.

      4) Evaluating complicated expressions exactly

         Let Delta = det(a_ij) be an n x n determinant with integer entries.
             Assume |a_ij| <= B.

         If p is prime, we can evaluate Delta mod p using Gaussian
         elimination.  

         So a reasonable strategy to evaluate Delta exactly would be this:

             Choose r primes p1, p2, ..., pr.
             Compute xi = Delta mod pi, for i=1,...,r.
             Use CRT (with n=p1 p2 ... pr) to recover Delta from the xi.

         To get the exact value of Delta, we need the product of
         the pi's to exceed 2 |Delta|.  (The factor 2 is there because
         the determinant could be negative.)

         Hadamard inequality: The absolute value of any determinant
         is no more than the product of the Euclidean lengths of
         the rows.  This has a nice geometric interpretation.

         So |Delta| <= [sqrt(n B^2)]^n = n^{n/2} B^n

         Since pi >= 2, it is sufficient that

           r > (n/2) log n + n log B

Lecture 8
2/3/17 Friday

   Topics:  Quadratic Residues
            Legendre and Jacobi Symbols

   B/S 5.8-5.9

   Zn = {0,1,...,n-1} 
   Zn* = invertible elements of Zn = elts a with gcd(a,n) = 1.
   phi(n) = size of Zn*.

   Zn* is an abelian group.  "Abelian" means that the multiplication
         is commutative, i.e. xy = yx for all x,y.

   Consider squaring in Zn*.

   E.g.     n=7    x = 1 2 3 4 5 6
                  x^2 = 1 4 2 1 4 2

      Elements of Zn*^2 are called quadratic residues
      Elements in Zn* - Zn*^2 are  quadratic nonresidues

   The quadratic residues form a subgroup of Zn*.

      Proof: This set is closed under multiplication:
             x^2 y^2 = (xy)^2 (we used the commutative law here)
             This is sufficient since Zn* is finite.

   In Z7*, exactly half of the elements are quadratic residues.

      Is it always half?  No.   Consider n=15.

          Z_{15}* = { 1, 2, 4, 7, and their negatives }

          Z_{15}*^2 = {1, 4}.

      The Chinese remainder theorem explains why this is

          Z_{15}^* == Z3* x Z5*

                      In each component, 50% of the elements are squares
                      So the overall fraction is (1/2)*(1/2) = 1/4.

   Thm: (Gauss) If p is prime, Zp* is a cyclic group.

        This means that for some g, the powers of g exhaust the group.

        g is called a primitive root or generator

        This is proved in Hardy and Wright, Section 7.5.

   Example: p = 11, g = 2

            x = 1 2 3 4 5  6 7 8 9 10 11 12
          2^x = 1 2 4 8 5 10 9 7 3  6  1  2 ... (repeats w/ period 10)
                *   *   *    *   *     *    <-- quadratic residues

   If g^x == a (mod p) we call x an index or discrete logarithm of a.
   Any two indices for a must be congruent mod p-1.

   Corollary: a in Zp* is a quadratic residue iff its index is even.

      There is no obvious way to compute the index.  However, there
      is a better way.

   Euler's criterion: When p is an odd prime, 

            a in Zp* is a quadratic residue <==> a^{(p-1)/2} == 1 (mod p).

   Proof: Let a = g^x, x = 2y+z, where z is 0 or 1.  Plug in and
          observe that any (p-1)-th power is 1, since p-1 = |Zp*|.

   This uses O(lg p) *'s in Zp*.

   When the power is not 1, what is it?  Let alpha = a^{(p-1)/2}.
   Since alpha^2 == 1 mod p, we have p | (alpha - 1)(alpha + 1).
   So the only possibilities for the power are +- 1.

       Note: This is an abuse of notation, since officially Zp*
       contains only positive numbers.  Think of -1 as another name
       for p-1, in this context.

   Legendre Symbol (just a function):

               0 if p divides x           
      (x|p) = +1 if x is a quadratic residue mod p
              -1 if x is a quadratic nonresidue mod p

   There is an extension in which the "downstairs" number can be
   any odd positive integer.

   Jacobi Symbol: Let n = p1^e1 ... pr^{er} (prime factorization)
                  be odd, >= 1.

      (x|n) = product_{i=1}^r (x | pi)^ei.

   This is a group homomorphism from Zn* into { +-1 }.

   Example: n = 15 = 3*5.  Quadratic residues marked with '

                    mod 3     mod 5    (x|15)

           x =  1'  1'        1'        +1
                2   2         2         +1
                4'  1'        4'        +1
                7   1'        2         -1
                8   2         3         +1
               11   2         1'        -1
               13   1'        3         -1
               14   2         4'        -1

    Note that all quadratic residues x have Jacobi symbol 1.
    The converse is not true.

    We can compute (x|n) quickly, without knowing the factorization
    of n.  Key fact: (m|n) and (n|m) are related.  This is called
    quadratic reciprocity.  It is one of the all time great results
    of number theory.  More fully:

    Thm: Let m,n be odd, >= 1.  Then

                  +1  for n == 1 (4)
    i)   (-1|n) = 
                  -1  for n == 3 (4) 


                  +1  for n == +- 1 (8)
    ii)   (2|n) =
                  -1  for n == +- 3 (8)

    iii) (m|n) = +- (n|m) where the minus sign is taken only when
                          m == n == 3 (mod 4).

    (For a proof based on clever counting, see Hardy and Wright, 
    Sections 6.12-6.13.)

    This leads to a very rapid algorithm for evaluating (m|n).

    Example:   (3|35)  =  - (35|3)  by iii)
                       =  - (2|3)
                       =  - (-1)    by ii)
                       = +1.

          You should check this by computing (3|5) and (3|7).

    Bit complexity is similar to Euclid's algorithm.

Lecture 9
2/6/17 Monday

    B/S 5.9, 7.1

    Review of quadratic residues / nonresidues
              Legendre and Jacobi symbols
              quadratic reciprocity and base cases 

    Mmemonic: in quadratic reciprocity, the + sign is associated
              with membership in a subgroup.

    Computing Jacobi symbol using a variation on Euclid's algorithm

         Suppose we want (m|n), n>1, odd.
         Replace m by m mod n (choose remainders so that 0 <= m < n).
         If m=0 set J=0
         Else
              m = m' * 2^nu, with m' odd  [partially factor m]
              Let sigma = +1 if n == +1 mod 8, -1 if == +-3 mod 8.
              If m' = 1 set J = sigma^nu
              Else 
                   tau = -1 if m' == n == 3 mod 4, +1 in other cases
                   Set J = sigma^nu * tau * (n | m').

    Note that the basic division step in Euclid's algorithm has
    been replaced by
                         0 if v | u
              u = qv + {
                         2^nu * r, with r odd

    You can do the same analysis we did for Euclid's algorithm and
    find that:

       # if division steps [= recursive calls] is O(lg max{m,n})
       # of bops is O( (lg m)(lg n) )

    Solving quadratic equations mod p (p = odd prime)

       If a is not a multiple of p, the solutions to ax^2 + bx + c = 0
       are given by

                  -b +- sqrt(b^2 - 4ac) / (2a).

          To prove this, complete the square as you did in high
          school algebra.

       So we are reduced to finding square roots.  There are two cases.

          p == 3 (mod 4):  use group theory (deterministic)
          p == 1 (mod 4):  use finite fields (randomized)

       1) Let |G| be an abelian group of odd order.

          The squaring map is an automorphism of groups (this means
          it commutes with the group multiplication, and is 1-1 and onto).

          If n = |G|, then ( a ^ {(n+1)/2} )^2 = a^{n+1} = a.  This
          gives the required square root of a in G.  (And, incidentally,
          proves that squaring is onto.)

          For p == 3 (mod 4),  | Zp*^2 | = (p-1)/2, so

              sqrt a = +- a ^ {(p+1)/4}.

          This formula was known to Lagrange in the 1700s.

       2) Let Zp[X] be the ring of polynomials with coefficients in
          the (finite) field Zp.  This has a version of the long division
          algorithm, and hence exhibits unique factorization into 
          irreducibles.

          Suppose that f = X^2 + cX + d is irreducible (this means
          it can't be factored except in trivial ways).  Then the
          system

                R = Fp[X]/(f)

          is a finite field F, with p^2 elements.  To do the ring 
          operations +,-,* in R, you just do the corresponding operation
          with polynomials, and then divide it by f, throwing away
          everything but the remainder.  

          In F, the p-th power map is an automorphism of fields:

                (ab)^p = a^p b^p
                (a+b)^p = a^p + b^p
                     This is sometimes called the freshman's binomial
                     theorem.  To prove it, you expand the left hand
                     side and observe that all binomial coefficients
                     except the first and the last are divisible by p.
                It is also 1-1 (hence onto).
                     Suppose you had y^p = z^p in Zp*.  Set x = y z^{-1}
                     and consider x^p. The order of x has to be 1 or
                     p.  It cannot be p, since p doesn't divide p^2 - 1.
                     Therefore x=1.

           The following was published by M. Cipolla about 1900.

               It is desired to find sqrt a mod p (p == 1 mod 4).
               Find an irreducible poly X^2 + bX + a
               In R = Zp[X]/(f), compute b = X^((p+1)/2).
               Then, b^2 = a.

           The last line holds because

               b^2 = X^{p+1} is the product of X's two conjugates,
                     hence the constant term of f.

               (Since p-th powering is a field automorphism, X^p
               has to be the conjugate of X.)

           We will discuss next time why this is likely to work.

Lecture 10
2/8/17 Wednesday

   Finish Modular Square Roots

   Last time we discussed solving x^2 == a (mod p) when p is an
   odd prime.

   When p==3 mod 4 there is a formula: sqrt(a) = +- a^{(p+1)/4}

   All known efficient algorithms for p == 1 mod 4 use randomization.

       Cipolla's algorithm (1903)

       Goal: find the square root of a in (Zp*)^2, p an odd prime.

       Choose t in Zp at random
       If t^2 - 4a == 0, return +- t/2
       If (t^2 - 4a|p) = +1, the algorithm fails (try again)
       Else [t^2 - 4a is a nonresidue for p]
            Let f = X^2 + tX + a  [irreducible over Zp]
            return +- X ^ {(p+1)/2} mod f(X).

    Correctness was discussed last time.

    Reliability: about 50% of the t's are "good".

       To prove this note that the only bad t's are those for
       which 

               there exists u != 0 [t^2 - 4a = u^2]

       So the number of bad t's is at most

               (1/2) [# of Zp solutions to t^2 - 4a = u^2]

       By changing variables, x = t/(2b), y = u/(2b) with b^2 = a,
       we will be done if we can count the solutions to

                x^2 - y^2 = 1.                    (*)

       It is known from classical geometry that any conic section
       has a parametrization by rational functions.  Here, we use

                x = (s^2+1)/(s^2 - 1), y = 2s/(s^2 - 1);

       the inverse mapping is

                s = y/(x-1).

       [You can get an idea of why it works by drawing the locus of
       (*) in R^2.  This is a hyperbola H, going through P = (1,0).
       s is the slope of the line L through P.  L meets H in exactly
       one point, as long as s != +- 1.  Draw the picture.]

       The "forward" map s |--> (x,y) is 1-1 from Zp - {+-1} onto
       {(x,y) : x^2 - y^2 = 1} - {(1,0)}.  So

              p - 2 = [# of Fp solutions to (*)] - 1,

       giving (1/2) [p-1] = p/2 - 1/2 as an upper estimate for the
       number of "bad" t's.
       
   This is a new kind of algorithm: it uses randomness in an essential
   way, and we can only guarantee good performance "on average," over
   the random choices used by the algorithm.  If you run it repeatedly
   until it succeeds, the expected number of bops is O( (lg p)^3 ).

   Computing square roots mod prime powers.

     We'll only treat p odd.  To solve x^2 == a (mod p^e), we
     can solve it modulo p, p^2, p^4, p^8, ... until the exponent
     is big enough.  We'll assume gcd(a,p) = 1.

     The base case is handled by previous algorithms.

     To go from p^k to p^{2k}, assume that x_0^2 == a (mod p^k).
     We seek an x_1 for which

        (x_0 + p^k x_1)^2 == a (mod p^{2k})

     Expanding the square and rearranging we get

        [x_0^2 - a] + 2 x_0 x_1 p^k == 0 (mod p^{2k})

     which holds iff

        [x_0^2 - a]/p + 2 a x_0 x_1 == 0 (mod p^k})

     This is a linear congruence that we can solve for x_1, since
     2 a x_0 is not divisible by p.

     This is similar to Newton iteration in that we double the
     "precision" (exponent of p) at each stage.  Therefore,
     the number of bops needed to get to p^e from the starting
     square root mod p is "O" of

         (lg p^e)^2 + (lg p^{e/2})^2 + (lg p^{(e/4))^2 + ...

         = O( lg^2 (p^e) ). 

   Solving x^2 == a mod n for a in (Zn*)^2.

      If we know the factorization of n, we can use the Chinese
      remainder theorem.  The idea is to let n = p1^e1 ... pr^er,
      find the square roots modulo each prime power, and recombine.
      If the pi are all odd there will be 2^r solutions.

   Is the factorization of n necessary?

      Suppose we have a magic box B that, when given any pair (a,n)
      where a is a quadratic residue mod n, provides a square root
      of a mod n.  Then we can use B to factor n efficiently.

      Rabin (~1979) gave an elegant proof of this.  All the ideas
      appear in the case n=pq, where p and q are distinct primes.
      Remember that by the Chinese Remainder theorem:

               ~
            Zn = Zp (+) Zq.

      I can choose y at random from Zn*, and present (a,n) to
      the box B, where a = y^2 mod n.  B will return some x, say 

             x <--> (x1,x2) 
           
      B did not know which y I squared to get a.  So there are
      four equally likely possibilities:
             
             y <--> ( x1, x2)
             y <--> ( x1,-x2)
	     y <--> (-x1, x2)
             y <--> (-x1,-x2)

      So
           x+y <--> ( 2 x1, 2 x2)
           x+y <--> ( 2 x1,    0)
	   x+y <--> (    0,   x2)
           x+y <--> (-2 x1,-2 x2)

      With probability 1/2, then, gcd(x+y,n) will split n.

Lecture 11
2/10/17 Friday

   New Section: Prime Numbers
                Goal: Present Agarwal, Nayan, and Saxena's proof that
                prime testing can be done in deterministic polynomial
                time (Annals of Math., 2004)

   Since primes are useful (for hashing, cryptography, randomized
   algorithms, ...) we should first learn if there are enough of
   them.

   Euclid's proof that there are infinitely many primes.

       First, every number > 1 can be factored into primes.  
       Suppose (for a contradiction) that there were only finitely
       many primes, say p1,p2,...,pn.  The number

             q = p1 p2 ... pn + 1

       isn't divisible by any of them, so it must have some new prime
       as a factor.

   What does this tell us about the density of primes?  Let

       pi(x) = # of primes <= x.

       Define x_1,x_2,... by the recurrence

             x_1 = 2
             x_n = x1 x2 ... x_{n-1} + 1

       Then p_n <= x_n.  Using induction, it can be shown that

             x_n <= 2^{2^n}.

       So (using binary logs), when x = 2^{2^n}, 
       pi(x) >= n = log log x (because p1<...<pn<=x).
       logs).  What about the other values of x?  Suppose that  

            2^{2^{n-1}} <= x <= 2^{2^n}.

       Then, pi(x) >= n-1 >= loglog x - 1.  So pi(x) = Omega(log log x).

   This isn't very sharp.  For example, the primes <= 16 are
   2,3,5,7,11,13 so pi(16) = 6.  But loglog(2^4) = 2.

   What's the correct density?  Around 1800 it was conjectured that

      pi(x) ~ x / log x, i.e.  lim_{x -> oo} pi(x)/( x / (log x) ) = 1.

     (For the rest of this lecture log means the natural log ln.)

   Around 1860 Chebyshev proved that

      pi(x) = Theta( x / (log x) )                     (*)

      which we can think of as "the computer scientist's prime number theorem."

      Note: for the rest of this lecture, log = ln, aka natural logarithm

   In 1900, Hadamard (France) and de la Vallee Poussin (Belgium) showed that

      pi(x) ~ integral_2^x dt / (log t)

      Nowadays called THE prime number theorem.  If you use integration by
      parts you can show that the integral is x/(log x) + O(x/(log^2 x)),
      so it implies the original conjecture.

      Numerically, this is a lot better: log 16 = 4 log 2 ~~ 2.8,
      so 16 / (log 16) ~~ 5.7.

   We'll now prove (*).  We need two auxiliary functions:

      theta(x) = sum_{p <= x} log p
      psi(x) = sum_{p^k <= x} log p

   Think of these as log-weighted counts of primes and prime powers,
   respectively.  They are not too far apart:

      psi(x) - theta(x) = sum_{p^k <= x, k>=2} log p
                        <= sqrt(x) log x + sum_{p^k <= x, 3>=2} log p
                        <= sqrt(x) log x + log x (x^{1/3}} + x^{1/5} + ...)
                        = sqrt(x) log x + log x ( x^{1/3} log_2(x) )
                        = O(sqrt(x) log x).


   Upper bound: pi(x) = O( x / (log x) )

     Proof by Erdos (1930)

        Let x > 1 be even.  Since binomial coefficients are integers,

           product of primes in (x/2,x] <= (x choose x/2) <= 2^x.

        Take logs, rearrange, and get

           theta(x) <= theta(x/2) + x log 2.

        This is a typical "divide and conquer" recurrence, and implies
        what when x = 2^k, theta(x) <= c*x for some constant c.  
        If x is not a power of 2, we can simply replace x by the next
        larger power, say x' <= 2x, and get from this theta(x) <= cx' < 2cx.

        So theta(x) = O(x).  Now we convert back to prime counts.

           pi(x) = sum_{p<=x} 1 = sum_{p<=x} (log p)/(log p)   (p>1!)
                 <= sum_{p<sqrt x} 1 + (2 / log x) sum_{p<=x} log p
                 <= sqrt(x) + O(theta(x)/(log x)) = O(x / log x).

   Lower bound: pi(x) = Omega( x / (log x) )

      Proof by Gelfand (1946)

           Note that 0 < x(1-x) <= (1/4), in fact > 0 on an interval
           of positive length.  So

              0 < integral_0^1 x^n(1-x)^n dx <= 4^{-n}.

           The integrand has the form sum_{i=0}^{2n} a_i x^i, where the
           a_i's are integers.  Since x^i integrates to 1/(i+1), we have
           (for n >= 1)

              1/lcm(1,2,...,2n+1) <= 4^{-n}

           Observe that log(lcm(2,3,...,2n+1)) = psi(2n+1).  So for
           odd integers x >= 3,

              psi(x) >= (x-1)/2 * log 4

           Since the largest odd integer <= x is >= x-2, we get

              psi(x) >= (x - 3) log 2,   about 0.7 * x.

           (This is pretty good for an elementary argument.)

           Then

             theta(x) = psi(x) - (psi(x) - theta(x))
                      = Omega(x) + O(log x sqrt x)
                      = Omega(x).

           Now we convert back to primes:

             pi(x) >= ( 1/(log x) ) theta(x) = Omega( x / (log x) ).

Lecture 12
2/13/17 Monday

   Review of P and NP.  The main points are:

      P and NP are classes of decision problems
      Problems in P can be solved in deterministic poly time
      Problems in NP have short, easily verifiable certificates.
      There is a class of "hardest" problems in NP, of provably equal
        difficulty (as far as poly time algs are concerned).

   An example of a "hardest" (NP-complete) problem is 0-1 integer
   programming: Given an integer matrix A and an integer column vector
   b, is there a 0-1 solution to Ax = b?  (We can just say "no" if
   the dimensions don't match up.)

   For background on these topics, see Garey and Johnson's book,
   Computers and Intractability.

   The set of composites (in binary) is in NP, since

       n is composite iff there exists d ( 1 < d < n  &  d|n ).

   Pratt (SIAM J. Computing 1975) showed that the set of primes (in binary)
   is also in NP.  Here is his proof.

       First, p is prime iff the group Zp* is cyclic, of order p-1.

          (The last condition is important.  Z4* is cyclic but order 2.)

          For a proof that every prime p has a primitive root (= generator
          of Zp*), see Hardy and Wright, 7.5.

       To prove Zp* is cyclic for any particular p, we can exhibit the
       prime factorization p-1 = q1^e1 ... qr^er, a generator g for Zp*, 
       and then verify that:

          1) g^{p-1} == 1 (p)
          2) If 1 <= i <= r,  g^{ (p-1)/q_i } !== 1 (p).

       (That is, the order of g in Zp* is a divisor of p-1 but cannot
        be a proper divisor.)

       The primality of the various qi's is done recursively.  The base
       case is p=2, which we accept as prime because there are no integers d
       with 1 < d < 2.

       We can present this proof as a tree.

           Nodes = pairs (p,g) with g a generator of Zp*

           Edges = links from (qi,hi (say)) up to (p,g)

       Here's a proof for p=7:

                 (7,3)                  4 nodes
                /      \                3 edges
              (2,1)   (3,2)
                        |
                      (2,1)

       Let N(p) = # of nodes and E(p) (= N(p) - 1) the number of edges.

       Claim: N(p) <= 2 log_2 p - 1.

       Proof: Induction on p.  Verify directly when p=2,3.  So let
       p >= 5.  Write p-1 = q1 q2 ... qs with qi prime (some of these
       might be equal).  Then

         N(p) = 1 + sum N(q_i)
              <= 1 + sum [ 2 log_2 q_i - 1 ]
              = 2 log_2 p + (1 - s)
              <= 2 log_2 p - 1, since s >= 2 for p >= 5.

       Each node requires O(lg p) bits, so the total proof length
       is O( lg p )^2.

       To finish the proof, we must ascertain that a verifier can
       check it in polyomial time.

       Time to check 1) and 2) at all nodes: We do an exponentiation
       for each node and for each edge, so this is

            <= 3 log_2 p * O(lg p)^3 = O(lg p)^4 bops.

       Time to verify that the alleged prime factors are correct:

            At each internal node, we use long division to verify that:

              q1 | (p-1)
              q1 | (p-1)/q1
              q1 | (p-1)/q1^2
                ...
              q1 doesn't divide (p-1)^(q1^(e1+1))  (we now know e1)

              q2 | (p-1)/(q1^e1)
              q2 | (p-1)/(q1^e1 q2)
                ...
              q2 doesn't divide (p-1)^(q1^e1 q2^e2)  (we now know e2)

              etc.  until we reach a dividend of 1.

           The number of bops is within a constant factor of

              sum_{i=1}^r [ (ei + 1) lg qi ] lg p
              = O ( [ sum_i ei * log qi ] * lg p )
              = O ( log (p-1) * lg p )
              = O( (lg p)^2 ).

           Since there are < log_2 p nodes, the total for this is
           O(lg p)^3 bops.

Lecture 13
2/15/17 Wednesday

Pseudoprimes
Carmichael Numbers
Randomized Prime Testing

Pseudoprimes

   Fermat's little theorem: when n is prime, a^n == n (mod n)

                              "  " "   "     a^{n-1} == 1 if gcd(a,n) = 1.

   This is often used as a quick and dirty prime test, since if the
   congruence fails when 2 <= a <= n-2, we are certain that n isn't
   prime.  However, satisfaction of the congruence does not imply
   that n is prime.

   Example: n = 341 = 11*33.   2^340 == 1 (mod 341), since 

                     == -1 mod 11
           2^5 = 32 
                     == +1 mod 33.

   We call 341 a (base 2) pseudoprime.

   Theorem.  For every reasonable base a, there are infinitely many 
             base a pseudoprimes. (Erdos?)

Carmichael Numbers   

   The worst possible case is if a^{n-1} == 1 for all a in Zn*.
   An odd composite with this property is called a Carmichael number.
   (Named after Robert Carmichael, who studied them systematically
   around 1912.)  The first few are

       561, 1105, 1729 (Ramanujan's taxicab number), ...

   Let's verify that 561 = 3 * 11 * 17 is Carmichael.  Use the 
   Chinese remainder theorem:

       Z_561 ~~ Z_3 (+) Z_11 (+) Z_17
   so
       Z_561* ~~ Z_3* x Z_11* x Z_17*
              ~~ C_2  x C_10  x C_16

   Note that 560 = 7 * 8 * 10 is divisible by 2, 10, and 16.
   So whenever a is prime to 561, a^561 == 1 (mod 561).

   In 1994, Alford, Granville, and Pomerance showed that there
   are infinitely many Carmichael numbers.

   So to get an effective prime test we are going to have to use
   something along with Fermat's theorem.
                     
Solovay-Strassen Randomized Prime Test

   Based on Euler's criterion:

     n odd prime ==> for all a in Zn*, (a|n) == a^{(n-1)/2} ( mod n ).

   D.H. Lehmer proved (around 1970) that this test has no Carmichael
   numbers.  We will prove a strong version of Lehmer's theorem shortly.

   This suggests a randomized prime test:

         Input n >= 3, odd.
         Choose a, 1 <= a <= n-1, at random.
         If (a|n) = 0 or (a|n) == a^{n-1} (mod n) say n is prime
         Else ( (a|n) !== a^{(n-1)/2} ) say n is composite.

   Theorem (Solovay and Strassen ~1975).  For this test,

                = 0    when n is primes
      Pr[error] 
                <= 1/2 when n is composite.

   Because of this one-sided property, we can run the test many times
   independently and achieve a very small error probability.  For example
   20 independent tests have combined error probability < 10^{-6}.

   Proof of the theorem:

     We only need to consider what happens when n is composite.
     Of course we need the Chinese remainder theorem.  Let
     n = p1^e1 * ... * pr^er.  Then

          Z_n^* ~~ Z_{p1^e1}^* x ... x Z_{pr^er}^*.

     Case 1: p^2 | n for some p, say p1.  Then p divides phi(n) = | Zn* |.
     (Because phi(p^e) = (p-1) p^e).  By Cauchy's theorem, Zn* has an
     element a of order p.  Then a^{n-1} !== 1 in Zn*.

     Case 2: n is square free, that is n = p1 ... pr.  Consider the
     image I of the (n-1)/2 -th power map.  This map is a group homomorphism
     from Zn* to itself, so I is a subgroup of Zn*.

         If I is not a subset of {+- 1} we are done.
 
         If I = {+- 1}, let

            a <--> (a1,...,ar) map to +1
            b <--> (b1,...,ar) map to -1

         Then

            c <--> (a1, b2, ..., br) 

         satisfies c^{(n-1)/2} !== +-1 so it cannot be a Jacobi symbol.
         So this subcase is actually impossible.

         If I = {1} let g be a quadratic nonresidue mod p1, and consider

            d <--> (g,1,...,1).

         By multiplicativity of the Jacobi symbol, 

            (d|n) = (-1)(+1)...(+1) = -1  !== d^{(n-1)/2}.

    At this point we have verified Lehmer's result.  But we can prove more.
    When n is composite, the set

         L = {x in Zn* : (x|n) == x^{(n-1)/2} mod n}

    is a proper subgroup of Zn*.  But the order of a subgroup always 
    divides the order of a group, so |L| <= phi(n)/2.  This gives us

      Pr[error] =   Pr[error | a in Zn*] Pr[ a in Zn* ]
                  + Pr[error | a not in Zn*] Pr[ a not in Zn* ]

                =   (1/2) * Pr[ a in Zn* ]
                  +     0 * Pr[ a not in Zn* ]

                = 1/2.

Lecture 14
2/17/17 Friday

    Miller-Rabin Prime Test
    PRIMES in P/poly 

    Review of Solovay-Strassen test (see last lecture).

      For future work, it will be convenient to allow bases a outside 
      1..n-1.  So let's assume that a is replaced by its remainder mod n
      before continuing with the rest of the test.  This raises the
      possibility that a=0.  Our convention will be that the algorithm
      says "prime" if this happens.

      Let L be the set of liars (in 0..n-1) for this extended test.
      Since phi(n) <= n-2 when n is composite, we still have

         |L| <= phi(n)/2 + 1 <= (n-2)/2 + 1 = n/2.

      We also observe that (-1|n) = (-1)^{(n-1)/2} (supplement to 
      quadratic reciprocity).  So a is a liar iff -a is, that is
      L = -L.

   "Practical" randomized prime testing usually employs a different
   test, studied by Miller and Rabin.

      Let n >= 3 be odd.  Let n-1 = 2^k m, where m is odd.
      Choose a random a, 1 <= a <= n-1.
      Compute (in Zn)

                     a^m, a^{2m}, ..., a^{2^(k-1)m}, a^{2^k m} = a^{n-1}.

      Say that n is prime if the sequence has the form

              * * * * -1 1 ... 1 
      or
              1 1 1 1  1 1 ... 1

      In all other cases, say n is composite.

   This test exploits another necessary condition for primality: When
   n is prime and n | x^2 - 1, then x == +-1 mod n. [Prove this.]

   It can be shown that the Miller-Rabin test has error probability <= 1/4.
   A proof can be found in B/S Section 9.5.

   It is also uniformly better (or at least no worse) than Solovay-Strassen,
   in the sense that the M-R liars form a subset of the SS liars.

   Small Boolean Circuits for Primality

      Theorem (Adleman, 1980): For every l, there is a Boolean circuit
      of size poly(l) that correctly tests l-bit numbers for primality.
      That is {primes} belongs to the complexity class P/poly.

      Lemma: Let n >= 3, odd composite.  Let m > 6n.  If we choose 
      a random a in 1..m, the probability that a is a liar for n is 
      at most 2/3.

      Proof: If m is not a multiple of n, there are at most n-1 numbers
      in 1..m past [m/n]*n.  Some of these might be liars, so the
      total number of liars is bounded by

          [m/n]*(n/2) + n-1 <= m/2 + n <= 2m/3.

      To prove the theorem, we use the Erdos probabilistic argument:
      to show that an element with a certain property exists, we can
      make up an appropriate probability space and show that the
      probability, when we draw elements from that that space, of
      having the property hold is positive.

      If n is an l-bit number, then 2^{l-1} <= n <= 2^l - 1.
      Let us take m = 6*2^l (= 3*2^{l-1}).

      Let a1,...,ak be chosen i.i.d. uniform from 1..m.  Then
      by the lemma,

         Pr[a1,...,ak are liars for some l-bit composite n]
        <= 2^l (2/3)^k 
         < 1

       as soon as (3/2)^k > 2^l, i.e. if k > l / log_2(3/2) = 1.7095... l. 

       So there must be some choice of k bases for which Solovay-Strassen
       tests primality correctly on all l-bit odd numbers.

       The size of the resulting circuit is proportional to

         [# of bases] * l^3 = O(l^4).

       The exponent could be reduced to l+o(1) using asymptotically
       fast multiplication and division techniques.

Lecture 15
2/20/17 Monday

  Riemann Hypotheses and Algorithm Efficiency

  Recall the prime number theorem:

           pi(x) ~ int_2^x dt/(log t)

     How accurate is this?  Computations show that about half the
     digits in this approximation tend to be correct.  Example:

            pi(10^6) = 78498 
            integral = 78626.504...

     This is consistent with a simple probabilistic model.  Suppose
     we let each number n, in the range 3..x, "decide" to be prime
     with probability 1/(log n).  These choices are independent.  Then
     for any epsilon>0, the following holds with probability 1:

      [# of "primes" <= x] = integral_2^x dt/(log t) + O(x^{1/2 + epsilon}),

     The *Riemann hypothesis* is that such an estimate holds for the actual
     primes.

  What did Riemann have to do with it?

     In 1859 Bernhard Riemann tied the behavior of the prime counting
     function to the complex function

            zeta(s) = sum_{n>=1} 1/n^s.

     Using the formula n^{sigma + it} = exp(it log n) n^sigma,
     we can see that this converges absolutely for Re(s) > 1,
     and diverges at s=1. [Why are real/imaginary parts called sigma and t?
     Tradition.]  The function zeta(s) can be continued beyond the 
     line Re(s) = 1.  (One way to do it involves Euler-Maclaurin summation.
     See the book by Harold Edwards, Riemann's Zeta Function.)  It is
     known that:

          0) zeta(s) is defined everywhere but s=1.
          1) zeta(s) is never 0 on Re(s) >= 1
          2) It has infinitely many zeroes in the strip 0 < Re(s) < 1.
             These zeroes are symmetrically placed with respect to
             the so-called critical line Re(s) = 1/2.

     Riemann computed the first few such zeroes and found that they all
     were on the line Re(s) = 1/2.  Another way to state the Riemann
     hypothesis is that this behavior continues on forever.

     Later authors proved that:

       If zeta(s) != 0 to the right of Re(s) = Theta, then

           |pi(x) - integral| = O(x^{Theta + epsilon})

       for every epsilon > 0.

     Thus, the error term with exponent 1/2 + epsilon represents the
     best possible prime number theorem.

  Other prime counting functions

     In 1837, Dirichlet showed that there are infinitely many
     primes p == a (mod n), as long as gcd(a,n).  To do this, he
     used a zeta-like function, one example of which is

       L_m(s) = sum_{n>=1} (n|m) / n^s.

     Since the nonzero values of the Jacobi symbol (n|m) are rougly
     balanced between +1 and -1, we might expect better convergence
     properties.  For Dirichlet's proof, it was important to study
     L(s) on the half line 1 <= x < oo, and in particular know that
     L_m (and similar "L functions") never vanishes at s=1.

     The functions like L_m can also be extended to the complex plane.
     The extended version of the Riemann hypothesis (ERH for short)
     asserts that

      [# p <= x with p == a (n)] = 1/phi(n) integral_2 x dt/(log t)
                                   + O( x^{1/2 + epsilon})

     Equivalently, no L function is zero to the right of Re(s) = 1/2.

     [Note: The L functions for m are in 1-1 correspondence with the
     characters mod m, which are group homomorphisms from Zm* to 
     the roots of unity, augmented to take the value 0 whenever
     their inputs are not prime to m.  If chi is such a character,
     its L function is given by sum_{n>=1} chi(n) n^{-s}.]

  Ankeny's theorem

     If L is a mod m L-function, the density of zeroes in the box
     0 < Re(s) < 1, T-1 < Im(s) is O(log m).  Using this, Titchmarsh
     (~ 1940) was able to show that if ERH holds,

      [# p <= x with p == a (n)] = 1/phi(n) integral_2 x dt/(log t)
                                   + O( x^{1/2 + epsilon} log m )

     Ankeny (1952) used this idea to estimate the the least quadratic 
     nonresidue mod q (prime).  His idea was to consider
     two prime number sums such as

        sum_{p<=x} 1        sum_{p <= x} (p|q)

        ~~ x / log x        ~~ x^{1/2 + epsilon} log m

      If (p|q) = +1 for all primes <= x, these sums are practically
      the same.  But this agreement cannot persist for too long,
      since the break point is, very roughly, when

           x ~~ sqrt(x) log m

      that is, when sqrt(x) ~~ log m, i.e. x ~~ (log m)^2.
      (Here I am ignoring "small" factors like x^epsilon.  Actually,
      Ankeny worked with more complicated sums, for which these 
      factors never appeared.)   So the least quadratic residue
      mod q is, assuming ERH, O( (log q)^2 ).

      Here is a modern version of the Ankeny theorem, with an explicit
      constant that I worked out in 1984.

         Let G be a nontrivial subgroup of Zm*.  Then there is some a>1
         lying outside G with a <= 2 (log m)^2.

   Application to prime testing
     
      In 1975 Miller, using what later became known as the Miller-Rabin
      test, observed that a deterministic polynomial time prime test
      would follow from the ERH.

      We'll use the Solovay-Strassen test.  Note that a base a, with
      1 <= a <= m-1, is a liar for testing primality of m precisely 
      when it is outside the proper subgroup

          L = {a in Zm* : (a|m) == a^{(m-1)/2} mod m }.

      If the ERH is true there is a base a outside L satisfying

         2 <= a <= 2 log^2 m.

      Therefore, trying these numbers as bases in the Solovay-Strassen
      test would give us an infallible deterministic polynomial time
      prime test.

Lecture 16
2/22/17 Wednesday

   A deterministic polynomial-time primality test
   Agarwal, Kayal, Saxena, Annals of Math., 2004 (on CS 812 web site)

   Background on polynomials

     R = Z[X] = ring of polys in one variable with integer coefficients
     If f is a polynomial in R and n is a number, let

       I = (f,n) = {af + bn : a,b in Z[X]}.

     This is an ideal of R (meaning that it is a subgroup of (R,+)
     and is closed under multiplication by arbitrary elements of R).

     So we can form the quotient ring R/I, which you should think of
     as the possible remainders you could have after removing multiples
     of f and n.

     AKS take f=X^r-1, where r>=1.  This means that remainders can
     be computed by applying the following two rules: 

       Reduce exponents of X mod r
       Reduce coefficients mod n.

     The size of R/I (that is, the number of possible remainders)
     is n^r.

   Divisibility of binomial coefficients

     The binomial coefficients (n choose i) can be arranged in Pascal's
     triangle.  Here is a piece of it

            n = 0              1
            n = 1            1   1
            n = 2          1   2   1
            n = 3        1   3   3   1
            n = 4      1   4   6   4   1
            n = 5    1   5  10   10  5   1
            n = 6  1   6  15   20  10  6   1

      When p is prime, all the "interior" binomial coefficients
      (p choose i) = p!/(i! (n-i)!) are divisible by p, since
      i and n-i < p.

      When n is composite, suppose that p^e || n.  Then

                        n   (n-1)...(n-p+1)                        
         (n choose p) = - * ---------------
                        p   (p-1) ... 1

      Since the second fraction contains no p's, we have

         p^{e-1} || (n choose p).

      In this case, then, there is some interior binomial that is
      not divisible by n.

      Examples:  5 10 10 5 are all multiples of 5
                 4 = 2^2 and (4 choose 2) = 6 = 2^1 * 3.

   A Fermat-like theorem for polynomials

      Let n>1.  Then n is prime iff for all a in Zn*,

                   (X+a)^n == X^n + a (mod n).                     (1)

      Proof: Use the binomial theorem, plus Fermat's little theorem.

      Here is an example.  Let n=4, the least composite number.  Then

         (X-1)^4 = X^4 - 4X^3 + 6X^2 - 4X + 1
                 == X^4 + 2X^2 + 1 (mod 4)
                 = (X^2 + 1)^2.

      Incidentally, this shows that unique factorization fails to hold
      in the ring Z4[X], since X^2+1 is irreducible.  (It cannot have
      a linear factor because -1 is not in (Z4)^2.)  Continuing with this
      idea a little, the ideal (X^2 + 1) in Z4[X] is not prime,
      since Z4[X]/(X^2 + 1) has zero divisors.  By the above calculation,
      (X-1) is one.

      Since the expansion of (X+a)^n has n+1 coefficients, the AKS test
      does not verify (1) directly.  Instead, a "small" r is carefully
      chosen, and then the test checks that

              (X+a)^n == X^n+a  ( mod (X^r - 1, n) )               (2)

      for a=1,2,... up to a "small" bound.  It turns out that no
      composite n can escape this test.

   How is r chosen?

      Recall that the order of x in Zr* is the least e>=1 for which
      x^e = 1.  The AKS test uses

          The least r>1 such that [ order of n in Zr* ] > log^2 n.

      (In their paper, log means log_2.)

      That there are such r is clear: if r exceeds n^(log^2 n}
      and is prime to n, the order will be large enough.

      AKS claim more: The least r with this property is O(log n)^5.

      Proof: Let's write [.] for the floor function.  For B>0 let

             M = n^[log B] * product_{i=1}^[log^2 n] (n^i - 1)      (3)

               < n^{ [log B] + ([log^2 n] + 1 choose 2) }.

      If B = [log^5 n] + 1, then for all sufficiently large n

           [log B] + (binomial coeff) ~ (log^4 n)/2  <= log^4 n.

      So M < n^{log^4 n} = 2^{log^5 n} <= 2^B.

      If all numbers in 1..B divide M, then (since M is a common
      divisor for them),

           lcm(1..B) <= M < 2^B.

      On the other hand, by the prime number theorem, if n (and
      therefore, B) is sufficiently large

           lcm(1..B) = exp(psi(B)) = exp( (1+o(1)) B ) > 2^B

      Here psi(.) denotes the von Mangoldt prime-power sensing function,
      defined by

             psi(x) = sum_{p^k <= x} log p.

      So there must be some r <= B not dividing M.  The least such
      r will do the job.  To see this, consider that a number r
      fails to qualify only if:

          i) gcd(n,r) > 1
      or
          ii) r | n^i - 1 for some i <= log^2 n.

      If r violates ii) then already r divides the second factor in 
      (3).  Suppose the least r not dividing M had gcd(n,r) > 1.
      If r is a prime power p^e then p^e = r <= B makes e <= log B,
      so p^e | n^[log B].  If r is not a prime power, then it has
      a relatively prime proper factorization r = r1*r2.  By the 
      minimality of r, both r1 and r2 divide M, and (since they are
      coprime) their product does too.

   How many bases are used?

      The AKS test uses bases a up to sqrt(phi(r)) log n.
      Since phi(r) <= r = O(log^5 n), this is O(log n)^{7/2})
      candidate bases.  For all sufficiently large n, this
      bound is less than n, so if a candidate base fails to
      be coprime with n, we immediately terminate the test.
      Otherwise, it is subjected to (2).

   We now have a deterministic polynomial-time test with a one-sided
   error guarantee: When n is prime the test will say "prime".
   However, it could, as far as we know, fail to catch some composite n.
   Further analysis (given in the next lecture) will show this is 
   not the case.

Lecture 17
2/24/17 Friday

   Today: Correctness of the AKS test on composite input.

   The algorithm.

         Input n (sufficiently large), not a perfect power.
         Find the least r s.t. order of n in Zr* exceeds log^2 n.
            [We showed last time that r <= log^5 n + 1.]
         Make sure n is coprime to all a in 1..r (if not, n is composite)
         For all a <= sqrt(phi(r)) log n (call this l), test if

           (X+a)^n == X^n + a  (X^r - 1, n)            (*)

         If n fails to satisfy one of the tests, n is composite
         Otherwise (it passes all tests) n is prime.

    Notes:  1) AKS include some extra steps to make sure the algorithm
               works on *all* n (not just large enough n as we have here).

            2) log = log base 2

            3) The congruence is modulo the ideal of Z[X] generated by
               X^r - 1 and n.  Usually one writes something like
               ... == ... (mod (X^r-1,n)).

            4) The algorithm is similar to ERH-based prime tests,
               in that we try small bases in order, and then give
               up and declare n prime when we run out of bases.

    Background on the factorization of X^r - 1

       The monic irreducible factors of X^r-1 in Z[X] are
       called *cyclotomic polynomials*.  There is one factor 
       for each divisor of r.  Example:
        
          X^4 - 1 = (X^2-1)(X^2 - 1)
                  = (X-1)  (X+1)   (X^2+1)
                      ^      ^         ^
                      |      |         |
                    Phi_1   Phi_2   Phi_4

       The d-th cyclotomic polynomial Phi_d has degree phi(d), 
       and has the primitive d-th roots of unity for its zeroes.

       If p doesn't divide r, Phi_r can have further factors in Zp[X].
       For example, if p == 1 (4) then (-1|p) = +1, and X^2+1 splits
       into linear factors mod p.

       These irreducible factors all have the same degree, which is
       the order of r mod p.  Furthermore, each such factor appears
       exactly once.  

       [There is a nice article on this: W.J. Guerrier, The factorization 
       of the cyclotomic polynomials mod p, American Math. Monthly, 75 
       (1968), p. 46.]

       Upshot: When p doesn't divide r, the ring A = Z[X]/(X^r - 1, p)
       is a direct sum of fields:

            ~
          A = F_{p^{d1}} (+) F_{p^{d2}} (+) ...  (+) F_{p^{ds}} 

       This is a consequence of the Chinese remainder theorem for 
       polynomials.

       Consequently, the *Frobenius map* x --> x^p is a ring automorphism
       on A.  In particular, it is 1-1.  [This is all we will need.  The
       idea is that it is 1-1 on each component.  If it wasn't, the 
       polynomial X^{p-1}-1 would have more than p-1 zeroes in some 
       field F_{p^d}.]

   Correctness on composite input.

       If n is composite, there must be some p|n for which the order
       of p in Zr* is > 1.

       For 0 <= a <= l, we have:

             (X+a)^n == X^n + a  (X^r-1,n)
             (X+a)^n == X^n + a  (X^r-1,p)  [since p|n]
             (X+a)^p == X^p + a  (X^r-1,p)  [since p is prime]

             (X+a)^{n/p} == X^{n/p} + a    (X^r - 1, p)

                  [Pf: raise both sides to the p-th power and use
                   the 1-1 property of Frobenius.]

       In the remainder of the proof, r and p are fixed.

       Defn: For a number m>=1 and polynomial f, we call the pair
       (m,f) *introspective* if

              f(X)^m == f(X^m)   (X^r - 1, p).

       (Note: AKS say "m is introspective for f.")

       The idea is that m-th powering acts like a Frobenius map, as
       far as we can tell by applying it to f.

       The introspection relation is multiplicative on both sides.
   
         Pf for polynomials: fg(X)^m = f(X)^m g(X)^m 
                                     = f(X^m) g(X^m) = fg(X^m).

         Pf for numbers: Suppose m,m' are introspective for f.  Then

            f(X)^{mm'} = (f(X)^m)^m' == f(X^m)^m'.

            Since m' is introspective, for the indeterminate Y,

               f(Y)^m' - f(Y^m') is a linear combination of Y^r-1 and n

            Now substitute Y = X^m and get

               f(X^m)^m' == f(X^{mm'})   (X^mr - 1, p)
            So
               f(X^m)^m' == f(X^{mm'})   (X^r - 1, p).

        Consequently, there are many "introspective pairs": any number
        in 

          I = {(n/p)^i p^j with i,j >= 0}

        is introspective for any polynomial in

          P = {product_{1=0}^l (X+a)^{e_a} with e_a >= 0}

        Since the introspection relation looks like a pairing
        we will draw a little diagram:

                    N [numbers] x Zp[X] [polys]

                                U

                         I      x     P

                     pi  |            | pi'
                         v            v

                         Zr*          F*  .

        Here the finite field F = Z_p[X]/(h(X)) is obtained by
        choosing a particular irreducible factor of Phi_r(X),
        whose degree equals the order of p in Zr*.  Note that this
        order is > 1.

        Let G = pi(I), and GG [script G in the paper] = pi'(P).
        These are subgroups of Zr* and F*, respectively.  This
        is because gcd(n,r) = 1 and the degree of h is >= 2.

        The idea now is to prove incompatible bounds on the size of GG,
        assuming that n (!= p^k) is composite.

        Let t = |G|.

        GG is large:  |GG| >= (t+l choose t-1)

          Pf: Count the polynomials of degree < t in P.  This is the
          same as the number of non-negative (l+2)-tuples summing
          to t-1.  (The last tuple element takes up the slack if
          the sum doesn't reach t-1.)  Now use the usual usual 
          "stars and bars" argument: Take an array of length
          t-1 + (l+1) = t + l, put in l+1 |'s and fill the rest of
          the entries with *'s:

                 ***|*|****||***|*

          This can be done in (t+l choose l+1) = (t+l choose t-1) ways.

          Now we observe that each such polynomial maps to a different
          element of F*.  Suppose we had a collision, such as f(X) = g(X).
          By the introspection relation, for all m in I,

                f(X^m) = g(X^m)   as elements of F.

          So the polynomial f-g, which has degree < t, has the t
          roots of unity given by X^m, m in I as zeroes.  (X^m is a root
          of unity in F because gcd(n,r) = 1.)   This is a contradiction.

       GG is small: If n != p^k is composite, |GG| <= n^sqrt(t).

          The idea is to find a polynomial in F[Y] that vanishes on all
          elements of GG, and then bound its degree.

          Consider the restricted set 

              J = { (n/p)^i p^j : 0 <= i,j <= sqrt t }.

          Since n != p^k this is a set of ([sqrt t] + 1)^2 > t
          natural numbers, by unique factorization.  So there is 
          a collision mod r: say m1 > m2 with m1 == m2 (r).  
          Using the introspection relation, all f in GG are zeroes 
          of the polynomial

              Y^m1 - Y^m2 ,

          whose degree is <= (n/p)^sqrt(t) p^sqrt(t) = n^sqrt(t).
          So |GG| cannot exceed this.

   Theorem: If n (!= p^k) passes all the tests (*) in the algorithm,
   then n is prime.

   Proof:  Use the counting results above.

     |GG| >= (t+l choose t-1) = ( (l+1) + t-1  choose t-1 )

          >= (l+l + [(sqrt t) log n] choose [(sqrt t) log n] 

               [Pf: t > log^2 n ==> sqrt t > log n ==> t > (sqrt t) log n,
                and (x+y choose y) is increasing in y.]

          >= (2 [(sqrt t) log n] + 1 choose [(sqrt t) log n] )

               [Pf: l = [sqrt(phi(r)) * log n] >= [(sqrt t) log n], and
                (x + y choose y) is increasing in x.]

     By Stirling's formula, the middle binomial coefficient 
     (2[x] + 1 choose [x]) is Theta( 4^x / sqrt x ).  So for 
     sufficiently large n, 

       |GG| > 2^{sqrt t log n} = n^sqrt(t).

     This contradicts the upper bound we found above for |GG|, so 
     n must be prime.

Lecture 18
2/27/17 Monday

New Topic: Integer Factorization

    Decision version: Given n,x, does n have a prime factor <= x?

        In NP n coNP

        Candidate for an "NP-intermediate problem": Something in NP 
        that is neither in P nor NP-complete.

    Splitting vs. Factoring

        Most factoring algorithms are designed to find a nontrivial
        splitting: say n = n1 n2 where 1 < n1, n2 < n.

        If n has Omega(n) prime factors (counted with multiplicity)
        we need to run the splitting procedure Omega(n) times to
        obtain a complete factorization.  This is O(log n).

    The average number of prime factors

        Let omega(n) = number of distinct prime factors.  

        If we pick x from the uniform distribution on 1..n, then
        omega becomes a random variable.  What is its expected value?

        Use indicator variables.  X_p = 1 if p|x, 0 if not.
        Since there are [n/p] multiples of p in 1..n, 

               1/p - 1/n <= E[X_p] <= 1/p

        So E[omega] = sum_{p <= n} 1/p + O(1).

        Using the prime number theorem (this can be made rigorous)

              sum_{p <= n} 1/p ~~ sum_{m <= n} 1/(m log m)
                               ~~ integral_2^n dt/(t log t)
                                = loglog n + O(1).

        So a typical number in 1..n has about loglog n different prime 
        factors.

     Factoring all numbers in a range with the sieve

        Using the Sieve of Eratosthenes, we can readily compute,
        for each x <= n, the largest prime factor of x.  The idea
        is to run the usual process, but instead of crossing out
        the multiples of p, overwrite them by p.  

        The number of arithmetic operations is about sum_{p <= x} n/p
        = n loglog n + O(n).

        So, overall, the cost per factorization is quite low.

    Random Numbers Factor Like Random Permutations

        Every permutation can be written as a product of cycles
        acting on disjoint sets:

        Example        i =  1 2 3 4 5 6 7 8 9 10
                sigma(i) =  7 2 9 6 1 3 4 5 8 10
                 sigma   = (1 7 4 6 3 9 8 5) (2) (10)

        This factorization is unique, up to the order of the
        cycles. 

        The Prime Permutation Theorem:  The probability that
        a random permutation on 1..n is an n-cycle is

            (n-1)!   1
            ------ = -
              n!     n

    Feller's Card Dealing Algorithm (~1945)

        This is a fast method for directly drawing a random permuation
        in cycle form.

           Shuffle the "deck" 1..n
           Deal out cards unti the smallest card in the deck appears.
              The cards, in order, give the first cycle.
           If cards remain, deal out the rest of the deck in the same
              manner.

        Observe that the length of the first cycle is uniformly distributed
        on 1..n.

        To determine the expected number of cycles, we observe that

           Pr[card i ends a cycle] = Pr[1,...,i-1 precede i] = 1/i.

        So by linearity of expectation, 

           E[# of cycles] = sum_{i=1}^i 1/i = H_n = log n + O(1).

        This is analogous to the result that the expected number of
        prime factors of a random number <= x is ~ loglog x.

Lecture 19
3/1/17 Wednesday

   A version of the "card dealing" algorithm for generating random
   numbers in factored form.

   Reference: E. Bach, SIAM J. Computing, 1988 (on course web page)

   Review of Feller's card dealing algorithm (see last lecture)

       The length of the cycle that 1 appears in is uniformly
       distributed.  This also holds if we replace 1 by any other
       number, x.  (If we take a random permutation and relabel
       numbers, it's still a random permutation.)

       Therefore, if we make the n! x n array giving all permutations
       of 1..n in cycle form, and choose a random position from this
       array, its cycle will have uniformly distributed length. 

       Example: n=3

                                   # of 1-cycles 2-cycles 3-cycles
          123 = (1)(2)(3)      x=1      2        2        2  
          132   (1)(2  3)        2      2        2        2
          213   (1  2)(3)        3      2        2        2
          231   (1  2  3)
          312   (1  3  2)
          321   (1  3)(2)

    How long is a random factor of a random number?

       Let's factor all the k-bit numbers, and put them into an
       array with 2^k rows.  (We can drop the leading bits, they
       carry no information.)  Since the total length of a factorization
       is comparable to the length of the number, this will have
       about k columns, on average.

       Let's choose a random bit from this array.  Thinking very roughly
       about it, the probability that our bit is part of the representation 
       of the prime p is about

        [fraction of rows with p] x [fraction of the row that p occupies]

            1/p * (log p)/(log N),

        if N=2^k.

        (Note we are ignoring the possibility of prime powers.)

        Now we do a little calculation.

             sum_{log p <= log x} (log p)/(p log N)
         =   1/(log N) sum_{p <= x} (log p)/p
         ~   1/(log N) integral_2^x dt / t   [by prime # thm]
         ~   (log x)/(log N).

        So a random prime factor of a random number has uniformly
        distributed length (approximately).

        This means we have identified, at least roughly, the "right"
        distribution on primes that will allow us to simulate card
        dealing.

        Our goal will now be to draw a random integer x in the range
        (N/2,N] in factored form.  The algorithm will attain an
        exact uniform distribution, and run in expected polynomial
        time.

   Rejection Sampling

        It is technically easier to draw x with a logarithmic bias
        (Pr[x] proportional to log x) and correct this to a uniform
        distribution.  

        The correction is made as follows.  Once you have x, draw
        a random real number lambda from (0,log x).  
        If lambda <= log(N/2), output x and stop.  If not,
        try again with a new x. 

        The acceptance region is the rectangle

                 A = (N/2, N] x [0, log(N/2)]

        whereas the sampling region is a larger set S, defined
        in the same way but with the curve lambda = log x as upper
        boundary.  (You should draw this.)

        The probability of acceptance is 

                  area(A)/area(S) = 1 - O( (log N)^{-1} ).

        So when N is large we do not expect to need many trials.

   Attaining the Logarithmic Bias

        Notation: #(a,b] denotes the number of integers in the 
        half open real interval (a,b].  This number is (b-a) + O(1).

        Let 
                           log p #(N/2q, N/q]
                           ----- ------------  if q=p^e (prime power)
                           log N      N
             Delta_N(q) =

                           0                   if not.

        Claim: If we choose q, 2 <= q <= N with probability proportional
        to Delta_N(q) and then recursively generate y (with a uniform
        distribution subject to N/2 < q*y <= N), and then output x = q*y,
        the probability we get x is proportional to log x.

        (Note: This the analog of choosing the first cycle in a permutation
        as in the card dealing algorithm, and then recursively choosing
        a random permutation of the remaining elements.)

        Proof: Compute: The probability of x is proportional to

             sum_{q = p^e | x} Delta_N(q) #(N/2q, N/q]

                               log p  #(...]      1
          =  sum_{q = p^e | x} ------ ------- * -----
                               log N    N       #(...]

          = 1/(N log N) sum_{p^e | x} log p

          = 1/(N log N) log x.

        (Note that the proportionality "constant" is actually a function
         of N.)

   A Mechanism for Drawing Prime Powers

        Let's imagine a ruler on which there are equally spaced 
        major divisions for powers of 2, with the numbers in between 
        filled in uniformly:

         2               4               8               16 ...
         |               |               |               |
         |       |       |   |   |   |   | | | | | | | | |||||...
         |               |               |               |

        We'll scale it so that the length between the marks for 
        powers of 2 is 1.

        (Mirror images of diagrams like this are used to illustrate
        binary floating point number systems.  See, e.g.  Figure D-1 
        in Goldberg, What Every Computer Scientist Should Know About 
        Floating Point Arithmetic.  It is also similar to the markings
        on a slide rule, except that slide rules used a strict
        logarithmic scale and did not make the minor divisions equally
        spaced.)

        Note that the first major division after 2,3 is 4,
        the first one after 4,5,6,7 is 8, and so on.  To have
        enough room for the numbers up to N we should go up to
        the major division for 2^L, where L = [log2 N] + 1.

        Our "dartboard" will be the real interval [2,L], with
        marks for every q in the range [2,N], put on as indicated
        above.

        Consider the region between the marked points for q and its 
        successor:

               q                          q+1
               |                          |
               <-- length 2^{-[log2 q]} -->

        (Sanity check: the length separating 5 and 6 is 1/4,
        and log2(5) = 2.3219... which rounds down to 2.)

        It can be shown that Delta_N(q) <= 2^{-[log2 q]} (see p.
        19 of the paper) which means that we can put an acceptance
        region of length Delta_N(q) inside the interval for q, q+1.
        Consequently, the acceptance intervals cannot overlap.

        Therefore, the following procedure will generate prime powers
        with probability proportional to Delta_N(q):

               Choose a random number r from U(2,L) (uniform distribution)
               Find the rightmost marked point that is <= r, suppose it is
                 the mark for q.  Let m_q denote this point, so that
                 m_q <= r < m_{q+1}.
               If q = p^e (prime power) and r <= m_q Delta_N(q),
                 output p^e.
               Otherwise, try again with a new r.

        We want to know the expected number of trials done by this
        procedure.  The total length of all acceptance intervals is

            sum_{q=p^e <= N} Delta_N(q)
        >= sum_{p <= N} Delta_N(q)
         = sum_{p <= N} (log p)*#(N/2p,N/p] / (N log N)
         = sum_{p <= N} (log p)*(N/2p + O(1)) / (N log N)
         = 1/(2 log N) sum_{p <= N} (log p) / p +O( theta(N)/(N log N) ),

        where theta(N) = sum_{p <= N} log p ( ~ N by the prime number thm).
        So (using the prime number thm on the first sum)

         Total acceptance interval length ~ 1/2.  

        Since the length of the entire interval is ~ L, the expected number
        of times we must pick r is about

          ( (1/2) / L )^{-1} = O(L) = O(log N).

   Putting everything together, we now have a procedure that can generate
   uniformly distributed integers in the range (N/2,N], but in factored
   form.  The expected running time will be discussed (briefly?) in the
   next lecture.

   One thing you may worry about is that computers don't actually have
   real numbers.  To handle this, we can draw our numbers bit by bit,
   using unbiased coin flips.  We only need enough bits to make decisions
   about interval membership, and on average this will be small.
   (See paper for details.)

Lecture 20
3/3/17 Friday

   Review of factored random number generation:

       1. Select prime power q=p^e, 2<=q<=N, with prob. ~~ log p /(q log N).
       2. Recursively select y, N/(2q) < y <= N, in factored form.
          /* Now x = q*y has been picked w.p. proportional to log x */
       3. Correct to obtain uniform distribution.

   Our run time analysis will count prime tests.  Note that prime testing
   is now in P, which was not known when the paper was written.

   The expected number of "dart throws" used by step 1 is ~ 2 log N.
   So the expected number of prime tests is no more than this.

   Let E_N be the expected number of tests to generate x.  The goal is
   to prove, that E_N <= 4 log N.

   If we ignore prime powers, and the possibility of restarting in step 3, 
   we get (using the double expectation formula on the recursive step)

      E_N <= 2 log N + sum_{p <= N} E_{N/p} (log p)/(p log N).

          <= 2 log N + sum_{p <= N} 4 log(N/p) (log p)/(p log N) [induction]
 
           = 2 log N + 4 sum_{p<=N} (log p)/p
                     - 4/(log N) sum_{p<=N} (log^2 p)/p

           ~ 2 log N + 4 log N                     [prime number theorem]
                     - 4/(log N) * (1/2) log^2 N   [ditto]

           = 4 log N.

   See the paper for a more complete argument.

   Pollard's "Rho" Method for Factoring [J. Pollard, BIT v.15, 1975, 331-334.]
   
   The Birthday Problem

     Let X1, X2, X3, ... be i.i.d. uniformly distributed on 1..N.
     Define the time of first duplication

       T = min{j : for some i < j, X_i = X_j}.

     By the pigeonhole principle, T <= N+1.

     Theorem: With high probability, T = O(sqrt N).

     Proof: The probability that X1,X2,...,Xm are all distinct is

      1 * (1 - 1/N) * (1 - 1/N) * (1 - 1/N) * ... * (1 - (m-1)/N)

      <= product_2^m exp( - (i-1)/N )     [since 1-x <= e^x] 

       =  exp( - m(m-1)/N )     [summing 1 up to (m-1)]

       If m >= sqrt(2N) + 1, this is <= 1/e = 0.367... .

   Pollard's idea was to use an iterated quadratic map to make pseudo-random
   numbers:

            Choose x,y in Z_n
            x[0] = x;
            for i = 1,2,3,... x[i] = x[i-1]^2 + y

     Observe that every iterate is a polynomial (with integer coeffs)
     in x,y.  For example

            x[2] =  x^4  + 2 x^2 y + y^2 + y.

     Because we have a functional iteration, there will be a collision.
     If you draw the picture, it looks like the Greek letter rho,
     with a tail and a cycle.

     A similar picture appears mod p, when p is a prime divisor of n.

   Floyd's cycle detector:

     Suppose we have numbers generated by iterating the function f.
     If the function maps X --> X, with X finite, there will always
     be a collision.  This can be found by moving two pointers along
     the "rho" graph.  The second one will take two steps for
     every step taken by the first one.

         a = x0
         b = f(x0)
         while a != b do
               a = f(a)
               b = f(f(b)

     There will be a "rho" subgraph starting at x0.  Let the
     tail length be t and the cycle length c.  After t iterations,
     both a and b will be in the cycle.  Then, relative to a, b
     moves forward one edge per cycle.  So with <= t more steps,
     we achieve a=b. 

  Back to Factoring

     In the rho method, we do not directly observe a,b mod p, but we 
     can compute gcd(b-a.n) at each step.  At the point of collision
     in Zp, we will have p | b-a.  What we hope is that 

           1 < gcd(b-a) < n,
 
     which then gives us a factor of n.

     The number of iterations needed to split n is *conjectured* to
     be O(sqrt p).  Since we can assume p <= n^{1/2}, this is
     O( n ^ {1/4} ).

     Although there is no rigorous analysis for Pollard's rho method,
     there is a deterministic algorithm (discovered independently by
     Pollard and Strassen) that uses n^{1/4 + o(1)} time *and* space.
     This is the best known rigorously proved running time for 
     deterministic factoring algorithms.
     
Lecture 21
3/6/17 Monday

  Today: Subexponential factoring algorithms

  J. D. Dixon, Math. Comp. 1981 (on course web page)

  All the factoring algorithms we've seen (trial division, Pollard rho)
  have run times that are exponential in log n.

  Importance of Dixon's alg: It has subexponential expected run time:

            exp( O(sqrt(log n loglog n)) ).

       This was the first published algorithm with provably 
       subexponential (expected) run time.  It does use 
       randomization.

  Difference of squares.

      Suppose that we know a,b for which a^2 == b^2 (mod n).  
      Then n | a^2 - b^2 = (a+b)(a-b), and we can see if

            1 < gcd( a +- b, n ) < n.

      Example: n = 77 = 7 * 11 

             13^2 = 169 = 154 + 15  == 15 (mod 77)
                           ^
                           |
                         2*77

             20^2 = 400 = 385 + 15  == 15 (mod 77)
                           ^
                           |
                          5*77

             gcd(30+13,77) = gcd(33,77) = 11.
             gcd(30-13) = gcd(7,77) = 7.

      If we tried to guess a,b, how effective would this be?
      Consider n = pq with p,q two large primes.  Then 

          |Zn*^2| = (p-1)/2 * (q-1)/2 ~ n/4.

      By the birthday problem, we need to consider O(sqrt n) random
      squares before we see a collision.

   Dixon's alg: Make quadratic residues with known factorization into
                small primes.  (Numbers that factor into small primes
                are called "smooth.")  Multiply some of these together
                to get a^2 == b^2.

       Example (continued):  n = 77

         13^2 = 169 == 15        =     3 * 5
         14^2 = 196 == 42        = 2 * 3     * 7
         35^2 = 1225 == 70       = 2     * 5 * 7

       Multiply these relations to get

         (13*14*35)^2 == (2*3*5*7)^2
       i.e.   6370^2  == 210^2

       a-b = 6160 = 7*880 (bad luck)
       a+b = 6580 = 7*940 (good, since 11 doesn't divide 94)

       gcd(a-b,n) = 7.

       This idea of combining smooth quadratic residues was known
       before (Morrison and Brillhart, Math. Comp., 1970) but Dixon's
       method was the first provably efficient one.

   How do we know which squares to combine?

       In the example, the factored quadratic residues can be
       represented by exponent vectors:

                 2  3  5  7
          13^2   0  1  1  0
          14^2   1  1  0  1
          35^2   1  1  1  1

       Note that each column sums mod 2 to 0.  In general, we will
       have vectors v_1,...,v_s with non-negative integer entries, and
       want to find a nonzero 0-1 vector x making 
       sum_i x_i v_i = 0 (mod 2)

       This is a linear algebra problem, over the finite field Z2.
       (Aka GF(2), or F_2.)

       Dixon's algorithm will not use the factorization of every quadratic
       residue it tests, only those that are B-smooth:

          Defn: x is B-smooth when each of its prime factors is <= B.

          Ex: (13^2 mod 77), (14^2 mod 77), (35^2 mod 77) are all 7-smooth.

    Dixon's algorithm (in detail):

      Input n>1, odd composite, not a prime power
      Choose smoothness bound B with 2 <= B < n.
      Let p1,...,pr be the primes <= B.  (If one of these divides n, done.)
      S = {}   (S is a set of relations)
      while |S| <= r:

           x = random element of Zn*
           y = x^2 mod n
           if y factors over the primes <= B, add x^2 == p1^e1 ... pr^er
                                              to S.

      (now |S| = r+1)
      Find a subset F of S whose exponent vectors sum to 0 mod 2.
      Multiply left and right sides of the relations in S to obtain
         a^2 == b^2 (mod n).
      See if 1 < gcd(a +- b, n) < n.

      The choice of B involves a tradeoff:

         Small B ==> easier to test B-smoothness, harder to find QR's
         Large B ==> harder to test B-smoothness, easier to find QR's.

    Theorem: At the end of the algorithm, Pr[n is split} >= 1/2.

    Proof: By the Chinese remainder theorem

             Zn* ~==~ Zq1* x Zq2* x ... x Zqm*,

    where n = q1*q2*...*qm is a factorization into different odd 
    prime powers.

    We will use the principle of deferred decisions. Imagine that we
    picked x in two steps: first by picking y, and then (after F has
    been found) , choosing a random square root of y for x.
    (This is allowed because squaring is a group homomorphism on Zn*.)

    Except for the choice of x in the loop, everything in the algorithm
    until the last two lines uses only the values of y.  So we will have

            b <--> (   b1,    b2, ...,    bm) 
            a <--> (+- b1, +- b2, ..., +- bm) 

    The signs (+ or -) are independent, and equally likely.
    This is because each Zqi* is cyclic of even order, so there 
    are exactly two square roots for any square.

    Since the qi are all odd, there are only two bad sign choices:
    the all + choice, and the all - choice.  So the probability of
    a bad choice is 2 / 2^m <= 1/2.
       
Lecture 22
3/8/17 Wednesday

    Continuing with Dixon's algorithm.  Our goal is to show that
    its expected run time is subexponential.  Remember that it
    splits n with probability >= 1/2.

    See last lecture for algorithm description, notation, etc.
    We will assume that n>1, odd, not a perfect power.

     Smooth numbers.

        Let Psi(z,B) be the number of B-smooth numbers in 1..z.
        p1,...,pr are the primes <= B.

        Psi(z,B) = # of ei >= 0 such that sum_i e_i log p_i <= log z

                 >= # of ei such that sum_i ei <= (log z)/(log B).

                                [since each p_i <= B.]

        Let lambda = (log z)/(log B), which we assume to be an integer.
        Then 

              Psi(z,B)  >= # of non-negative (r+1)-tuples summing to lambda
                         = (lambda + r choose lambda)
                        >= r^lambda / lambda^lambda.

        r = pi(B) >= B / log B (for suff lg B), and plugging this in
        we get

              Psi(z,B)          B^lambda / z 
              --------  >= --------------------------------
                 z         (log B)^lambda * lambda^lambda

                         = 1 / (log z)^lambda  [using defn of lambda]

     Smooth quadratic residues.

       This is the heart of Dixon's argument.  We want to know
       how many of the QR's in Zn* are smooth.

                      <--- d copies --->
       Zn* / Zn*^2 ~~ C2 x C2 x ... x C2.  So there are 2^d cosets.

       If x,x' are in the same coset, <= sqrt n, and B-smooth,

         xx' is a quadratic residue in Zn*.

         [Note: xx' != n since n>1, and isn't a square.]

       Let's try making smooth quadratic residues this way.

       Example: n = 35 = 5*7, sqrt(35) = 5.916...

         The units <= sqrt(n) are 1,2,3,4

         Group them into cosets using Legendre symbols:

              x     (x|5)    (x|7)

              1       1        1
              2      -1        1
              3      -1       -1
              4       1        1

         Then make all possible products of elements in the same
         coset:

                       coset
                       ++      -+     --

          coset ++     1  4     2      3

                1      1  4
                4      4  16

          coset -+

                2               4

          coset --

                3                       9

          (Note: this is like a Cartesian product but it is "fibered"
          in the sense that we only take pairs from the same coset.)

          Note that each entry y occurs <= tau(y) times, where
          tau(y) is the sum of y's positive divisors.
  
               E.g. tau(4) = 3, and 4 appears 3 times.

          This multiplicity is inconvenient for counting, so we 
          use 1/tau(x) as a weight on x.  This gives:

                       coset
                       ++                   -+            --

          coset ++     1 * 1   1/3 * 4    1/2 * 2      1/2 * 3

            1 * 1      1 * 1   1/3 * 4
          1/3 * 4    1/3 * 4   1/9 * 16

          coset -+

          1/2 *  2                        1/4 * 4

          coset --

          1/2 *  3                                      1/4 * 9

          Now, 

            total weight for 4 = 1/3 + 1/3 + 1/4 = 11/12 <= 1.

          This bound will hold for all QR's appearing in the array, 
          since tau is submultiplicative:

               tau(xx') <= tau(x) tau(x').

          [Proof: use tau(p1^e1*...*pr^er) = (e1+1)*...*(er+1).]

          Let 

              S_i = sum_{x<=sqrt n, B-smooth, in i-th coset} 1/tau(x)

              S = sum_i S_i.

          Then

           [# of B-smooth QRs] >= sum_i S_i^2       [total wt <= 1]
                               >= sum_i (avg Si)^2  [variance >= 0!]
                                = S^2 / 2^d         [algebra]
                                = (SS')^2 / ( S'^2 2^d ),

          where S' = sum_{x <= sqrt n, B-smooth} tau(x).

          Now we bound the bottom and top expressions:

            S' <= sum_{x <= sqrt n} tau(x) ~ sqrt n log(sqrt n)
                                           < sqrt n log n [n suff lg.]

               [For the average value of tau, see Hardy & Wright, Section
                18.2]
          
            Psi(sqrt n, B)^2 = ( sum_{x <= sqrt n, B-smooth} 1 )^2

               = (sum_{x <= sqrt n, B-smooth} tau(x)^{-1/2}*tau(x)^{1/2})^2

               <= (sum 1/tau(x)) * (sum tau(x))    [Cauchy-Schwartz]

                = SS'.

        Using these two bounds and phi(n) <= n, we get

         [# B-smooth QR's in Zn*]      Psi(sqrt n, B)^4       1
         ------------------------  >= ----------------- * ---------
             | Zn*^2 |                n log^2 n 2^d         n / 2^d

                      = (Psi(sqrt n,B)/sqrt n)^4 / (log^2 n).

    Expected Run Time

      Let

        P_B = Prob[a random element of Zn*^2 is B-smooth]
    
      The number of arithmetic operations used by the algorithm
      is upper bounded by a constant times

        P_B^{-1} B^2             +           B^3

             ^                                ^
             |                                |
        A smoothness test using         Gaussian elimination to
        trial division costs B.         find a,b.
        We expect to test 1/P_B
        numbers before finding a
        smooth one.  We need B
        smooth quadratic residues.

     This equals

          ( PB^{-1} + B ) B^2.

      Inside the parens, the first term wants B to be large (more
      smooth numbers), and the second term wants B to be small
      (less work for smoothness testing).  In "tradeoff" situations
      like this, we can try to make them equal.  So consider

         log P_B^{-1} <= lambda loglog z + 2 loglog n
                       ~ lambda loglog n + 2 loglog n   [z = sqrt n]

         log B         = log z / lambda = log n /(2 lambda) [defn of B]

       The leading terms match asymptotically when 

          lambda loglog n ~ log n / (2 lambda)

       i.e.

          lambda ~ 1/sqrt(2) * sqrt( log n loglog n )        (*)

       Choose (for each eligible n) a value of lambda in Z so that
       the sequence of lambda's has property (*).  The values
       of B (real) are then determined by the relation between B and
       lambda.

       Upon plugging back in, we then see that

        P_B^{-1} B^2  = exp( (3/sqrt 2 + o(1)) sqrt( log n loglog n ) )

          B^3         = same

       So the expected run time for one trial is as advertised.

       The constant isn't bad: 3/sqrt 2 = 2.121... .

Lecture 23
3/10/17 Friday

   Topic du jour: Quadratic Sieve
                  See Koblitz, A Course in Number Theory and Cryptography,
                  Section V.5.

   Quick review of Dixon random squares algorithm (last lecture)

   QS improves Dixon in two ways:

      1. Uses quadratic residues from Zn* that are about sqrt n in size.
         These are more likely to be smooth.

      2. The quadratic residues are successive values of a quadratic
         polynomial, so they can be factored en masse with a sieve
         process.

   Widely used for practical factoring.  However:

      3. Believed to reduce the exponent in Dixon's running time to a
         number close to 1.  No rigorous proof of any subexponential
         run time.

   Let n>1, odd, not a perfect power.  As with Dixon, the goal is to
   populate the congruence a^2 == b^2 mod n.

   A Quadratic Polynomial

     Let f(t) = (t + [sqrt n])^2 - n.   This belongs to Z[t].

         If sqrt n = [sqrt n] + epsilon, with 0 < epsilon < 1,
         some algebra shows

            f(t) = t^2 + 2(t - epsilon)[sqrt n] - epsilon^2.

         If t is small (t = n^{o(1)), then f(t) < n^{1/2 + o(1)}.

         The first form of f shows that it is an increasing function
         for t >= 0.

         Also, for every t in Z, f(t) == (t + [sqrt n])^2 mod n,
               so f(t) is a quadratic reside for every t.

   An Example

      n = 247=13*19, [sqrt n] = 15, f(t) = (t+15)^2 - 247.
      
      t   f(t)     factorization

      1      9       3   3
      2     42    2  3             7 
      3     77                     7    11
      4    114    2  3                              19 
      5    153       3   3                      17
      6    194    2                                       97
      7    237       3                                    79
      8    282    2  3                                    47  
      9    329                      7                     47  
     10    378    2  3   3   3      7
     11    429       3                  11 13  

     The columns exhibit some periodicities.  For example:

              2 | f(t) iff t is even
              3 | f(t) for t == 1,2 mod 3
              5 never appears

     This is because

              f(t) == (t+1)^2 + 1 == t^2 == t  mod 2

              f(t) == t^2 - 1 mod 2

              An odd prime p with (n|p) = -1 will never divide
              f(t).  Consider p=5, which has (247|5) = (2|5) = -1.
              Mod 5, f(t) == t^2 - 2 has no zeroes.

     More generally, if (n|p) = -1, (t+[sqrt n])^2 !== n (mod p).

    The periodicity means that we can use a sieve to factor the values
    of f.

        t =  1  2  3  4  5  6  7  8  9 10 11 12
                *     *     *     *     *     *   <- divide by 2
             *  *     *  *     *  *     *  *      <- divide by 3
             etc.

    The QS Algorithm

        M = sieving parameter to be chosen later
        A[t] = f(t) for t = 1..M

        for all p <= B with (n|p) = +1  /* we're done if (n|p) = 0 */

            for all e <= log_p A[M]     /* A[M] = max f(t) we'll need */

                t0, t1 = zeroes of f mod p^e

                k = 0
                while t0 + k p^e <= M
                                      divide A[x0 + kp^e] by M
                                      k++
                do same for t1, if t1 != t0.

       At the end of the algorithm, A[t] = 1 indicates a value of t
       for which f(t) is completely factored.

       Note that:

         Finding zeroes of f mod p is easy (use Cipolla's algorithm, see
                                            lecture 2/6/17)

         Going from a zero of f mod p^{i-1} to a zero mod p^i is easy
               (discussed in lecture 2/8/17).

         Rather than divide A[t] by p, we can start with log A[t]
         in floating point and subtract log p.  We then look for
         t for which A[t] is close to 0.  This gets the work of the
         inner loop down to a few add/subtract operations.

         It will turn out that the main work of the algorithm is 
         in identifying the values of t for which f(t) factors.
         Once this is done, they can all be factored by trial division
         in time that is negligible compared to the rest of the algorithm.

    Run Time Analysis:

      Theorem (Canfield, Erdos, Pomerance, J. Number Theory, 1983):

        Let z,B --> oo subject to log^2 x <= B <= z.  Let
        lambda = (log z)/(log B).  Then

          Psi(z,B) / z  = lambda ^ {-lambda + o(lambda)}.

        We will assume, without proof, that this asymptotic relation
        is valid for the quadratic residues (= values of f(t)) used
        in the algorithm.  Practical experience is consistent with this.
        
      Choose M = n^{o(1)}.  Our analysis will ignore factors that
      are polylog in n or smaller.

         A[M] <= 2M sqrt(n), so
         log A[M] <= log n [n suff. large]

         # of roots to find <= 2 sum_{p <= B} log_p A[M} 
                             = O( log n) sum_{p <= B} 1/(log p)

              By the prime number theorem the sum is ~ B, so this is
              roughly B (we drop the log factor)

         Cost to sieve: M sum_{p^e <= B} 1/p^e ~ M loglog B, about M
                        (again using the prime number theorem).

         Length ratio lambda = log(sqrt(n))/(log B)
                             = (log n)/(2 log B),

                   so B = n^{1/(2 lambda)}.

         Total cost:

              n ^ {1/(2 lambda)} lambda^lambda +   n^{3/(2 lambda)}

                  ^                    ^                ^
                  |                    |                |

              sieving, root     avg distance btwn    linear
              finding           smooth f(x)'s        algebra
                                

         If we set the two terms equal, we get

                lambda ~ sqrt( (2 log n)/(loglog n) )

         so the run time is

                exp( (9/8 + o(1)) sqrt( log n loglog n ) ).

         Note that 9/8 = 1.125, so sqrt(9/8) is about 1.06.
         This is much better than our bound for Dixon's algorithm,
         but does not come with a rigorous proof.
      
Lecture 24
3/13/17 Monday

   Is the quadratic sieve the last word on factoring? No.

      1. Subsequent work (Pomerance and Lenstra) led to a randomized
         factoring algorithm algorithm with expected run time 
         exp((1+o(1))sqrt( log n loglog n )).  It is not considered
         practically competitive.

      2. The number field sieve, an algorithm that exploits algebraic
         number theory, is believed to have run time

           exp((c + o(1))(log n)^{1/2}(loglog n)^{2/3}).

         This has been the algorithm of choice for all recent record-breaking
         factorization efforts.
         algebraic integers

   Intro to discrete logarithms

     G = group, a,b in G.  If a^x = b for some integer x, we call x
     a discrete logarithm (or index) for b, relative to the base a.

       Usually, |G| < oo, and we reduce x modulo the order of a.
       The order of a is the least m >= 1 for which a^m = 1.
       Making 0 <= x < m makes x unique.

       Algebraic properties of discrete logs are similar to ordinary
       logs in R, for example

           log_a(b1 b2) = log_a(b1) + log_a(b2).

       Just as with ordinary logarithms, discrete logarithms were tabulated
       and used to shorten computations.  Jacobi's, Canon Arithmeticus
       (1839) did this for all prime powers <= 100.

     Difficulty of DLP depends dramatically on what the group is and
     how it is represented.

        Examples: G = (Zn, +).  The equation to solve is ax == b (mod n).
        We can use the extended Euclidean algorithm.

        G = Fq*, where Fq is a finite field with q elements.
        Fq* is cyclic.  As we'll see, this is easy when q-1 is
        smooth, and thought to be hard otherwise.

        If order(a) <= m, we can always try to solve a^x = b by brute
        force: compare a, a^2, a^3, ... to b.  Worst case: about
        m group operations.

     Baby step / giant step algorithm (Shanks 1969)

        For simplicity assume that m is the exact order of a.
        Want to solve a^x = b, 0 <= x < m.

        Choose r >= sqrt m.  Split x by writing it in base r:

           x = x0 + x1 r,  0 <= x0, x1 < r.

        Then

           a^x = b iff a^x0 = b (a^{-r})^x1

        So we form two lists:

            B = { (a^x0, x0, 0) : 0 <= x0 < r }
            G = { (b(a^r)^x1, x1, 1) : 0 <= x1 < r }

        Combine them and sort on the first entry.  When we get
        a collision (with different third components), we have

                    x^x0 = (a^{-r})^x1,

        so a^{x0 + x1 r) = b.

        To explain why collision is inevitable, Shanks drew a circular
        diagram that has become part of our field's folklore.  I'll
        unroll it so it looks better in this format.

           baby
           steps:
          |///////|-------------------------------------> [wrap around]

                                      giant steps:
                         <--- ...  |       |       |

        Due to our choice of r, the giant steps cannot skip over the
        interval of baby steps.  

        Run time analysis: Charge 1 time unit for group operations
        (*, testing for equality).

                 a^{-1} = a^{m-1}    is O(log m)
                 a^{-r}              is O(log m)
                 B                   is O(r) = O(sqrt m)
                 G                   is O(r) = O(sqrt m)
                 sorting             O(r log r) = O(sqrt m log m).

        The time (as well as the space) needed for this algorithm
        is m^{1/2 + o(1)}.

    Enhancement by Pohlig and Hellman (IEEE Trans. Inf. Thy, 1978).

        Suppose we know the exact order m and its prime factorization:

                          m = q1^e1 * ... * qr^er.

        Then <a> (group generated by a) is isomorphic to

                C_m ~==~  C_{q1^e1} (x) ... (x) C_{qr^er}.

        (by the Chinese remainder theorem: C_m is just the additive
        group of Z_m.)  Let mi' = m/product_{j != i} qj^ej.  Then
        a projection pi_i onto the ith direct factor is given by

                z --> z^{mi'}.

        To solve a^x = b, we can apply pi_i to both sides and get

               pi_i(a)^x == pi_i(b),     i=1,...,r

        which is r discrete log problems in the smaller groups of order
        qi^ei.   If xi is the solution to the i-th such problem,
        a solution to our original problem is given by any x
        that satisfies

               x == xi (mod qi^ei),   i=1,..,r.

        This can be found using the Chinese remainder theorem.

        For exponents exceeding 1, a further reduction is possible.
        A cyclic group of order q^e has a chain of subgroups

             1 [ C_q [ C_{q^2} [ ... [ C_{q^e},

        where each quotient group C_{q^i}/C_{q^{i-1}} is cyclic of
        order q.    We can then use baby-step / giant-step to compute
        each digit in the base-q expansion of the discrete log, say

             x = x0 + x1 q + x2 q^2 + ... + x_{e-1} q^{e-1}.

        This is done from left to right.  For example, taking both
        sides of a^x = b and raising to the power q^{e-2} gives us

            a^{x q^{e-1}} = (a^{q^{e-1}})^x0 = b^{q^{e-1}},

        which is a discrete log problem in a group of order q.  Once
        we know x0, we set y = x-x0, and solve (recursively) 

            a^y = a^{-x0} b,

        which is in a group of order q^{e-1}.

        Consequently, the cost to solve a^x = b is Q^{1/2 + o(1)},
        where Q is the larges prime factor in m's prime decomposition.

Lecture 25
3/15/17 Wednesday

    Kangaroos

    J. M. Pollard, Math. Comp. 1978.

    Assume G is cyclic, generated by a, and |G| = m.
    The next algorithm assumes we know m (or at least a multiple of it).

    Let f : G --> Zm be a pseudo-random function.  

        Example: When a generates Zp* (so m = p-1), Pollard suggests

                      cx,    if 0<x<p/3
              f(x) =  x^2,      p/2 < x < 2p/3.  
                      ax        2p/3 < x < p,

        where c is some fixed element of Zp*.  The expressions on the
        right are to be taken mod m.  [Could f(x) ever be 0?  This
        seems possible but unlikely.]

    Each element x in G has a *successor* x' = x a^f(x).

    We will think of f(x) as a distance from x to x'.  (Not necessarily
    the shortest distance, but one that does the job.)  

    The method:

       Choose a random starting point a^r, and follow successors n
       time:

          x0 ------> x1 ------> x2 ------> x3 ------> ... -------> xn
        = a^r      = x0'      = x1'      = x2'            = x{n-1}'

       Pollard calls this the *kangaroo trap*.  Observe that 

            xi = a^{ r + f(x0) + ... + f(x_{i-1} }

       We remember the final element xn, and the accumulated distance
       from x0.

       Choose another starting point y0 = ba^s (s is random), and iterate
       similarly:

          y1 = y0',  y2 = y1', ...

          As we create y we update its distance from b.

       We hope that some yj lands in the trap (that it, it equals xi for
       some i, 0<=i<= n.  Pollard draws a picture for this:

            x0 --------------> jy = xi ------------------> xn
                                  ^
                                 /
                                /
                               /
                              /
                             /
                            /
                           /
                          /
                         y0

            (like a Greek letter lambda)

        Once it is in the trap, it will follow the x's.  Eventually
        yk will hit xn.  At that time we know:

            Distance from y0 (= ba^s) to xn
            Distance from element x0 (with known discrete log) to xn
            So we can get log a by subtraction.

        In formulas:

            yk = b a^{x + sum_{j=0}^{k-1} f(yj) }
         =  xn = a^{r + sum_{i=0}^{n-1} f(xi) } 

         ==> b = a^{r + sum_i f(xi) - s - sum_j f(yj) }

                            ^
                            |
                      reduce this mod m to get soln to a^x = b.

        How big should n be?  No rigorous analysis is known.

        Let's argue heuristically, and pretend that the yj's are
        i.i.d. uniform on G.  Under this assumption,

           Pr[y0,...,yk avoid the trap] = (1 - (n+1)/m)^{k+1}
                      <= exp( -(n+1)(k+1)/m }.   [using (1-x/n)^n <= e^{-x}.]

        If n,k >= floor(sqrt(m)), n+1, k+1 are >= sqrt m, so this
        gives

           Pr[ avoid the trap ] <= e^{-1} = 0.37 (approx.)

    Comparison with Shanks algorithm:

        Shanks is deterministic.  This uses randomization (to set
        the starting points).

        Shanks is fully proved.  This has not been rigorously analyzed.

        Shanks uses m^{1/2 + o(1)} time *and* space.   This 
        has the same time bound, but uses space for O(1) group
        elements.

Lecture 26
3/17/17 Friday

    A subexponential algorithm for discrete logs in Zp*
    L. Adleman, Proc. IEEE Foundations of Computer Science, 1979.

    Background:

       Western and Miller (1968) discussed a method for discrete logs
       based on multiplying congruences.  This came to be called
       index-calculus, since "index" is an old fashioned work
       for the discrete logarithm.  The name has stuck, and applies
       to any algorithm that is based on combining elements with
       smooth representations.

       Adleman's contribution was twofold: 1) A full statement
       of the procedure, and 2) Analysis indicating that its run
       time is L(p){c + o(1)}.  Here we define

             L(p) = exp(sqrt(log p loglog p)).

       Here is the algebraic context.  Let B, with 2 <= B < p
       be a smoothness bound, and p1,...,pk be the primes <= B.
       These primes generate a subgroup S of Q* (multiplicative group
       of the nonzero rational numbers).  The algorithm exploits
       the group homomorphism

                     S ---> Zp* .

   Reduction to prime powers

       We want to solve a^x = b, where a is a generator for Zp*.
       Using Dixon's algorithm, we can do factoring within our time
       bound, so let

           p-1 = q1^e1 ... qm^em,   qi distinct primes.

       (Note: It is implied in the paper that the Morrison-Brillhart 
       factoring algorithm has a rigorous analysis, but this is an
       open problem.)

       It is enough to determine xi = x mod qi^ei, i=1,...,m.  
       To recover x, we can use the Chinese remainder theorem to solve

             x == xi (mod qi^e1),  i=1,...,m.

   Linear equations mod prime powers

       Fact.  Let E (e_ij) be a k x k matrix with entries from Zq^e, where
              q is prime.  Then the rows of E generate (Zq^e)^k mod q^e
              iff they generate mod Zq^k mod q.

       Proof: <== is the interesting part.  By hypothesis, 
              det(e_ij) is invertible mod q.  Pick

                b = (b1,...,bk).

              Then there is an x with xE == b (mod q).
              We now attempt to find a y such that

                (x + yq)E == b (mod q^2).

              This holds iff 

                q  yE == b - xE (mod q^2), 

              so it is sufficient that

                yE == [(b - xE)/q] (mod q),

              where the expression in brackets is evaluated mod q^2.
              So we can take

                y == [(b - xE)/q] E^{-1} (mod q).

              Repeating this process, we get preimages of x mod
              q^3, q^4 ,..., q^e.

         Note: This is an instance of (or at least closely related to)
         Nakayama's Lemma, which is a staple of graduate
         algebra courses.

    Computing the discrete log mod q^e

         Let q^e be a maximal prime power divisor of p-1.
         Let 2 <= B < p, p1,...,pk the primes <= B.

         Part 1:

           E = {}  /* list of row vectors */
           repeat

              r = random number from 0..p-2
              try to factor (a^r mod p) = p1^e1 ... pk^ek in Z.
              if successful, and rank(E + (e1,...,ek)) > rank(E),
                  append (e1,...,ek) to E. /* rank is over Zq */

           until rank(E) = k.

         Part 2:

           choose random s in 0..p-2
           until (ba^2 mod p) = p1^f1 ... pk^fk in Z.

           solve (x1,...,xk) E = (f1,...,fk) mod q^e

           Then, a^{sum_i xi ri} = b a^s * a^c, where c == 0 mod q^e.

           This implies sum_i xi ri == log_a(b) + s  (mod q^e), so

               log_a(b) == sum_i r_i x_i - s (mod q^e)

         Part 1 can be thought of a building an algebraic version of
         the kangaroo trap, and Part 2 launches kangaroos toward the
         trap. (The method has not been vetted by the University's
         Animal Experiment Control Board.)

     With probability 1, Part 1 will terminate.  This is because
     the primes p1,...,pk are smooth, and we have a nonzero probability
     to pick each one of them.

    Run Time Analysis

     We ignore factors of order log p and smaller (including
     constants).

     We will assume that Gauss-Jordan elimination is used for
     the rank test.  In particular, when E has j rows, we have
     generators for the row space in echelon form, say

          1 * * * * * ... *
          0 1 * * * * ... *      (j rows)
          0 0 1 * * * ... *
          0 0 0 1 * * ... * .

     When a new exponent vector v arrives, we can use back substitution
     to find the unique linear combination of these row vectors 
     that matches the initial j components of v.  Subtracting these
     off, we then see if the nonzero components remaining include
     any element of Zq*.  If so, we can add v this to E.
     With this proviso, each rank test costs B^2, since k <= B.

     To factor an element by trial division costs B.

     At each point, the rows of E generate a subspace S of Zq^k.  
     If S != Zq^k, the probability that a new vector (e1,...,ek) is
     outside the subspace is >= 1 - 1/q >= 1/2.  (We adopt the
     heuristic assumption that the exponent vectors we make are
     random enough to make this true.)

     Therefore, the expected number of r's used to extend the rank
     is <= 2 P_B^{-1}, where

          P_B = Pr[a random draw from 1..p is B-smooth].

     Since p isn't in the factor base, this is the same as

              = Pr[a random draw from 1..(p-1) is B-smooth].

     So we get (in expectation)

        Cost for trial division               P_B^{-1} B^2

        Cost for rank tests                   B^3

        Cost to get one smooth ba^s          P_B^{-1} B.

        Cost for final linear algebra         B^3.

     Let's make the trial division cost small.

        Let lambda = (log p)/(log B)

        Canfield-Erdos-Pomerance: P_B = lambda^{-lambda + o(lambda)}

        Consider f(lambda) = lambda^lambda B^2.

                 g = log f = lambda log lambda + 2 log B

                 g' = log lambda + 1 - 2(log p)/(lambda^2) = 0

                    when lambda^2 ~ sqrt(2(log p)/(log lambda))

                         ==> log lambda ~ (1/2) loglog p.

        So we will take

              lambda = 2 sqrt( (log p)/(loglog p) ).

        This implies

               log B = log p / lambda = (1/2) sqrt (log p loglog p).

               lambda log lambda ~ sqrt(log p loglog p).

        Plugging back in we get:

        Trial division         P_B^{-1} B^2 = L(p)^{2 + o(1)}

        Rank tests             B^3          = L(p)^{3/2 + o(1)}
        Final linear algebra

        Finding smooth ba^a    P_B^{-1} B.  = L(p)^{3/2 + o(1)}

   Rigorous Analysis

      To get a fully justified run time, we can follow Adleman
      and interleave building the trap with testing for a solution.

      Let us do the following for j=1,...,2k:

           By trying random s, find s for which wj = ba^s mod p is smooth.
           If its exponent vector w is dependent mod q on previously
                collected exponent vectors, save (s, ba^s, wj) for later.
           Similarly obtain a value of r so that vj = a^r mod p is smooth, 
                and add it to the collection.

      Then, if any w's were saved, we can finish the algorithm as
      before.

      Suppose that v1,v2,...,v2k are the exponent vectors in the final
      collection.  Let 

                     1 if vj depends on v1,...,v{j-1} mod q
               X_j =
                     0 if not.

      By linear algebra, sum_j I_j >= k.  But I_j has the same
      distribution as

                      1 if w depends on v1,...,v{j-1} mod q
               Y_j =
                      0 if not.

      So sum_{j=1}^{2k} E[Y_j] >= k, and since there are 2k non-negative
      terms, at least one of them must be >= 1/2.

      This shows that the probability of success is >= 1/2.

      We can analyze the run time as before, and get L(p)^{2 + o(1)}
      as a fully proved bound on the complexity.

3/20-3/24 NO CLASS -- SPRING BREAK

Lecture 27
3/27/17 Monday

   Coppersmith's Algorithm -- IEEE-IT, 1984.


Lecture 28
3/29/17 Wednesday

   Finish Coppersmith's algorithm

   Berlekamp polynomial factoring algorithm (briefly)

      We want to factor f(X) in the ring Z2[X].   We can assume
      that f is monic (leading coefficient 1) and has positive 
      degree.  Let

           f = g1^e1 ... gr^er,   where the gi are distinct monic
                                  irreducibles.

      By the Chinese Remainder Theorem

        R := Z2[X]/(f) 
          ~=~ Z2[X]/(g1^e1) (+) Z2[X]/(g2^e2) (+) ... (+) Z2[X]/(gr^er).
                                  
                     U               U            ...           U

                   GF(2^d1)      GF(2^d2)                    GF(2^dr)

                     U               U            ...           U

                   GF(2)           GF(2)                      GF(2)

      R is a ring and a vector space over GF(2).  In R, the squaring
      map is GF(2)-linear (use the freshman binomial theorem:
      (a+b)^2 = a^2 + b^2.).

      To factor f, we can use linear algebra to find a nonconstant
      solution to a^2 - a = 0.  What do the solutions look like?
      Any polynomial h in Z2[X]/(g^e), g irreducible, can be written
      in "base g" notation:

              a = a0 + a1*g + a2*g^2 + ... ,    deg ai < deg g

      If a^2 = a, then a0^2 == a0 (mod g), so (since polys mod g are
      a field) a0 = 0,1.  Using this, we get a1=a2=... = 0.

      So if a^2 = a in R,

            a <---> (0/1, 0/1, ..., 0/1).    [0/1 means "is 0 or 1".]

      If a is nonconstant, not all components will be different, so
      gcd(a,f) splits f.

      If f has degree d, the run time for this is O(d^3), which is
      polynomial in d.

Lecture 29
3/31/17 Friday

   New Topic: Discrete Dynamics and Pseudo-Random Number Generation

   References: Knuth, ACP v. 2, Chapter 3.
               Marsaglia, Proc. AMS Symp. Appl. Math, 1992

   Dynamics

         X = set, f : X --> X.  This induces a digraph whose edges
         are x --> f(x).   For example: 

              X = Z5
              f(x) = x^2 + 1
              Graph is the 3-cycle (012) plus two edges pointing into it,
              at 0 and 2.

         In an iteration digraph, each vertex has out-degree 1.

         When |X| < oo, each component will be a cycle, with some
         trees rooted at vertices of the cycle.  Note that the edges
         in the tree point toward the root.  More than one cycle
         is possible, e.g. the identity map on X has |X| 1-cycles.

         Defn: If h is a permutation on X, the map

                     g(x) = h^{-1}( f(h(x)) )

               is *conjugate* to f.

         Conjugation preserves all the features of the iteration graph.

         If X = X1 x X2 x ... x Xr is a Cartesian product, and
         f acts component by component (meaning that there are maps
         fi : Xi --> Xi making f(x1,...,xn) = (f1(x1),...,fr(xr)) ),
         then the period of f is the least common multiple of the
         periods of the fi's.

    Pseudo-random numbers

         f : X --> X, |X| < oo.  The starting point x0 is called
         the *seed*.  

         g : X --> Y is called the *output function*.  This can be
         trivial: Y=X and g is the identity function.

         If n random numbers are needed, we generate 

           x0, x1 = f(x0), x2 = f(x2), ..., xn = f(x{n-1})

         and then deliver y0, y1, ... y{n-1} to the user, where
         yi = g(xi).

         Typically, the user does not see the xi's, only
         the result of applying g to them.  The sequence of y's
         will be ultimately periodic, because the sequence of x's is.

    Design goals:

         1) The output should "look random":

              Pass some statistical tests
              Have a long period (a true random sequence will have
               no period, with probability 1)
              No obvious biases over a full period

         2) The output is difficult to predict from previous outputs.

              This is good to have in an adversarial situation.  Imagine,
              for example, that you needed a pseudo-random generator for
              an electronic roulette wheel.

         3) Outputs should be easy to compute

         4) The generator should be easy to reason about.

         Of course these are in conflict.  1)--2) say the generator is
         complex, whereas 3)--4) say it is simple.  This makes the
         selection of a generator more like a real-world engineering
         problem than a purely mathematical one.  Different users will
         have different requirements.

   Linear Congruential Method (D.H. Lehmer, 1950s):

         X (state space) = Zm

         If you want random numbers in [0,1) use the output function
               g: Zm-->Q given by x --> x/m.

         f(x) = ax + b mod m     (called an affine map)

            We need O(1) operations in Zm to compute the next iterate.
            Commonly, m fits in a single machine word.

         Explicit formula for the iterates:

            x0 = seed
            x1 = a x0 + b
            x2 = a^2 x0 + (a+1)b
            ...
            xn = a^n x0 + (a^{n-1} + ... a+1)b

               = a^n x0 + (a^n - 1)/(a - 1) if a-1 is in Zm*.

         The iteration is reversible iff a is in Zm*.  

         Period <= m (since |X|=m).  Can it be this large?  Yes,
         x --> x+1 (mod m) has period m.

         Which sequences have maximum period?  Use the Chinese
         remainder theorem:

            Zm ~==~ Z_{p1^e1} (+) ... (+) Z_{pr^er}

         and conclude that x --> ax + b has maximum period mod m
         iff its period is maximum for each prime-power divisor of m.

         So we consider x --> ax + b, mod p^e.  When does this have
            period p^e?

            Necessary condition: p does not divide p.  (If p | b,
            the sequence starting from x0=0 has all iterates divisible
            by p.  In that case the period is <= p^{e-1}.

            Assume b is prime to p.  Since 

                        ax + b = b[ a(b^{-1} x) + 1],

            the map is conjugate (by multiplication) to the "twist and
            increment" function 

                        ax + 1.

            So we may as well assume b=1.  To get period p^e, it is
            necessary that p not divide a.  (If it did, then mult
            by a would not be 1-1, giving the iteration graph a
            branch.)

            If a !== 1 mod p, the iterates are

                         a^i - 1
                   xi = ---------
                          a - 1

             whose period is the order of a mod p^e.  This is at
             most phi(p^e) = (p-1)p^{e-1}, which is less than p.

         Therefore, two necessary conditions for full period are that
         a, b be relatively prime to p.

         When p=2 and e>=2 (power of 2 modulus 4 or higher) there is
         an extra condition that a == 1 (mod 4).  To see why this
         is needed, suppose that a = 4k+3.  Then (again for x0=0),

                         a^i - 1     a^i - 1
                   xi = --------- = ---------
                          a - 1     2(2k + 1)

         So mod 4, a^i     is  -1 +1 -1 +1 -1 +1 ...
                   a^i - 1 is   2  0  2  0  2  0 ...

         Then, the sequence mod 4 is

             (2k+1)^{-1}  times 1  0  1  0  1  0 ...

         which only takes 2 values.

       In the next lecture we will show that these conditions on a,b
       are sufficient as well.  This result was published by Hall
       and Dobell [SIAM Review, 1962].

Lecture 30
4/3/17 Monday

   p-adic numbers (briefly)
   Lehmer generator maximum period criterion
   Iterated linear transformations

   p-adic numbers

     We go from Q (rationals) to R (reals) by "filling in the holes".
     This gives a nice intuitive picture of a real number as an unending
     sequence of digits:

         sqrt 2 = 1.41421356237...

     This is ultimately based on a notion of distance.  For example,
     sqrt 2 is the length of a diagonal on the unit square.

     At the end of the 19th century, Kurt Hensel introduced a new notion
     of distance that is useful for number theory problems.  [Trivia:
     he was a second cousin to the composer Felix Mendelssohn, which
     makes him related by marriage to Dirichlet.]

     Let p be a prime.  Define

        v_p(a) = # of p's in a's prime factorization (if a in Z, a != 0)

        v_p(b/c) = v_p(b) - v_p(c), if b/c is a nonzero rational number

     If x is in Q, let

                0, if x=0
        |x|_p =
                1/p^{v_p(x)}, if x != 0.

     The (p-adic) distance from x to y is |x-y|_p.  This obeys the standard
     properties of a distance function, including a strong version of
     the triangle inequality:

       |x-z|_p <= max{ |x-y|_p, |y-z|_p }.

     Therefore we will think of x as "close" to y if the difference x-y
     is divisible by a high power of p.

     In a manner similar to constructing the reals, we can fill in the
     gaps to make the p-adic numbers Q_p.   For example:

       p=7   sqrt(2) = ...21216213.0

       Note that the base 7 digits go from right to left.  The infinite
       sequence should be thought of as abbreviating a whole sequence
       of congruences modulo increasing powers of p.  You can check
       that

            3^2 == 2 (mod 7)
            (1*7 + 3)^2 == 2 (mod 7^2)

       More digits, if wanted, could be found using the Newton iteration
 
          x' = (1/2)(2 + x/2),

       where now, the approximation is 7-adic.

       Computationally,

          +,-,* work as in grade school

          Inversion requires a new algorithm.  We could, if we like
          use Newton iteration:

                 x' := x(2 - a*x)

  Lehmer's Linear Congruential Generator

      Recall:   x0 = seed, x_i = a x_{i-1} + b (mod m)

      To obtain the period, we can analyze the case m=p^e, and
      apply the Chinese remainder theorem to get the period mod m.

      Hull-Dobell max period criterion:  When m=p^e, the period
      equals the maximum possible value (p^e) iff:

           p doesn't divide b

      and
                  1 mod p,     p odd
           a ==   1 mod 4,     p=2, e >= 2
                  1 mod 2,     p=2, e=1.,

      The theorem is saying that the maximum period iterations are those
      that are close, p-adically, to maximum-period translations

          x |---> x + b,    b not divisible to p.

      Last time we proved (==>).  Now for (<==).  Assume p is odd (p=2
      can be done as an exercise.)

          If b != 1, the map is conjugate to x --> ax+1.  (Last time.)

          Let a = 1 + eps, p|eps.  We can consider the sequence with x0=0.
          Then,

          xk = (a^k-1)/(a-1) = ((1+eps)^k - 1)/eps

                             = sum_{i=1}^k (k choose i) eps^{i-1}

                             = k + sum_{i-1}^k k (k-1 choose i-1) eps^{i-1}/i

            For i>=2, v_p(eps^{i-1}/i) = (i-1)v_p(eps) - v_p(i)
                                      >= (i-1) - log_p(i) > 0.

                         (The extremal case is p=3, i=2.)

            On the other hand, this is an integer, so we conclude that

             xk = k (1 + [# divisible by p]).

            (This makes sense.  After all, for the increment sequence
            xk *is* k, so when we perturb it a little bit we should
            get something close to k.  The magic of p-adic analysis 
            guarantees that the errors don't pile up, the way they
            would with a floating point calculation.  So we get a nice
            bound for the relative error in the approximation xk ~~ k.)

            This implies that the period divides p^e (since x_{p^e}
            is congruent to 0 mod p^e), but it is not smaller
            than that (since x_{p^{e-1}} == p^{e-1} mod p^e).

            Done!

   Iterated Linear Maps

     Since we are going to need polynomials, we let
                   
         S [state space] = GF(2)^n   (length n 0-1 vectors). 

         A [iteration function] = GF(2) linear map from GF(2)^n to itself

         So we can think of A as an nxn 0-1 matrix, whose determinant
            is odd.

     In practice, we would like A to be sparse.  Traditionally, there
     are two ways to accomplish this.

       1) Linear feedback shift registers.

          Choose an irreducible monic polynomial 

             f = X^n - c{n-1} X^{n-1} - ... - c1 X - c0.

          (If you replace 2 by an odd prime, you need the minus signs.)

          Define a bit sequence {x_i}_{i >= 0} by

              x0, x2, ..., x_{n-1}  :  initial values

              For t >= n

                x_t = c{n-1} x_{t-1} + ... + c1 x_{t-(n-1)} + c0 x_{t-n}

          This has a simple digital circuit realization.  Here is
          an example.  Each [ ] represents a cell that can hold one
          bit.

             f = 1 + X + X^3

             [ ] <--- [ ] <--- [ ]<--|
              |        |             |           
              v        v             |
             [ + mod 2 ] -------------

             For any device like this, the state 00...0 is a fixed point.
             This one has the maximum possible period 7 = 2^3 - 1.

             The states evolve as follows:

                    0 0 1
                    0 1 0
                    1 0 1
                    0 1 1
                    1 1 1
                    1 1 0
                    1 0 0

              The original sequence can be recovered using the output
              function that sends a state vector to the leftmost bit.

          By analogy with the Fibonacci sequence, these devices are
          sometimes called Fibonacci shift registers.

          Note that the feedback is many-to-one.

       2) Galois shift registers.

          Choose f as above.  We now think of S as the finite ring

             GF(2)[X]/(f)       [ = GF(2^n) when f is irreducible ]

          and the state evolution function is multiplication by X.

          This can also be realized easily using standard digital parts.
          For example, to multiply by X modulo 1 + X + X^3, we can
          use

                 a              bX      cX^2

           |--> [ ] --> (+) --> [ ] --> [ ] --|
           |             ^                    |
           |             |                    |
           ------------------------------------

          Here, the feedback is one-to-many.

          (In general, you need an XOR in between pairs of cells
          for which the corresponding coefficient of f is != 0.)

          Starting with the same initial load, the sequence of states
          is

                0 0 1
                1 1 0
                0 1 1
                1 1 1
                1 0 1
                1 0 0
                0 1 0

          Again the period is 7.

Lecture 31
4/5/17 Wednesday

   Continuing with Pseudo-Random Numbers

   1. Lehmer generator: state space Zm, iteration x' = ax+b (mod m)

   2. Iterated linear transformations: 

         state space GF(2)^n  (length n 0-1 row vectors)
         iteration x' = xA (n is a nxn matrix w/ entries in GF(2))

       A lot of recent designs do this.  Example: Mersenne Twister
       (Matsumoto and Nishimura, ACM Trans. Modeling and Simulation,
       1998).  This has n=19937 (a prime).  An output function
       uses 32 bits from the state.

   Two standard architectures for scheme 2:

       2a) Fibonacci shift register.  

         Example

             f = 1 + X + X^3

             [ ] <--- [ ] <--- [ ]<--|
              |        |             |           
              v        v             |
             [ + mod 2 ] -------------

       
             (x1 x2 x3)' = (x1 x2 x3)( 0 0 1 )
                                     ( 1 0 1 )
                                     ( 0 1 0 )

         The contents of any cell (say the first) obey the recurrence 
         relation

            x_t = x_{t-2} + x_{t-3}

          Characteristic polynomial:

             X^3 - X - 1.

        2b) Galois shift register.

         The idea is to implement multiplication modulo some degree n
         polynomial f.  We'll call this the characteristic polynomial
         as well.

         Example: f = X^3 + X + 1.

               a              bX      cX^2

           |--> [ ] --> (+) --> [ ] --> [ ] --|
           |             ^                    |
           |             |                    |
           ------------------------------------

         As an iterated linear transformation this is

             (x1 x2 x3)' = (x1 x2 x3)( 0 1 0 )
                                     ( 0 0 1 )
                                     ( 1 1 0 )

      The state evolution matrices for 2a) and 2b) are transposes.
      This always happens.  Because of this we can analyze both
      devices at once.

      Theorem (A. A. Albert, Modern Higher Algebra, p. X): A and A^T are
      similar.  (This means there is some invertible matrix P for which

              A = P^{-1} A P.

      Proof: We will rely on a theorem of linear algebra (appears in 
      L.E.  Dickson, Modern Algebraic Theories, 1926, art. 51)
      The canonical form for similarity (over a field) is obtained
      by taking A - lambda I (lambda is an indeterminate, so this
      matrix has polynomials for the entries) and then doing row and
      column operations to put it in the form

          (d1             )
          (  d2           )
          (    d3         )
          (     ...       ) ,  di monic, d1 | d2 | ... | dr.
          (        dr     )
          (          0    )
          (          ...  )
          (             0 )

       Two matrices are similar iff the resulting di's (called the
       invariant factors) are the same.  (Actually, there cannot be
       any zero invariant factors here, since the product of the di's
       is, up to sign, the characteristic polynomial.)

       Example: Let's try it on the A from 2b) above.

             ( 0 1 0 )
       A =   ( 0 0 1 )
             ( 1 1 0 )

                       ( lambda 1           0 )
       A - lambda I =  ( 0      lambda      1 )
                       ( 1      1      lambda )

         ------------>
         [swap rows]
                       ( 1      1      lambda )
                       ( 0      lambda      1 )
                       ( lambda 1           0 )

         ------------>
         [row 3 += lambda * row 1]

                       ( 1      1          lambda )
                       ( 0      lambda          1 )
                       ( 0      1+lambda lambda^2 )

         ------------>
         [column ops]

                       ( 1      0               0 )
                       ( 0      lambda          1 )
                       ( 0      1+lambda lambda^2 )

         ------------>
         [swap columns]

                       ( 1      0                0 )
                       ( 0      1           lambda )
                       ( 0      lambda^2  1+lambda )

         ------------>
         [row 3 += lambda^2 * row 1]

                       ( 1      0                0   )
                       ( 0      1           lambda   )
                       ( 0      0  1+lambda+lambda^2 )

         ------------>
         [column ops]
                       ( 1      0                0   )
                       ( 0      1                0   )
                       ( 0      0  1+lambda+lambda^2 )

        Since the resulting matrix is diagonal, if we interchange
        the row and column operations, we can put A^T - lambda I
        in the same form.  So A and A^T are similar.

     The same argument works for nxn matrices.

     Corollary: The linear maps x --> xA and x --> xA^T are conjugate.
     So they have isomorphic iteration graphs.

     Applications:

     1) Golomb's eldest bit rule: The linear feedback shift register

             [ ] <--- [ ] <--- [ ]<-- ... <--- [ ] <---|
              |        |        |               |      |
              c0       c1      c2     ...     c{n-1}   |
              |        |        |               |      |
              v        v        v               v      |
             [------------------------------------]    |
             [             + mod 2                ] ----
             [------------------------------------]

        is reversible (this means the iteration graph consists only
        of cycles) iff c0 != 0.  That is, iff the eldest bit
        is used with a nonzero coefficient.

        To prove this, consider the ring R of polynomials modulo

            f(X) = X^n - ... - c1 X - c0.
       
        Multiplication by X (what the corresponding Galois shift register
        does) is invertible iff X is invertible in R, iff gcd(X,f) = 1.
        This happens iff f has a nonzero constant term.

      2) Maximum period criterion: A {Fibonacci, Galois} shift register
         with characteristic polynomial f has a cycle of 2^n-1 nonzero
         states iff f is primitive.  

         (Recall that f is primitive if X has order 2^n-1 in the ring R
          defined above.)

        Testing for primitivity is hard, since we have to know the
        factorization of 2^n-1.  There is one case in which it is
        easy: if 2^p - 1 and p are both prime, then a degree p polynomial
        is primitive (over GF(2)) iff it is irreducible.  Furthermore,
        irreducibility of f can be tested by seeing if X^{2^p - 1} is
        1 modulo f.  

        Note: 2^p-1 is prime only if p is prime.  Such primes 2^p - 1
        are called Mersenne primes.  For the Mersenne Twister pseudo-random
        generator, 2^19937 - 1 is indeed prime.  (The Maple prime test
        confirmed this in 16 seconds on my workstation.)

        Example: f = X^3 + X + 1.  Then, mod f

                    X^3 = X+1
                    X^6 = X^2+1
                    X^7 = X^3 + X = (X+1) + X = 1.

                 So the order of X mod 7 divides 7, but it's not 1,
                 therefore it is 7.  

                 This means that the sequence X,  X^2 ,..., X^7
                 contains 2^3-1 distinct elements, all prime to f,
                 so GF(2)[X]/(f(X)) has no zero divisors, making
                 f irreducible.

Lecture 32
4/7/17 Friday

    Today: Predicting linear congruential generators

           J. Plumstead, Proc. IEEE FOCS 1982
           J. Boyar, J. ACM, 1989

    Stream ciphers

      Suppose we have plaintext p0, p1, p2, ...   in Zm
      and pseudo-random keys    k0, k1, k2, ...

      We can form ciphertext    c0, c1, c2, ...     ci = pi + ki (mod m)

      If this is sent to a receiver who can reproduce k0, k1, ...
      then the key can be removed by subtraction, to reveal
      the plaintext.

      The traditional solution method (for an unauthorized recipient)
      is to guess some piece of plaintext, which may as well be
      p0, p1, ..., pt.  Then, ki = ci-pi, i=0,...,t.  If the rest
      of the pseudo-random stream can be predicted from this short
      segment, all of the original plaintext can be read.

   Multiplicative congruential generators:

            x_i = (ax_{i-1} + b) mod m

      We study the problem of determining a,b,m from the sequence
      x0, x1, x2, ... .  Observe that:

        We cannot know m with certainty from a finite number of
        observations.  If we observe 0,1,2,...,k then all we know
        is that m>k.  

        The triple (a,b,c) may not be uniquely determined by the data.
        Consider 

               x_i = 2 x_{i-1} + 2   (mod 8)
               x_i = 6 x_{i-1} + 2   (mod 8)

        If we restrict to even numbers, these two maps have the same 
        iteration graph: A cycle (0 2 6) and an edge pointing from
        4 to 2.

  Prediction

    Inference method: This produces a sequence of triples (a,b,m).
    Each triple is consistent with the data so far.  When an inconsistency
    appears, the method produces a new tuple.  In Boyar's method,
    the coefficients a,b are determined once and for all, and then
    the moduli m are updated as needed.

    I'll be explicit in distinguishing between congruence (==) and
    equality in Z (=).

    The prediction method relies on two facts about the commutative ring
    R = Zm:

           R is a principal ideal ring (every ideal has a generator)
           R is Noetherian (no infinite ascending chains of ideals)

    Try proving these as exercises.  In fact, every principal ideal
    ring *is* Noetherian (another exercise for you).

    1) Finding a.

       First eliminate b by differencing:

         y_i = x_i - x_{i-1}
             == (a x_{i-1} + b) - (a x_{i-1} + b)   (mod m)
             == a y_{i-1} (mod m).

       Ideally, gcd(y1,...,yt) = 1.  If so, we can find ui in Z with
       sum_i ui yi = 1.  Then,

         sum_{i=1}^t u_i y_{i+1} == sum_{i=1}^{t} u_i (a y_i )
                                 == a ( sum_{i=1}^t u_i y_i )
                                  = a

       Unfortunately we cannot guarantee this.  The following will handle
       all cases:

         If y1 = 0 set a=1.
         Else:
               Find least t s.t. d = gcd(y_1,...,y_t) | y_{t+1}
               (Note d>0.)
               Find u_i in Z making d = u_1 y_1 + ... + u_t y_t = d.
                   [Here, using principal ideal property.]
               Then
                      a = (1/d) sum_{i=1}^t u_i y_{i+1}  (in Z)

                        ( == a (sum{i=1}^t u_i y_i / d) )

       Theorem 1: After O(log m) observations, we acquire an a^ such
       that: 

             exists an m [ y_i == a^ y_{i-1},  all i >= 2 ]

       Proof.  If y1 = 0, all further yi vanish, so a=1 works.

               If y1 != 0, 1 <= |y1| = |x1 - x0| <= m.
               So we start with d>0 at t=1.  If the gcd is changed,
                  it is divided by at least 2, so there can be
                  <= log_2 m changes before it divides y_{t+1}.

               (This is where we needed the Noetherian property: the
                chain (y1) [ (y1,y2) [ (y1, y2, y3) [ ... must stabilize.) 

               Now, let a^ be the value we found, a the value used
               to generate the sequence.  Then

                 a^ d  = sum_{i=1}^t u_i y_{i+1}    [by what we computed]
                      == sum_{i=1}^t a u_i y_i      [defn of a]
                       = a sum_{i=1}^t u_i y_i
                       = ad                         [defn of d]

               So (a^ - a) d == 0 (m).  But d divides all y_i so

                  (a^ - a) y_i == 0 (m),   i >= 1

                   a^ y_i == a y_i         i>= i.

    2) Finding b.

        Let b = x1 - a x0.

        Theorem 2: If y_{i+1} == a y_i (m) for i=1,...,s, then
                      x_{i+1} == a x_i + b (m) for i=0,...,s-1.

        Proof:   x_{i+1} - (a x_i + b)
              =  x_{i+1} - x_1 - (a x_i + b - x_1)
              =  x_{i+1} - x_1 - a (x_i - x0)               [defn of b]
              = (y_{i+1} + ... + y_2) - a (y_i + ... + y1)  [telescoping sums]
              = y_{i+1} - a y_i + ... + (y2 - a  y_1)
              == 0 (mod m).

    3) Finding a sequence of m's.

       We have determined a,b as above.

       Initial guess: m=0, output 0.
       Update steps: For i>=1:

                         xi = observed value
                         x_i^ = a x_{i-1} + b  [predicted value]
                         if x_i^ != x_i, then 
                              m := gcd(m, (observed - predicted)).
                              output m.

       Theorem 3: This method outputs a sequence m0, m1, ..., mk
       with m0=0, mi | m_{i-1}, mi distinct, and k=O(log m).

       Note: k = O(log m) is a bound on the number of corrections,
       not on the number of observations. 

       This theorem looks obvious, but we do need to bound the 
       coefficients a,b that are found in steps 1)--2).

       Proof of Thm 3: 

             |y_i| = |x_i - x_{i-1}| <= m

             d <= m     [since d | y_{t+1}]

             So for the coeffs from 1), |u_i| <= m [see Knuth, v. 2, 4.5.2]

             Either a=1 or |a| <= |sum_i u_i y_{i+1}| <= t m^2 
                                                      = O(m^2 log m) [Thm 1]

             THen |b| <= m + |a|m = O(m^3 log m).

             At the first correction x_i^ = a x_i + b
                                 so |x_i^| = O(m^3 log m)

             Therefore, m1 = O(m^3 log m).

                        [Note: paper shows <= 2m^2, see p. 158]

             Since the mi's do not increase, the number of corrections
             is O(log m1) = O(log m).

    Note: The notion of "learnability" or "predictability" used here
    dates from a paper by E.M. Gold, Information and Control, v, 10,
    pp. 447-474 (1967).  The idea is that the learner should arrive,
    after a finite time, at a correct model of the data.

Lecture 33
4/10/17 Monday

    Prediction for "linear device" pseudo-random generators

      Suppose our random bits are obtained by iterating x' = xA,
      (x,x' length n vectors in GF()2)^n, A = n x n 0-1 matrix)
      using the linear output function g : {0,1}^n --> {0,1}.

      Then we can predict the bit sequence by solving linear
      equations over GF(2).

      Example (from two lectures ago):  Fibonacci shift register

             [ ] <--- [ ] <--- [ ]<--|
              |        |             |
              v        v             |
             [ + mod 2 ] -------------
  
        Output function copies the first bit.
 
        Full cycle of states is  0 0 1
                                 0 1 0
                                 1 0 1
                                 0 1 1
                                 1 1 1
                                 1 1 0
                                 1 0 0

        We can observe 00101110010111...

        Suppose we knew the overall architecture of the device but 
        not where the feedback taps were.  Replace them by undetermined
        coefficients a,b,c.  Then from the first 6 output bits we get

            [0 0 1] [a]   [0]
            [0 1 0] [b] = [1]
            [1 0 1] [c]   [1]

        The determinant of the matrix is -1, so we can uniquely solve
        this system mod 2 and obtain a=b=1, c=0,

        For Fibonacci shift registers of length n with unknown initial
        state and unknown feedback connections, 2n-1 observations from
        the bit stream are sufficient.  See the discussion of decoding 
        for the BCH error correcting code in W.W. Peterson,
        Error-Correcting Codes, Section 9.4.

        There is a more efficient -- O(n^2) -- algorithm based on 
        continued fractions.  Nowadays it is called by Berlekamp-Massey
        algorithm: see J. Massey, Shift-Register Synthesis and BCH
        Decoding, Proc. IEEE Trans. Information Theory, 1969.

        Here is why continued fractions are relevant.  Suppose
        we view the bit stream  00101110010111... as providing
        coefficients for the formal power series

             phi = Z^2 + Z^4 + Z^5 + Z^6 + Z^9 + Z^12 + ... 

        We know that phi is rational (an element of GF(2)(Z)); indeed,
        it is the ratio of two "short" polynomials:

                        Z^2
             phi = --------------
                   1 + Z^2 + Z^3

        (note: the denominator polynomial is the *reverse* of the 
        device's characteristic polynomial).  So it makes sense that 
        numerator and denominator will be convergents to the 
        continued fraction of phi.

   "Unpredictable" generators

      Reference: Blum & Micali, SIAM J. Computing, 1984

      Let's broaden our notion of pseudo-random generator.  A PRNG
      is a family of algorithms, each of which takes a (short) seed
      in {0,1}^n and produces a (much longer) sequence in {0,1}^m.
      Usually, there is more than one algorithm for each n.
      When the seed is chosen at random, the output sequence should
      "look random".  In particular, it should be difficult to
      predict the entire sequence from a small initial segment.

      Note: For every generator known or proposed for use, prediction
      is easy once we know the seed (and possibly other parameters of
      the algorithm).  Guessing this data is an NP problem, so any
      assertion that a pseudorandom generator is unpredictable has
      consequences for complexity theory.  The best we can hope for,
      assuming current knowledge, is to prove that efficient prediction
      implies that some currently "hard" problem becomes significantly 
      easier.

      Blum & Micali's generator:

        Let p be an odd prime and g a generator for the cyclic group
        Zp*.  Recall that we have a pair of group isomorphisms

                             g^x
                      ----------------->
            Z_{p-1}^+                    Zp*
                      <-----------------
                          log_g(y)

        For convenience, we'll take residue classes in the additive
        group to be {1,...,p-1}.  (This is the system of "clock
        arithmetic" that the new math tried to popularize.)  Then,
        both maps are permutations on S = {1,...,p-1}.

        Defn: Suppose that a is a quadratic residue mod p.  Then we 
        can write a = g^{2s}, with 1 <= s <= (p-1)/2.  The 
        *principal square root* of a is g^s.

        It is essentially (although not precisely) correct to think
        that x is the principal square root of x^2 when the top bit
        of x's discrete logarithm is 0.  

        Let 
                       1, if x is the principal square root of x^2
               b(x) =
                       0, if it isn't.

        Details of the generator.  Choose an odd prime p and generator
        g.  Define m elements of S by the recurrence

                 x0 = seed
                 x_{i+1} = g^{x_i} in Zp*, i=0,...,m.

        This gives m pseudorandom bits, defined by

                 y_1 = b(x_m)
                 y_2 = b(x_{m-1})
                 ...
                 y_m = b(x_1)

        (This can be done because we know the discrete log of every
         x_i in the sequence save the initial one.)

        The authors assume that p is chosen uniformly from
        the k-bit odd primes, and g is a randomly chosen generator.  This
        can be done efficiently using the algorithm of Lecture 19 (3/1/17):
        choose p-1 in factored form, test p for primality, and if so,
        guess random g in Zp* until one can be proved to be a generator,
        using exponentiation.

        Theorem.  Suppose that there is an algorithm A that 
        (for each p,g) infallibly computes b(x) for x in Zp*.
        Then there is a poly-time algorithm to solve g^x = a in Zp*, 
        which uses A as an oracle.

        Proof: The intuitive idea is to peel off bits of x from right
        to left.  Precisely this goes as follows:

              Suppose we need to solve g^x = a, 1 <= x <= p-1.
              If a=g, return x=1.
              If not, test if a is a quadratic residue
                 If (a|p) = +1, let a' be the principal square root of a
                                (computed with the help of A)
                                recursively solve g^y = a', then x = 2y.
                 Else, let a' be the principal square root of ag^{-1}.
                                recursively solve g^y = a', then x = 2y+1.

              Example: p=11, g=2. From the table

                 x = 1 2 3 4  5  6 7 8 9 10
               2^x = 2 4 8 5 10  9 7 3 6  1

              we see that 1,3,6,7,9 are principal square roots of
              1,9,3,5, and 4, respectively.  To solve 2^x = 9:
              (9|11) = +1, and the square roots of 9 are 2,8.  Since
              b(8) = 1, we solve 2^y = 8.  Now (8|11) = -1, so
              we work with 8/2 = 4.  Since (4|11) = +1, and the
              principal square root of 4 is 2, we solve 2^z=2.  This
              gives z=1, y = 2z+1 = 3, x = 2y = 6.

         (In class, I sketched a "left to right" algorithm, but this
          one is better to have later.)
 
         Now we are going to investigate the consequences of having an
         A that only works with high probability.  First, suppose that A is 
         correct with probability >= 1/2 + epsilon, uniformly over the 
         x in Zp*.   If successive invocations of A are independent,
         we can make the error probability as close as we like
         to 1 with a "voting" procedure.  For such applications, the 
         following result from probability theory can be used:

           Lemma (Chernoff-Azuma inequality): Let X1,...,Xn be i.i.d.
           Bernoulli trials where Pr[Xi=1] = p > 1/2.  Let mu = np 
           (the expected number of 1's).  If X denotes the observed 
           number of 1's,

               Pr[X < (1-alpha)mu] <= exp(- np alpha^2 / 2).

         This is a great result because the bound decreases exponentially
         in the number of trials.  We use majority rule: accept the most 
         popular response from A.  We make a wrong decision when X < n/2,
         so by the Lemma, taking

                alpha = 1 - 1/(1 + 2 epsilon) ~ 2 epsilon,

         the probability of an incorrect decision is bounded by
         (approximately)

              exp(-n(1/2 + epsilon) (2 epsilon)^2 / 2 )
            = exp( - n epsilon^2 ) [to first order in epsilon.]

         In computing a discrete logarithm, we will use A at most
         log2(p) times, since each recursive call replaces
         x by a number that is at worst only half as big.  If A is incorrect
         with probability delta, the probability that the run is flawed
         is, by the union bound, at most delta * log2(p), which is less
         than 1/2 for delta <= 1/log2(p).  Taking

             e^{-n epsilon^2} < 1/log2(p)

         and solving for n gives us n > ln log2(p) / epsilon^2, which
         is O(loglog p) since epsilon is fixed.

     In the paper, it is proved that discrete logs can be computed
     efficiently, under the weaker assumption that A is correct on
     >= 1/2 + epsilon of the x's.  Here is a sketch of the main ideas
     of that proof.

         Choose an integer t and call a number in 1..p-1 "small"
         if it is <= [(p-1)/t].  Arbitrary discrete logs reduce to
         O(t) instances of computing small discrete logs, using 
         the following idea:

             For i=0,1,2,...
                   a' = a*g^{-i[(p-1)/t]}
                   try to solve g^y = a', assuming y is small
                   if successful, return x = y + i[(p-1)/t] as the
                       discrete log of a and stop.

         Note: this relies on the ability to check any purported
         discrete log.

         Using A, we can try to compute the principal square root
         of a random quadratic residue r, as follows.  Let u = sqrt(r),
         choosing at random between the two choices.  Let v == -u (p-1).
         Run A on u, and assert that 

                If A says that b(u)=1, then u is the principal square root,
                otherwise v is.

         Since u is a random element of Zp*, the answers will be correct
         with probability >= 1/2 + epsilon.

         The next idea is to compute the principal square root of a quadratic
         residue q with a small index:

                Choose s, 1 <= s <= (p-1)/2, at random.
                Let r = q * g^{2s}.  (A random quadratic residue.)
                Use the above procedure to get a guess u for the principal
                    square root r.
                Return u*g^{-s}.

         If q = g^j with j <= (p-1)/t, the probability that j+s overflows
         into something larger than (p-1) is about 1/t.  Then, the
         probability of error is, by the union bound,

                <= 1/t + 1/2 + epsilon <= 1/2 + 2 epsilon

         provided that t >= epsilon^{-1}.  Using a voting procedure as
         indicated above we can make the probability of correctness
         arbitrarily close to 1.

         Finally, we use the reduction indicated above to compute
         small discrete logs.  It is important that, at every recursive
         call, the discrete log we are seeking is small.

Lecture 34
4/12/17 Wednesday

   New Section: Applications of Algebraic Geometry

   Algebraic Geometry = study of sets defined by polynomial equations
  
      This is (intentionally) a "lowbrow" definition.  

      Why study it?

        It is a major influence on modern number theory, e.g. Fermat's
        last theorem describes the solutions to x^n + y^n = 1
        with x,y in Q.

        The geometric viewpoint gives different insights, and sometimes 
        allows the elimination of special cases.
 
        You already know some of it: High school analytic geometry treats
        polynomial equations in two real variables.  Linear algebra 
        deals with simultaneous degree 1 equations in several variables.

   The Projective Plane

      Consider the hyperbola xy = 1 in R^2.

            [picture]

        As x--> +- oo, the curve gets closer and closer to the line y=0.
        Similarly for y --> +- oo, and x=0.

        By adding new points to our space, we can capture these limiting
           behaviors.
     
      Defn: P2(R) = all nonzero triples (x:y:z), modulo the equivalence
                    relation (x:y:z) ~ (lambda x : lambda y : lambda z) 

         Henceforth we will call R^2 the *affine plane* A2(R).

         A2(R) sits inside P2(R), via the map

              (x,y) |--> (x:y:1).

         To go backwards, we can use (if z != 0)

               (x/z,y/z) <---| (x:y:z)

         In projective coordinates, the equation for the hyperbola is

             (x/z)(y/z) = 1,  that is, xy = z^2 .

             Note that y=0 ==> z=0 and x != 0, coordinates for (1:0:0).
                       x=0 ==> z=0 and y != 0, coordinates for (0:1:0).

             So the limiting behavior is represented by these two new points

             In projective coordinates, the hyperbola becomes connected.

             (Exercise for you: investigate the parabola y=x^2.)

         We can think of P2(R) as being composed of all the lines through
            the origin in R^2.  Each line "projects" onto two antipodal
            points on the sphere, which are identified to make P2.

    Projective Planes from Finite Fields

         Good reference: M. Hall, Combinatorial Theory, Chapter 12.

         P2(GF(q)) has the identical definition, but now the
                   coordinates have to come from the finite field
                   GF(q).

         The number of points in P2(GF(q)) is

          # of nonzero triples     q^3 - 1
          --------------------  =  ------- = q^2 + q + 1.
          # of nonzero lambda's      q-1

         Defn: A *line* in P2 is the solution set to a linear equation

              ax + by + cz = 0,    a,b,c not all 0

           Since multiplying the equation by a nonzero lambda gives
           us the same solutions, there are q^2 + q + 1 lines.

         In the projective plane, points and lines are dual to each
         other.  Any theorem mentioning them has a dual version, in
         which "point" and "line" are interchanged.

         Example: The Fano plane (q=2).

               [picture -- Wikipedia has a nice one]

           There are 7 points and 7 lines
           Each point lies on 3 lines
           Each line has 3 points

     Projective Space

        Pn(GF(q)) = all nonzero n-tuples (x0:x1:...:xn)
                    modulo (x0:...:xn) ~ (lambda x0:...: lambda xn),
                                         lambda != 0.

        This can be done over any field, but here we take the xi's
        to be in the finite field GF(q).

        Pn(GF(q)) has q^n + q^{n-1} + ... + q + 1 points.

           Alternate proof: Wolog each point has a representation
           in which the first nonzero coordinate is 1.  The first term
           counts the number of ways to fill in (1:*:...:*), 
           the second is for (0:1:*:...:*), and so on.

        Hyperplane: The solution set of sum_i a_i x_i = 0, where not
        all the a_i are 0.  As in P2, the solution set is unchanged
        if we multiply each a_i by the same nonzero constant.

        In Pn, the duality is between points and hyperplanes.

     Blakley' Secret Sharing Method (1979)

        We would like to divide a piece of information between n parties,
        so that:

           Any k of them can recover it, uniquely.
           Less than k of them cannot.

        This is a common "access control" requirement.  E.g. a bank
        safe deposit box needs two keys: one from the customer and
        one from the bank.  (This is n=k=2.)

        The Method

           Choose a sufficiently large prime power q.

           Secret: A random point P in Pk(GF(q))

           Share: A hyperplane H containing P.
       
           If the hyperplanes H1,...,Hk are in general position 
           (explained below), P can be recovered by solving the 
           k x (k+1) linear system that says P is on Hi, i=1,...,k.

        What is General Position?

           As special cases are the curse of all geometry (Dieudonne'),
           a typical geometric theorem assumes that the data are
           in "general position."  The idea is that there are no
           special relationships among the data that would spoil
           the truth of the theorem.

           Example: In the Euclidean plane, two lines in general position
           will meet in one point.  (The exceptions are identical lines,
           and two parallel lines.)

           Here, we decree that general position is exactly that 
           which makes the above result true.
           
           Defn: Let H1,...,Hk be k hyperplanes, i.e. Hi is given by

              a^(i)_0 x_0 + ... + a^(i)_k x_k = 0.

              They are in general position if the matrix

                 [a^(1)_0 ... a^(1)_k]
                 [a^(2)_0 ... a^(2)_k]
                 [        ...        ]
                 [a^(k)_0 ... a^(k)_k]

               has rank k.
             
        Share Creation

          Wolog P is (1:x1:...:xn).

          The hyperplanes with coefficient vectors

                 v1 = (x1: -1 :  0 : ... : 0 )
                 v2 = (x2:  0 : -1 : ... : 0 )
                             ...
                 vk = (xk:  0 :  0 : ... : -1)

          are combined (using random multipliers t1,...,tk from GF(q))
          to form a share.

          Rather than check the general position requirement, we
          will simply make q large enough that failure is unlikely.

        A Bound on Error Probability

          Consider the k x (k+1) matrix

                 [a^(1)_0 a^(1)_1 ... a^(1)_k]
                 [a^(2)_0 a^(2)_1 ... a^(2)_k]
                 [        ...                ]
                 [a^(k)_0 a^(2)_1 ... a^(k)_k]

          It will have rank k if the submatrix (delete the first column)

                 [a^(1)_0 a^(1)_1 ... a^(1)_k]
                 [a^(2)_0 a^(2)_1 ... a^(2)_k]
                 [        ...                ]
                 [a^(k)_0 a^(2)_1 ... a^(k)_k]

          has nonzero determinant.  The number of such matrices is

                 (q^n - 1)(q^n - q) ... (q^n - q^{n-1})

          Dividing by the total number of matrices, q^{n^2}, we get

            Pr[nonsingular] = (1 - 1/q)(1 - 1/q^2) ... (1 - 1/q^n)

                            >= product_{i >= 1} (1 - 1/q^i).

            Euler's pentagonal number theorem gives an explicit formula
            for this product, as multiplied out.  Using this,

                  product_{i >= 1} (1 - 1/q^i)

                  = 1 - 1/q - 1/q^2 + 1/q^4 + 1/q^5 - ...

            where all coefficients are 0 or +- 1.  Therefore

                 Pr[singular] <= 1/q + 1/q^2 + 1/q^3 + ...
                               = 1/(q-1)
                               <= 2/q.

            If we make n shares, the probability that some k of them
            will not determine the secret is (by the union bound)

                 <= 2 (n choose k) / q

                 <= 2^{n+1} / q

            We can make this less than epsilon by choosing

                 log_2(q) > n+1 - log_2(epsilon^{-1}).

        Conspiracies are Futile

            Suppose we have fewer than k shares, say k-1.
            Then the linear system for secret recovery has
            rank <= k-1, so there will be >= q+1 possible solutions.
            (That is, the set of secrets consistent with the data
            is a line or something even larger.)

            Thus, the probability of guessing the secret from k-1
            shares is 1/(q+1).

            This can be made as small as desired.

            Observe that the secret sharing method is not perfect:
            a small amount of information about the secret is
            leaked to a conspiracy with < k members.

Lecture 35
4/14/17 Friday

  Topic du jour: Algebraic curves

  References: Miles Reid, Undergraduate Algebraic Geometry
              Silverman and Tate, Rational Points on Elliptic Curves
              William Fulton, Algebraic Curves (more advanced)

  Let F be a field.   Recall A2(F) = F x F
                             P2(F) = nonzero triples in FxFxF, with
                                     nonzero multiples identified.

  Naively, we think of a curve in A2 or P2 as the solution set to a 
  single polynomial equation.  This has some problems.

        1) x^2 - y^2 = 0, in R^2  (= A2(R)).

           Since x^2 - y^2 = (x-y)(x+y), the zero locus consists
           of two intersecting lines.

                   [picture in x,y plane]

        2) x^2 + y^2 = 0, in R^2.

           The only solution is x=y=0, which is a point.  

           We can get a better picture by considering solutions in C^2.
           With complex coefficients, there is a factorization

                x^2 + y^2 = (x + iy)(x - iy).

           If we add a third axis (labelled iy) we can draw the solutions
           that have x=0.  This part looks just like the solutions to 1).

                [picture in x,y,iy space]

   This motivates the following.

      Defn: An algebraic curve in {A2, P2} is the solution set 
            to a single absolutely irreducible {--, homogeneous}
            polynomial equation.

      What do the new terms mean:

           A polynomial with coefficients in F is absolutely irreducible 
           if you can't factor it over any larger field F'.  For example,
           x^2 + y^2 is irreducible over R but not absolutely irreducible.

           A polynomial is homogeneous if, when fully expanded, all of
           its monomial terms have the same degree.  

              Example: y - x^2 is not homogeneous, but yz = x^2 is.

              We need this so that (x:y:z) will lie on a curve in P2 iff
              (lambda x : lambda y : lambda z) (lambda != 0) does.
              That is, the property of incidence should not depend on
              how we represent points.

      Examples of curves:

          1) Lines ax + by + cz = 0, in P2.  If z != 0, this
             equation is the same as

                   a(x/z) + b(y/z) + c = 0,

             i.e. 
                     au + bv + c = 0

             as usual in the affine plane.

          2) Conic sections, e.g.  xy = 1 (in A2) or xy=z^2 (in P2).  

          3) The semicubical parabola y^2 = x^3 (in A2).

                   [picture]

             Observe that the curve has a "kink" at the origin.
             On the other hand, if I take any other point P, say (1,1),
             the neigborhood of P on the curve looks like a nice open
             interval.

             We say that this curve has a *singularity* at (0,0).

      Detecting singularities algebraically

         If f = y^2 - x^3, then

           partial f / partial x = 2x
           partial f / partial y = -3y^2.

         At the origin, this becomes (0,0).  Therefore, the space
         of normal vectors

           { (a,b) : a (partial f/partial x) + b(partial f/partial y) = 0 }

         is two-dimensional.  Also, if T is the tangent space,

           T = N^\perp = 0.

         When the Jacobian matrix 

                J = (partial f / partial x, partial f / partial y)

         has rank 1, none of these bad things can happen.

         Defn: The curve given by f=0 in {A2, P2} is nonsingular if
         its Jacobian matrix {(partial f/partial x, partial f/partial y),
         (partial f/partial x, partial f/partial y, partial f/ partial z)}
         is never 0 along the curve.

   Elliptic curves

     Defn: An elliptic curve (in P2(F)) is a nonsingular degree 3 curve,
     together with a distinguished point.

     Example: Consider y^2 = x(x^2 - 1) in A2(R).

          The curve is symmetric under flipping the sign of y.
          As x --> +oo, y --> +- oo. 
          If x<-1 there are no solutions.
          If 0 < x < 1, there are no solutions.
          y = 0 when x= 0, +- 1
          Differentiating both sides of the equation, we get

             2 y (dy/dx) = 3x^2 - 1,

          so there is a horizontal tangent with x = - 1/sqrt(3) = - 0.577...
          Since dx/dy = 2y/(3x^2 + 1), the x-intercepts have vertical
             tangents.

          This is enough information to draw the picture:

              [picture]

      We can extend to P2 by homogenizing:

         (y/z)^2 = (x/z)((x/z)^2 - 1) ==> yz^2 = x^3 - xz^2.

      The affine equation captures all the points with z != 0, so
      we plug in z=0:

              y*0 = x^3,

      so x = 0.  This forces y != 0, which we may as well take to be
      1 (since the other two coordinates are 0).  We will make the
      "point at infinity" (0:1:0) the distiguished point, thinking of
      it as infinitely far up (and infinitely far down!) the y axis.

      In P2(R), this curve has two loops.

     Over C (that is, in P2(C)), the curve is, topologically, a torus:

              [picture]

       We can think of the two loops as coming from an intersection
       with the (2-dimensional) plane RxR.

   What are elliptic curves doing in our course???

     An elliptic curve has an algebraically defined group operation.
     (This group is abelian).  This has been exploited for factoring,
     prime testing, cryptography, ... .  So if you learn elliptic
     curves you are in a position to understand many algorithms.

   Some intuition for the group law.

     Let L = Z[i] (Gaussian integers) = { a+bi : a,b in Z }.

     L is a lattice and a subgroup of (C,+).  (Remember that addition
     of complex numbers is just vector addition.)

     The quotient group results from taking the unit square [0,1]^2 and
     gluing together the horizontal edges, and doing the same for the
     vertical edges:

                      ------/----->
                      ^           ^
                      |           |
                      |           |
                     //          //
                      |           |
                      |           |
                      ------/----->

      So it makes sense that there should be a group law on the torus,
      hence a group law on an elliptic curve.

      The miracle is that this can be defined totally algebraically,
      using only the equation for the elliptic curve.  More about this 
      next time.

Lecture 36
4/17/17 Monday

   Tools for counting points on curves over finite fields (Weil, Bezout)
   Group law on an elliptic curve

   Review: Curves in P2, lines (degree 1), conic sections (degree 2),
           elliptic curves (degree 3 + nonsingular).

   Although natural, it turns out that the degree is not a very good
   basis for the classification of curves.  One problem is that it
   depends on the embedding (this is especially a problem for space
   curves).  [Example?]   In algebraic geometry (and other geometric
   theories, like topology), the *genus* is used.

   Defn (for this course) If C is a nonsingular curve in P2, defined
   by an absolutely irreducible equation of degree d, its genus is
   given by

             g = (d-1 choose 2).

       Thus lines and conic sections have genus 0
            elliptic curves have genus 1

       Over C, the genus is the number of "holes" that the Riemann
       surface (the curve in P2(C)) has.

             g=0: sphere, no holes
             g=1: a torus, one hole. 

       (Inconveniently, the object that a topologist calls a surface
       is called a curve here.  It's important to remember that they
       are the same thing!)

   Theorem (A. Weil, 1950s):  If C is a curve in P2 defined by an equation
   over F = GF(q), with genus g and Nq points with coordinates in Fq,
   we have

             | Nq - (q+1) | <= 2g sqrt(q).

      Examples:

          A line ax + by + cz = 0 has (q^2-1)/(q-1) = q+1 points.

          A conic section also has q+1 points.  

            This can be proved using a rational parameterization, 
            similar to what we did in analyzing Cipolla's algorithm.
           (Lecture 10, 2/8/17.)

   Corollary (Hasse, 1930s): The number Nq of Fq-points on an elliptic
                             curve satisfies

                 q+1 - 2 sqrt(q) <= Nq <= (q+1) + 2 sqrt(q).            

   A final bit of classical curve theory:

   Bezout's theorem (1700s): In P2, two different curves of degrees d1, d2
   have d1 d2 points of intersection, if counted properly.

     "Counted properly" means:

         1. The coordinates of a point may lie in an extension field.

         2. Multiplicity must be taken into account.

     Here's an example for 2.

           Line L = {y=0}, Parabola P = {y = x^2}.

           The origin (0,0) is a point of tangency, and at the origin,

              d/dx     (x^2) = 2x = 0
              d^2/dx^2 (x^2) = 2  != 0.

           So we say that this is an intersection of multiplicity 2.

           [Question: Is there a procedure for computing this multiplicity?
            Yes. You can use linear algebra.  See Fulton, p. 75, Thm. 3.]

    Cor: Over any field, the number of common points is <= d1 d2.

    Cor: If f(x,y,z) is a homogeneous polynomial and the system

                        f =  0
                        partial f / partial x = 0
                        partial f / partial y = 0
                        partial f / partial z = 0
   
    has no common solutions, then f is absolutely irreducible.

      Idea of the proof: If f = gh, then the curves for g=0 and h=0
      would have an intersection point, which would satisfy the equations
      for a singularity.  If f = g^m, then g divides all the partial
      derivatives, by the chain rule.

    Elliptic Curves in P2

      Theorem: If q = p^k, p != 2,3, any elliptic curve can be put
      into Weierstrass form:

               y = x^3 + ax + b   (for the points in A2(GF(q))
               plus  O = (0:1:0).
                
      Example: the curve we studied last time, y^2 = x^3 - x.

      Let's look for singularities of a Weierstrass form curve:

                            f = y^2 - (x^3 + ax + b)
        partial f / partial x = -(3x^2 + a) 
        partial f / partial y = 2y

      These can only be 0 simultaneously when x is a double root of
      the cubic polynomial x^3 + ax + b.  The classical test for
      this is the *discriminant* D, defined by

             D = -(4a^3 + 27b^2).

      More precisely, there are no double roots iff D != 0.  Note that
      this test only involves the coefficients for the equation.  We
      do not have to hunt for special points with y=0.

            Mnemonic for D: it is (up to sign) 2^3 a^2 + 3^2 b^3
                            note that the exponents sum to 5 in both terms.

      We also need to investigate the behavior of the curve near
      O = (0:1:0).  The homogenization of the Weierstrass equation is

              y z^2 = x^3 + a x z^2 + b z^3 .

      Since y != 0 at O, we can let u = x/y, v = z/y.  Then the
      equation becomes

              g = v  - (u^3 + a u v^2 + b v^3) = 0,
      so 
           partial g / partial v = 1 + ... (higher order terms)

      which does not vanish at u=v=0.  So O = (0:1:0) is not a singularity
      either.

   The Group Structure on an Elliptic Curve

      Theorem: If (E,O) is an elliptic curve in P2, the points
      of E can be made into an abelian group, so that:

            O is the identity element

           P+Q+R = O iff P,Q,R are collinear.

      This is not easy to prove.  The hard thing is to verify
      the associative law.  A proof based on classical geometry
      may be found in Fulton, p. 127.

      Example: C is y^2 = x^3 + 17

               [picture -- over R there is one component.]

               y = 0 at x = - [cube root of 17] = -2.57128... .

               You can check that P = (-2,3) and Q = (-1,4) are points
               on the curve.  

               The line L thru P,Q is given by y = x+5.  [Check this.]
 
               Bezout's theorem guarantees that there is one more point
               R on C n L.  Its x-coordinate has to satisfy

                 (x+5)^2 = x^2 + 10 x + 25 = x^3 + 17,  so

                    x^3 - x^2 - 10 x - 8 = 0.

               We already know two roots of this cubic, and we can
               obtain the third with rational operations (e.g. long
               division!).  So
          
                    x^3 - x^2 - 10 x - 8 = (x+2)(x+1)(x-4) .

               So R = (4,9).

               In the group P + Q + R = 0, i.e. P + Q = -R.

               What's -R?  We want R, -R, O to be collinear, which
               means that -R is just R with the sign of its y-coordinate
               changed.

               Therefore (in the group): 

                        (-2,3) + (-1,4) = (4, -9).

       In the next lecture, we will do all of this symbolically,
       and give formulas for the group operation.

       Historical note: according to Paul Hewitt (2005), the "chord and
       tangent" procedure was originally used by Diophantus, on
       conic sections.  He then realized that it would work to get
       a third solution from two solutions to a cubic.


Lecture 37
4/19/17 Monday

  Formulas for the addition law on an elliptic curve

      Let the curve E be given by y^2 = ax + b in affine coordinates.
      The identity for the group law is the "point at infinity" 
      O = (0:1:0)

      We now want to work out symbolically the "chord and tangent" 
      procedure that used last time.  So let's assume we are given 
      two points P,Q on E.  

      Special cases first: If either one is O, their sum is the other one.

                           If P=Q=(x,0), then the tangent line is vertical,
                           and we set P+Q (=2P) = O.

                           If P,Q share an x-coordinate, say P=(x,y1) and
                           Q = (x,y2) with y1 = y2, the secant line is
                           vertical, so we set P+Q = O.

      Next, let P=(x1,y1) and Q=(x2,y2), with x1 != x2.

         Let L be the line through P and Q.  Its equation will be

                y = lambda x + mu,

         which has the slope lambda = (y1 - y2)/(x1-x2).  The y-intercept
         mu is then determined by subtraction: mu = y1 - lambda x1.

      Finally, suppose P=Q=(x1,y1).  Then we must take L to be the
      tangent line to the curve at P.  Its slope is found
      by differentiation:

         y^2 = x^3 + ax + b, so 2y (dy/dx) = 3x^2 + a,

      giving

         lambda = (2 x1^2 + a)/(2 y1)    [this is why we need 2 != 0]

      and mu as before.

      Now, we use L to find a third point R.  If y = lambda x + mu,
      we must have

         (lambda x + mu)^2 = x^3 + ax + b, so

         x^3 - 2 lambda^2 x + [degree <= 1 poly in x] = 0.

      x1 and x2 are two roots for this cubic, so we can find the third
      one:

          x1 + x2 + x3 = 2 lambda^2,

      so     x3 = 2 lambda^2 - x1 - x2,

      We get y3 from the equation of L:  y3 = lambda x3 + mu.

      This gives R = (x3, y3), and then P+Q = (x3, -y3).

      Observe that 

         1) The addition law is symmetric: P+Q = Q+P

         2) We have inverses: -P is obtained by flipping P around the
                              x-axis.
        
         3) We used only rational operations and did *not* have to
            leave the field to get P+Q.

      Unfortunately, the associative law does not seem to have a simple
      proof.
  
  Diffie-Hellman key exchange on elliptic curves

     In an influential paper (IEEE-IT, 1976), Diffie and Hellman
     indicated a procedure whereby two parties could agree on a
     piece of shared information that only they knew.  This offered
     a solution to the "key distribution" problem that plagued
     cryptographers.  This was one of the first methods of public-key
     cryptography, and won them the ACM Turing Award.

     Here's their protocol.

        Public: A group G and a generator g for a large cyclic subgroup
                of G.  Although correctness does not require this, 
                security is enhanced if the order of g is a large prime p.
                We also want the discrete logarithm problem to be
                "hard" for this sub group.

        A: Choose random x, 0 <= x < p.

                ---------  Send g^x to B ------>

                                                 B: choose random y,
                                                    0 <= y < p
                <--------  Send g^y to A -------

        Computes h = (g^y)^x

                                                 Computes h = (g^x)^y

     A and B both have a common group element, and can then use it 
     (via some pre-arranged procedure) to derive a cryptographic
     key for further communication.

     D+H originally proposed that G should be Zp* (p is a large odd
     prime) and g a generator for that cyclic group.  Later authors
     realized that any finite cyclic group could be used.

     Unfortunately, the discrete log problem in Zp* is solvable in
     subexponential time.  (See discussion of Adleman's algorithm
     in Lecture 26, 3/17/17).  Presently, there is no known algorithm
     with similar performance for elliptic curve groups.

     So the idea is to take

           G = point addition gp on an elliptic curve E, defined
               over the finite field GF(q).

           g = point on E whose order is a large prime p (this means
               that p*g = 0 but g != O).

     It is thought that the best methods available to a cryptanalyst
     have complexity about p^{1/2}.

Lecture 38
4/21/17 Friday

  Pollard's p-1 factoring method

     n > 1, odd number to be factored.  Assume that n is not a perfect
     power (we can test this).  Let p be prime with p^k || n, and 
     m = n/p^k.

     Note that there is a group homomorphism from Zn* onto Zp*.  (Reduce
     mod p.)

     Suppose we know a multiple E of p-1.

     Choose x in Zn* at random.  (Try 1 < x < n, if it isn't in Zn*,
                                  we can factor n.)

     Compute x^E mod n. 

       We will have x^E == 1 mod p, and we hope that x^E !== 1 mod m.

       If this is true, then 1 < gcd(x^E - 1, n) < n.

       (Possible problem: What if the E-th power map sends all of Zn* to 1?
        In that case, which we will suspect if x^E == 1 for many such x,
        we can try again with E/2, E/4, etc.  Eventually we will reach
        an exponent for which our hope can be realized with probability
        >= 1/2.  [Reference?])

     Suppose that we suspect that p-1 is B-smooth.  This means that

           p-1 = prod_{l <= B} l^{epsilon_l),

     where l (ell) denotes a prime.  This would imply that

          l ^ {epsilon_l} | p-1 <= n,

     so epsilon_l <= log n / log l.

     We therefore set

           E = prod_{l <= B} l^[ log n / log l ].

     If we evaluate x^E by repeated squaring, the number of multiplications
     is (up to constant factors) bounded by

           log E = sum{l <= B} ( log n / log l ) * log l.

                 = log n * pi(B).

     (It is actually not necessary to form E.  We can make the power
     prime by prime.)

     So, ignoring log n factors, the number of bops to split n with
     the p-1 method is proportional to the smoothness bound B.

     There are generalizations that exploit smoothness of p+1, p^2+1,
     p^2+p+1, etc.  

     The reason this doesn't quickly factor all numbers is that most
     numbers, and very likely most (p-1)'s , are not very smooth.

  Generalization to elliptic curves (Lenstra, Ann. Math. 1987).

     Let n, p, etc. be as before.

     We now replace Zp* by the group of an elliptic curve C mod p.
     Recall that this is given by

           y^2 = x^3 + ax + b  (affine points (x,y))
     plus
           O = (0:1:0)  (point at infinity, the group identity)
     where
            4a^3 + 27b^2 !== 0 (mod p).

     The big advantage to elliptic curves is that the size of C varies
     as we change a and b.  It is known that

           p+1 - 2 sqrt p <= |C| <= p+1 + 2 sqrt p,

     and all values in the interval (called the Hasse interval) are
     possible.  So if we fail to split n, we can simply try another curve.

     We will now work mod n, and use the same addition law formulas as
     before, with the proviso that the operations are in the ring Zn.

     Problem: It is "hard" to find a random point on a given elliptic
     curve.  For example, if we picked a and b, and then x, we would
     have to find y by computing a square root mod n.  We saw earlier
     that square roots mod n is as hard as factoring (Lecture 10,
     2/8/17).

     Lenstra's elegant solution is to pick the point and *then* pick
     the curve.  More precisely, choose x,y,a at random from Zn, and
     then b to satisfy

            y^2 == x^3 + ax + b (mod n).

     We hope that 4a^3 + 27b^2 is not a multiple of n.  If it isn't,
     we test its gcd with n.  If gcd > 1, we can factor n.  If gcd = 1,
     we continue with the algorithm.
     
     The largest possible size for C (mod p) is the top of the 
     Hasse interval:

           p+1 + 2 sqrt p <= 2p   (for large enough p)
                          <= n    (since m > 1)

     As before, we use

           E = prod_{l <= B} l^[ log n / log l ].

     and try to compute E*P, where P = (x,y).  The number of point
     additions and point doublings is, as before,

           log E <= (log n) pi(B).

     At this point, the rational-function nature of the addition law
     becomes crucial.  If n were a prime p, and E a multiple of |C|,
     we'd have E*P = O.  Right before getting O we would have to
     double a point Q or add two points R,S.  The slopes of the tangent
     and secant lines for these operations are given by

                    (y1 - y2)/(x1 - x2)
         lambda = {
                    (x^3 + a)/(2y),

     and they will be vertical, meaning that x1 == x2 (mod p) or
     y == 0 (mod p).  (Remember that p is odd.) 

     Unless we are unlucky, these congruences will *not* hold mod m,
     which means that the group operation will fail, by attempting to
     divide by a non-unit.  At this point we can factor n, unless both
     denominators are 0 in Zn.

     (Algorithms like this are sometimes called pretend-field algorithms:
     you pretend that Zn is a field, and wait for this to be falsified.)

     Run time analysis:

      Let's assume that the size of an elliptic curve mod p constructed
      by this procedure is smooth, with the same probability as a random
      integer of the same length.  Then, ignoring factors of order log n
      or smaller, the expected run time is about

           T = lambda ^ lambda        *     B

               [# of curves to try]       [work per curve]

      where lambda (the length ratio) is (log p)/(log B).  The first
      factor comes from the idea that random numbers near p are B-smooth
      with probability lambda^{(1 + o(1)) lambda}.  (See discussion
      of the Canfield/Erdos/Pomerance theorem in Lecture 23, 3/10/17.)

      We can try to minimize

          log T = lambda log lambda  +  (log p)/lambda.

      Observe that the first term is large for lambda --> oo,
      and the second one is for lambda --> 0.  Let's make them
      equal. (This is a shortcut; minimizing via calculus would
      give essentially the same thing.)  Then,

          lambda^2 log lambda = log p,

      which implies

          lambda ~ sqrt ( (2 + o(1)) log p / loglog p ).

      This gives a value for B, and if we plug back into the 
      formula for the run time, we get

          T = exp( (sqrt 2 + o(1)) log p log log p ).

   This is a nice result because the run time is sensitive to p.
   For this reason, elliptic curve factorization is a good "second"
   method to use, when trial division fails to completely factor n.

   It is an open problem to make this analysis rigorous.  The problem
   is not with the uniformity of curve sizes within the Hasse interval
   -- which is "true enough" -- but with the difficulty in proving
   that this interval contain enough smooth numbers.  You can get an idea
   of this difficulty by noting that analytic number theory still cannot
   prove that the Hasse interval contains even one prime.

Lecture 39
4/24/17 Monday

   Bilinear Maps, Pairings, and Applications

   Bilinear maps.

     Let G1, G1', G2 be abelian groups (written additively).  A function
     phi : G1 x G1' ---> G2 is *bilinear* if

           phi(x+y,z) = phi(x,z) + phi(y,z)
     and
           phi(x,y+z) = phi(x,y) + phi(x,z)

     (This is a kind of distributive law for the "operation" phi.)

     Example 1: G1 = G1' = G2 = additive group of a ring R.
                phi(x,y) = xy.

     Example 2: n = pq (RSA modulus, so p<q are odd primes)

                Then Zn* ~~ Zp* x Zq*.

                Let G1 = Zp*, G1' = Zq*, G2 = {+-1} (we're using
                                                    multiplicative notation
                                                    for this one now.)

                (x,y) ---> (x|p)(y|p) [Jacobi symbol] is bilinear.

     Example 3: G2 as in last example.  Consider the elliptic curve
                E given in affine coordinates by

                  y^2 = x^3 - x  (= x(x-1)(x+1)).

                Then O = (0:1:0) is the point at infinity and the 
                identity element for the group law.

                Defn: P is a division point of order N if NP = O,
                that is, if its order (in the sense of group theory)
                divides N.

                E[N] = subgp of E consisting of all order N division points
                (usually the field is extended when you do this).

                For the curve above, E[2] = {O, P, Q, R}.

                   On E[2], the addition law says that the sum of any
                   two points from {P, Q, R} equals the third one.

                   So the addition table for E[2] is

                                   O   P   Q   R

                              O    O   P   Q   R

                              P    P   O   R   Q

                              Q    Q   R   O   P

                              Q    R   Q   P   O

                   Inspection of this tells us that E[2] ~~ C2 x C2,
                   the Klein four-group.

                So there is a bilinear map from E[2] x E[2] to {+- 1}.

                The interesting thing is that we can compute it 
                algebraically, without reference to details of the
                addition table.  Define three polynomials in x:

                    f_P = x+1   (vanishes at P)
                    f_Q = x     (vanishes at Q)
                    f_R = x-1   (vanishes at R)

                Then, for X,Y in E[2] we define

                               1, if X=Y or one of them equals O 
                    e2(X,Y) = 
                               f_X(Y)/f_Y(X).

                The idea behind this is that we are making two affine
                points "interact" by plugging the x-coordinate of one
                into a function that represents the other one.

                Then, we have

                  e2(P,Q) = (0+1)/(-1)     = -1,    e2(Q,P) = -1
                  e2(Q,R) = (1)/(0-1)      = -1,    e2(R,Q) = -1
                  e2(R,P) = (-1-1)/(1+1))  = -1,    e2(P,R) = -1

                (We don't have to do any calculations for the second
                column, since interchanging X and Y in the definition
                interchanges numerator and denominator.)

                As a consequence of our computations we have

                  1) e2(X,Y) is an N-th root of unity (for N=2);

                  2) e2(X,Y) = e2(Y,X)^{-1}  (skew-symmetry)

                  3) e2(X+Y,Z) = e2(X,Z) * e2(Y,Z), same on other side
                     (bilinearity)

                  4) for any X of exact order N (=2 here), there 
                     is a Y s.t.  e2(X,Y) != 1  (non-degeneracy)

        Theorem (Weil, 1940): For any EC E, integer N >= 1 (prime
        to the characteristic of the field over which E is defined),
        there is a map eN : E[N] x E[N] ---> mu_N [N-th roots of unity]
        satisfying 1) -- 4) (with N in place of 2).

        This map is now called the Weil pairing.   (Note: it is not
        uniquely defined by 1)--4), since we could always use
        in its place eN*(X,Y) = eN(X,Y)^{-1}.)

     Why are bilinear maps like this called pairings?

        This is probably the original example.  Let V = R^n, and
        V* = {all linear maps from V to R} (the dual space).
        The inner product <x,y> is bilinear.  It can be shown that
        any lambda in V* can be represented *uniquely* as
        lambda(x) = <a,x>  (call this lambda_a(x)).  So the inner product
        has paired up vectors and dual vectors:

              ----------------- a -----------------    V
                                |
                                |
              ------------- lambda_ a -------------    V*

    It's interesting and important that the Weil pairing can be computed
    algebraically, using only the equation for the curve.  In 1986,
    V. Miller designed an efficient algorithm to do this, for curves
    over finite fields.  (This was an unpublished underground classic
    for years, but was finally updated and published in J. Cryptology.)

    Tripartite Key Agreement (A. Joux, ANTS-IV, 2000):

      A,B,C want to agree on a secret piece of information that only
      they know.  This will then be used to derive a common secret
      session key.

         There is a two round protocol for this using the Diffie-Hellman
         protocol several times.  (Exercise.)

         Using the Weil pairing, we can do the following, with one
         one send/receive round:

            Public: A curve E and a finite field GF(q) for which 
            E[N] [ E(GF(q)).  Two points G,H of large prime order, q.
            It is required that eN(G,H) != 1.

            A: sends aG, aH to B

               receives bG, bH from B
               receives cG, cH from C

               computes (bG,cH)^a = (G,H)^{abc}.

            (the protocols for B and C are similar.)

            Since multiplication of exponents is associative and
            commutative, all parties are can now compute the
            the common informaton (bG,cH)^a = (G,H)^{abc}.

        [Note: In class I presented a version in which G=H.  Using
        this with the Weil pairing requires "distortion maps," which
        we have not yet covered.]

Lecture 40
4/26/17 Wednesday

  Today: Functions and divisors on elliptic curves
  
  Reference: V. Miller, Short programs for functions on curves,
             manuscript, 1986.

  Review: Elliptic curves, division points E[N], Weil pairing
          (bilinear map E[N] x E[N] --> mu_N).

          Our ultimate goal is to define this pairing and show how it
          can be efficiently computed.  This requires more background
          on algebraic curves.

  Functions on Curves

    C = curve in P2(F) (nonsingular).  For us, that means a line,
    a conic section, or an elliptic curve.

    Each curve has a function field F(C).  These are the maps from
    C to P1 that are given in local coordinates by rational functions.

       Example: C = P1.  The affine line A1 sits inside P1, and on A1
       we can use x as a local coordinate:

                    ...------------0-------x---------... 

       Rational functions are ratios of polyomials, say p(x)/q(x).
 
       Around the point (1:0) we can use the local coordinate z = 1/x.
       Then,

                  p(z)/q(z) = p(1/x) / q(1/x) = r(x)/s(x)  (say).

       So F(P1) = F(x), the quotient field of the polynomial ring F[x].

   Let P be a point on the curve C, f in F(C).  We define the
   *vanishing order* of f at P:

                   n>0 if f has zero of order n at P             
         v_P(f) =  0   if f(P) != 0, oo.
                  -n<0 if f has pole of order n at P             
                       (i.e. 1/f has a zero of that order)

       Example: On A^2, f(x) = x^3 has a triple zero at x=0,
       so v_0(x^3) = 3.

       It's not obvious how to compute the vanishing order, so I'll
       illustrate with an example.

            C = parabola y=x^2
            f = x-y,  so f=0 when x=y.

                 [draw picture]

            At P = (0,0), x is a "good" local coordinate (meaning
            that f is non-constant to first order, when you vary
            x on the curve).  [Note: in precise technical algebraic 
            geometry, good local coordinates are called *uniformizers*.
            Fulton, p. 70, Thm. 1 gives a recipe for choosing one.]

            x-y = x - x^2 = x(1-x), so v_P(f) = 1.

            Similarly:

              At Q = (1,1), v_Q(f) = 1

              At R = (0:1:0), v_R(f) = -2  (In projective coordinates
              the parabola is yz = x^2, let r = x,y, s = z/y, and
              take r as the local coordinate.)

              You can do these as exercises.

   Defn: A *divisor* on C is a finite formal sum, using integer coefficients,
         of points on the curve.  Thus it is sum_P a_p P, where only a 
         finite number of the integers a_P are 0.  We call sum_P a_p the
         *degree* of the divisor.

         Note: This is basically the topologist's 0-cycle.

   Thm: Any nonzero function on C has only finitely many zeroes and poles.

      [Proof: Use Bezout's theorem.  Locally f is p(x,y)/q(x,y),
       there can be only a finite number of solutions on the curve
       to p(x,y) = 0.  Similarly for q.]

   Defn: The divisor of the function f is (f) = sum_P nu_P(f) * P.

      Intuitively, you can think of the divisor as telling you where
      the "interesting" points for f are, with some information about what
      type of point they are.

   Theorem: deg( (f) ) = 0, that is, f has the same number of zeroes
   as it has poles.  (Counted with multiplicity, of course.)

      For the example above, this gives us a shortcut.  We can determine
      that the only possible pole for f is at R, and since we already
      know two simple zeroes (the only ones), it must be the case that 
      v_R(f) = -2.

   Warning. Not every degree 0 divisor comes from a function.  To measure
   how bad this problem is, we make up a group.

   Defn: The Picard group of C is

        {degree 0 divisors}
      -----------------------   <-- denoted Pic(C)
      {divisors of functions}

      You should think of Pic(C) as a measure of the curve C's
      "complexity."  Example: Pic(P1) = 0, since every collection
      of d zeroes and d poles comes from a function.

      (Note: This kind of thing, where if you can't solve a problem,
      you at least make an algebraic structure out of the obstruction,
      is called "cohomology."  I believe it dates from late 19th century
      attempts to find global integrals for differential forms.)

   Equivalence in Pic(C) is called *linear equivalence.*

      D1 ~ D2 [linear equivalence] means that for some function f,
                                   D1 + (f) = D2.

      Exercise: Check that this is reflexive, symmetric, and transitive.

   If C is an elliptic curve, Pic(C) is isomorphic to the group of
   points on the curve.  The isomorphism is given by

             P-O (degree 0 divisor) <-----> P (pt) 

      Let's check that this is consistent with previous definitions.

            [draw elliptic curve picture w/ 3 collinear pts P,Q,R]

                 P+Q+R = 0
            <=>  there exists a line L : ax + by + c = 0, thru P,Q,R 
            <=>  there exists an affine function f = ax + by + c with
                 zeroes P,Q,R and a triple pole at O
             =>  (P+Q) + (R+O) + (R+O) = 0 in Pic(C).

Lecture 41
4/28/17 Friday

  Computing the Weil Pairing

  References: V. Miller loc. cit. and V. Miller, J. Cryptology, 2004

  The last two lectures introduced:

              Rational functions on curves
              Divisors
              Picard group
              Isomorphism w/ group law (on elliptic curves)
              Division points E[N] = {P : NP = 0}

  Thm: If F is big enough, E[N] ~~ ZN (+) ZN   (as additive groups).
       (This should have a reference.)

     Example: Consider y^2 = x(x^2 - a), over F = Zp, with (a|p) = -1.
                 plus  O = (0:1:0)

              An affine point is in E[2] iff its y-coordinate is 0
              So E[2] has two 2-division points: (0,0) and O.

              If we extend the field to F' = GF(p^2), every nonresidue
              becomes a square.  (Exercise for you: figure out why.)
              Then we get two additional 2-division points: (+- sqrt a, 0).

     We will now assume that this is the case.  We also assume that 
     N is prime to the field characteristic (e.g.  F = GF(p^k) with p 
     not a divisor of N).  We will denote our elliptic curve by (C,O).

  Weil Pairing

     Let P be in E[N].  Then N(P - O) = 0 on Pic(C), that is,
     NP - NO is the divisor of a function f_P.  Note that f_P
     is only defined up to multiplication by a nonzero constant.

     Provisional definition:  If Q is another point in E[N], the
     Weil pairing is:

                           f_P(Q - O)          f_P(Q)   f_Q(O)
        e_N(P,Q) = (-1)^N ----------- = (-1)^N ------ * ------
                           f_Q(P - O)          f_Q(P)   f_P(O)

     (We can drop the sign if N is even, but we need it when N is odd
     to make the bilinearity come out right.)

     Recall the example from 4/24/17:

           y^2 = x(x^2 - 1)

           P = (-1,0)
           Q = ( 0,0)
           R = ( 1,0)

                   [draw picture]

           So E[2] = {O, P, Q, R}

           In the affine coordinates x,y 

           f_P = x+1
           f_Q = x
           f_R = x-1

           To evaluate, say, f_P at Q, we just plug in the coordinates
           of Q:

                 f_P(Q) = x+1 @ x=0 = 0+1 = 1.

     Now we have a problem.  According to our definition of rational
     function, f_Q maps C to P1, and f_Q sends O to the point
     at infinity of P1.  This result is not a field element, so
     it is not clear what it means to divide two of them.

     The solution is to work directly with the ratio, which *will*
     give us something in the field.  Note that the function g = f_Q / f_P
     has
               v_O(g) = v_O(f_Q) - v_O(f_P)
                      = -2  - (-2)
                      = 0.

     So if we plug the coordinates of O into this ratio, we will
     produce a nonzero field element.

     Continuing with the example:

             y^2 = x^3 - x

             y^2 z = x^3 - x z^2   (in projective coordinates)
  
             A neighborhood of O = (0:1:0) is { (x:y:z) : y != 0 }
             so we use u = x/y and v = z/y as coordinates there.
             (Note that the neighborhood is just another copy of A2.)

             Curve eqn becomes v = u^3 - uv^2.

             Also   f_P = x/z + 1 = (x+z)/z = (u+v)/v = 1 + u/v
                    f_Q = x/z               = u/v

             So (locally, around O), f_P/f_Q = (u+v)/u = 1 + v/u

             Since v = u^3 - uv^2 on the curve, another expression for
             this ratio is 

                      f_P/f_Q = (u+v)/u = 1 + (u^3 - uv^2)/u 
                                        = 1 + u^2 - v^2.

             If we plug u=0, v=0 into this we get 1.

    Was this cancellation a magical coincidence?

       No.  We have the freedom to modify our functions f_P and f_Q
       by nonzero constants, so let's assume they have been normalized
       so that the lead coefficients are 1.  That is, we know that
       f_P and f_Q each have a pole of order N at O.  If z is a
       good local coordinate at O, then, for constants a_P, b_P != 0,

            f_P = a_P z^{-N} + ... + a_{-1} z^{-1} + a_0 + ...
       and
            g_P = b_P z^{-N} + ... + b_{-1} z^{-1} + b_0 + ...

       so we normalize so that a_P = b_P = 1.  This guarantees

            f_P(O) / g_P(O) = a_P/b_P = 1/1 = 1. 

       As a consequence of this, we can ignore the second factor
       when we compute the Weil pairing.

   Computing and Evaluating f_P

       Now we need a method that takes P and produces the normalized
       function f_P.

       Since N may be large, we do not write f_P out explicitly.
       Rather, we produce it in factored form (as a product of
       linear polynomials, raised to positive and negative powers).

       Miller's solution is to inductively define functions f_{m,P}
       with
               ( f_{m,P} ) = m P - P* - (m-1)O,               (+)

       where P* = mP in the group of the curve.  All such functions
       will be normalized.

       Note that (as desired)

               ( f_{N,P} ) = N P - P* - (N-1)O,
                           = N P - O - (N-1)O,     [P* = NP = O in the grp]
                           = NP - NO

       Defn: Let P,Q belong to E.  L_{P,Q} is the normalized degree 1 
       function chosen so that L_{P,Q} = 0 is the line thru P,Q.
       (E.g. for the N=2 example above, L_{P,Q} = x.) We take the 
       tangent line when P=Q.

       Let us define
                                   L_{P,Q}
                     g_{P,Q} = ----------------
                                L_{P+Q, -(P+Q)}

       This is a kind of "converter" to handle switching from the point 
       collinear with P,Q to the point that is P+Q in the group of 
       the curve.

       Now we define f_{m,P}, for all m >= 1.

           Base case: f_{1,P} = 1.    [Sanity check: (f_{1,P}) = 0.)
    
           Induction step: f_{m+m',P} = f_{m,P} f_{m',P} g_{mP, m'P}

       In particular, then, we have

           f_{m+1,P} = f_{m,P} g_{mP, P}

           f_{2m,P} = f_{m,P}^2 g_{mP, mP}

       [Exercise: Check that this works, and that eqn. (+) above holds.]

       The last two equalities can be used in an analog of the 
       "square and multiply" exponentiation algorithm.  For example,
       if we wanted f_{10,P} we could compute f_{m,P} for
       m = 1,2,3,5,10.

       In evaluating the Weil pairing, it is not necessary to determine
       the functions f_{m,P} explicitly.  Rather, we can simply
       compute f_{m,P}(Q) using the recurrences.  This will produce
       a field element at the end.  

       Complexity analysis: Using any standard addition chain (e.g. with
       the recurrence in Lecture 6 (1/30/17)), we will determine
       f_P(Q) with O(log N) additions on the elliptic curve and
       other arithmetic operations in F.  When F = GF(q) this is
       O( (log N) (log q)^2 ) bops (using standard arithmetic).
       The same holds for f_Q(P).

       Thus, we can think of a Weil pairing evaluation as being
       similar in complexity to an exponentiation.

   One Final Detail

       What if one of the intermediate functions f_{m,P} vanishes
       at Q?  Then the final result would come out 0, which is
       not acceptable in the Weil pairing context.  Miller 
       proposes two solutions to this problem.

       1) Carry the zero/pole information along symbolically.  That is
       associate with each f_{m,P} its leading behavior at Q, that is,
       alpha_m z^{v_Q(f_{m,P}), alpha != 0.  Here z is a good local
       coordinate for the evaluation point Q.  At the end the powers
       of z will cancel out and we will be left with a nonzero field
       element.

       2) Randomize and make the problem go away.  (At least with
       high probability.)

          Choose a random point S of C, and then

               Replace Q by Q* - S, where Q* = Q+S in the group.

                                  f_P(Q*)
               Replace f_P(Q) by ---------
                                   f_P(S)

               Do the same for P and f_Q.

        Miller shows that when the field size is large, the
        chance of running afoul of a pole or zero for the modified
        evaluation points is small.

        Usually, introducing randomization makes computations faster.
        But here, it seems that 2) involves about twice the work of 1).

Lecture 42
5/1/17 Monday

  Today's subject: Negative and positive applications of the Weil pairing

  References: Menezes, Okamoto, and Vanstone, IEE-IE, 1993
              Boneh and Franklin, SIAM J. Computing, 2003

  Supersingular Elliptic Curves

     Let C be an EC / GF(q), q = p^k.  Assume p != 2,3 

     # of points on C is q+1-t, with |t| <= 2 sqrt(q) [Hasse]

     C is *supersingular* if t == 0 (p).

     The definition looks accidental, but supersingular curves have
     all kinds of special properties.  In particular, they have
     more automorphisms than they "deserve" to have.

     Example:   y = x^3 + a, with a != 0 in GF(q), q == 2 (3).
                The condition on q guarantees that 3 doesn't divide
                q-1, so cubing and cubing followed by a shift are
                permutations of GF(q).  It follows that the number
                of points is

                    1 [for O] + 1 [for y=0] + 2*(q-1)/2 [for y != 0].
                =   q+1.

     Another example (exercise for you): y^2 = x^3 - x.

     The ease of point counting for these examples led to proposals
     like the following for elliptic curve cryptography:

        Let p = 4q - 1, where q is an odd prime.  Then y^2 = x^3 - x
        has E[2] ~~ C2 x C2, as we've seen.  The rest of the curve
        has order (p+1)/4 = q.  So by Cauchy's theorem there has
        to be a point of order q.  It can be found easily by 
        taking a random point != O, and doubling it to eliminate
        the C2 x C2 part.  Since 2 != q, we have a random point from
        the Cq subgroup.

   MOV Reduction

     This reduces a discrete log problem on an elliptic curve to
     a discrete log problem in a finite field.

     Given: C, elliptic curve over F = GF(q), a point P of prime order r,
            and another point Q such that Q = lambda P.  The goal is
            to find the unknown multiplier lambda.

     The Reduction:

       Find the least k > 1 such that r | q^k - 1.  (That is, find
       the order of q in Zr*.) 

       Construct an extension F' / F of degree k.  [This is easy to
       do using randomization.  About 1/r of the monic polynomials
       in F[X] are irreducible (an analog of the prime number theorem).
       Find one by trial; it can be checked using a poly-time irreducibility
       test, e.g. Butler's algorithm [Quart. J. Math Oxford, 1954].)

       Note: F' contains a primitive r-th root of unity.  k is called
       the *embedding degree*.

       Find S on C(F') with e_r(P,S) != 1, where e_r is the Weil pairing.
       (Again, this is easy.  The map S --> e_r(P,S) is a homomorphism
       of groups, so the chance of picking S outside its kernel is
       1 - 1/r.)

       Let zeta = e_r(P,S)   [a primitive r-th root of unity]
            eta = e_r(Q,S)   [an r-th root of unity]

       Now, eta = e_r(lambda P, S) = e_r(P,S)^lambda = zeta^lambda.

       This is a discrete log problem in F' which we can then try to
       solve.

     It can be shown (using work of Waterhouse) that if C is supersingular,
     the embedding degree is small.  In particular, k <= 6.

     For the second example given above, k = 2 (if p is sufficiently large).
     It cannot be 1 because q would have to also divide p-1.  But
     then we'd have q dividing p+1 - (p-1) = 2.  k=2 suffices because
     GF(p^2)* is cyclic, of order p^2 - 1 = (p-1)(p+1).

     Renee Lovorn [PhD Diss., U. Georgia, 1992] found a rigorously 
     analyzable version of the Adleman subexponential algorithm for
     discrete logs in GF(p^2).  (One of the complications in doing this
     is the lack of unique factorization in rings of quadratic integers.)

     This has now been generalized to handle all GF(p^k) with k = O(1).
     As a consequence, supersingular curves offer no advantage for
     discrete log-based cryptosystems over the multiplicative groups of
     finite fields.

     However, general curves are still safe: the embedding degree of
     a "general" elliptic curve is likely to be large [REF?]

   ID-Based Public Key Encryption

     Some context: In the real world, RSA-style public key cryptography
     differs from the "libertarian" ideal in requiring a central 
     authority to issue *certificates* (= signed public keys).
     Ultimately, a commonly trusted entity has to vouch for the signatures,
     much as a notary public certifies that a certain person appeared
     before them and signed the document.  They do *not* make any
     claims about the document itself.

     On today's Internet, your ID is your e-mail address.  For example,
     the commonly used password-reset feature of many web sites simply 
     communicates to an e-mail address that was provided to it before.
     So it is a natural idea to try to derive public keys from text
     identifiers.  The question of doing this was raised (but not 
     answered) by Shamir in the 1980s.

     Boneh-Franklin Scheme:

        p is a large prime, p == 2 (3).

        The elliptic curve C: y^2 = x^3 + 1 is supersingular (as shown above).

             This curve has another special property that we will
             exploit.  Let C' be the curve defined over F' = GF(p^2).
             Then, since p^2 == 1 (mod 3), F' contains a primtive cube
             root of unity, zeta.  C' has the additional automorphism
             phi: (x,y) |--> (zeta x, y).

             This is called "complex multiplication," or, more properly
             "complex multiplication by Z[zeta]."

        q is a large prime factor of p+1.  (One way to guarantee this
        is to choose q first, and then test numbers of the form xq-1
        for primality.  One could also use the algorithm of Lecture
        17, 3/17/17.)

        P is an element of exact order q on C.

             Since phi(Q) is not in C unless Q=O, we can define
             a pairing on <P> x <phi(P)>  (here, <P> is the subgroup
             of all multiples of P) using the Weil pairing:

                 e(X,Y) = e_q(X,phi(Y)).

             This is non-degenerate in the sense that e(P,P) != 1.
             Used in this context, a map like phi is sometimes called
             a *distortion map.*

        Private keys are produced by a central authority called the
        Private Key Generator (PKG).   The PKG chooses

            C,p,q,P as above
            [The field GF(p^2) will be implemented using the
             irreducible polynomial Z^2+Z+1, so zeta = Z here.]

        and then makes a private master key s in Zq*.  The
        following are then made public:

            p,q,C,P

            sP, the PKG's public key

            Two hash functions  H1 : {0,1}* --> {pts of exact order q on C}

                                H2 : {qth roots of unity} --> {0,1}^n

        The hash functions are provided as "translators" that convert
        between text -- strings of bits -- from the real world, and
        the ideal algebraic objects of number-theoretic cryptography.

        The cryptographic scheme is designed to provide a "mini" stream
        cipher, in which an n-bit message m is sent as a packet

              (hint, m (+) [pseudo-random mask]).

        Both sender and receiver have to be able to make the mask,
        the latter with assistance from the hint.

        Three protocols are now defined:

          1) [To set up A's public key]

             The PKG takes A's ID in {0,1}*  (e.g. an e-mail address),
             computes Q_ID = H1(ID) and then sends the private key
             s Q_ID to A.  (This can be thought of as a "blinded"
             copy of A's ID.)

          2) Encryption [B does this to send a msg to A]

             Q_ID = H1(ID)
             Choose random r in Zq*  (an exponent)
             gamma = e(Q_ID, sP)  (a root of unity)
             Sends c = (rP, m (+) H2(gamma^r)).

          3) Decryption [what A does to read B's msg]

             Computes H2( e(s Q_ID, rP) ) and uses this as
             a mask to decrypt the message.

       Correctness (A can read the message) follows from 

             gamma^r = e(Q_ID, sP)^r = e(s Q_ID, rP).

       Computation cost: Per message, the sender does two exponentiations
       in groups of order q, and one pairing; the receiver does one
       pairing.  Thus (by the analysis of Miller's algorithm last
       time), the cost for both parties is O(lg p)^3 bops, similar 
       to RSA.

       Security is more problematic.  The embedding degree is low (=2)
       so we cannot rely on the difficulty of discrete logs on C.
       Assuming that the hash functions are randomly chosen functions,
       Boneh and Franklin give a reduction from the "bilinear
       Diffie-Hellman problem" (BDH) to cryptanalysis of their
       scheme.  Thus, if cryptanalysis is easy, so is BDH.

       BDH is the following computational problem.  Given the curve
       (etc.) as above, and four points P, aP, bP, cP of order q,
       compute e(P,P)^abc = e_q( P, phi(P) )^abc.

       BDH is definitely an ad hoc creation that was cooked up just
       for this scheme, so we would do well to wonder how hard it
       really is.  But, in the 15-odd years since this scheme was 
       published, no one has come up with an algorithm for BDH that 
       is substantially better than first computing the discrete 
       logarithms a,b,c.

Lecture 43
5/3/17 Wednesday

  Today's topics (and a couple of references)

       Why are the curves we have studied called elliptic?

           See Ezra Brown, Three Fermat Trails to Elliptic Curves,
           College Math. J., v. 31, 2000, 162-172.

       Elliptic curves and the symbolic integration problem

           J. M. Davenport, On the Integration of Algebraic Functions,
           Springer-Verlag 1979.

  These topics illustrate the point that if you learn about elliptic
  curves, you are in a position to understand a lot of pure and
  applied mathematics.

  The Origin of Elliptic Curves

    Ellipse: (x/a)^2 + (y/b)^2 = 1, where a != b, both > 0.

             We can parameterize this using

                x = a cos theta 
                y = b sin theta

             The arc from theta_1 to theta_2 has length given by

        int_{theta_1}^{theta_2} sqrt(a^2 sin^2 theta + b^2 cos^2 theta) d theta

             and if we add and subtract b^2 sin^2 theta, this simplifies to
             
        const * int_{theta_1}^{theta_2} sqrt(1 - k^2 sin^2 theta) d theta,

             where k^2 = b^2 - a^2.  We will assume k^2 != 0,1. 

    We can put the arc length integral in algebraic form by taking

             u = sin theta, so du = cos theta d theta = sqrt(1-u^2) d theta

    Then we get (dropping the constant out front, and converting the
    limits of integration)

          int_{u_1}^{u_2} sqrt(1-u^2)/sqrt(1-k^2 u^2) du

       =  int_{u_1}^{u_2} (1-u^2)/v du,

    where v abbreviates sqrt( (1-u^2)(1 - k^2 u^2) ).

    If we like, we can think of the integrand (1-u^2)/v as a rational
    function on the curve
                          
              v^2 = (1-u)(1+u)(1 - ku)(1 + ku)

    To put this curve in a more familiar form, divide through by (1-u)^4.
    We get

      (v / (1-u)^2)^2 = (1 + 2u/(1+u)) (1 + (1-k)u/(1+u)) (1 + (1+k)u/(1+u))

    which we can write as 

             y^2 = (1 + 2x)(1 + (1-k)x)(1 + (1+k)x),  

    using y = v/(1-u)^2, x = u/(1-u).  This is an elliptic curve, since the
    cubic in x has distinct roots. (Check this.)

    The change of variables is reversible:

             u = x / (1+x),    v = y/(1+x)^2  [Check.]

    Therefore, the arc length integral has the form

          int [rational fn of x, sqrt(cubic in x)] dx.
                                  ^
                                  |
                                  y

    Integrals like this came to be called elliptic integrals.  (And, by
    extension, the name was also used for integrals like the two preceding
    arc length integrals.)

    One problem with understanding an integral like the above is that
    the integrand is multivalued.  If y^2 = cubic(x), which sign should
    I take when y appears in a formula?

    Riemann's solution to this problem was to think of the integrand
    not as a function of x,  but as a function of the pair P = (x,y),
    where (x,y) is chosen from

         { (x,y) : y^2 = cubic(x) }.

    This set is today's elliptic curve, or more precisely, the affine
    part of one.

    Nowadays, when x,y are complex, then we can think of the set of 
    pairs, plus points at infinity as forming a 2-dimensional manifold 
    called a *Riemann surface*.  The Riemann surface for an elliptic
    curve is, topologically, a torus.

    Another way around the problem is to consider not an elliptic
    integrals, but the inverse of the function

               f(z) = int_a^z R(x,y) dx

    These inverses, called *elliptic functions*, have marvelous 
    properties.  For example, they are doubly periodic.

  Symbolic Integration

    This is a computational problem dating from the earliest days
    of the calculus.

    Suppose we are given omega = phi(x) dx.  We would like to find
    a function f for which df = omega, or prove there is none.

    (As with all great problems, this is horribly vague.  To make it
    precise, we must specify from what class we will draw f.)

    An algorithm to handle

           phi = rational function of x, sqrt(quadratic in x)

    was known by the middle of the 19th century, and is commonly taught
    in calculus courses.

         Note: what about rational functions of x, sqrt(linear fn)?  These 
         are really just rational functions.  If y^2 = ax + b, then 
         2y dy = a dx, so we can just use y as our integration variable.

         A similar remark applies to rational(x,sqrt(quadratic))

    The next level of complexity is a problem where

           phi = rational function of x, sqrt(x^3 + ax + b)

      We'll assume the rational function coefficients are in Q,
      as are a,b.  Also 4a^3 + 27b^2 != 0, to guarantee the cubic
      has distinct zeroes.

    Associated with such a problem is the elliptic curve

           E : y^2 = x^3 + ax + b.

    In some cases, we can treat symbolic integration by ad hoc methods.

        Example: omega = dx / sqrt(x^3 + ax + b) = dx / y.

                 This is a *differential form* on the curve E.
                 (Although there is a precise mathematical definition,
                 we'll just think of differential forms as symbolic
                 objects obeying the laws of calculus.)

                 It can be shown (exercise for you) that omega is
                 defined everywhere on E -- that is, there are no points
                 at which omega "blows up" (has a pole).  As a sanity
                 check consider a point P where y=0.  This looks like 
                 a problem, but the equation of the curve implies

                      2y dy = (3x^2 + 1) dx,

                 so 2/(3x^2 + 1) dy = dx/y.  If y = 0, then 3x^2 + 1,
                 the derivative of the cubic, is not 0.  So omega is
                 defined at P.   

                 Now suppose there were a rational function f(x,y) with
                 df = omega.  By Bezout's theorem, f would have to have
                 a pole someplace, but differentiation only makes poles
                 worse.  (Note that d(z^{-k}) = -k z^{-k-1}.)  So there
                 is no such f.
 
                 (I found this proof sketched in Harris Hancock's book,
                 Elliptic Functions.)

     It can be shown that omega is a generator for the rational differential
     forms on E.  That is, any such is

           eta = R(x,y) dx / y,       R in the field Q(x,y).

     In the early 70s, Risch (Bull. AMS 1970) gave a procedure to decide
     if such a differential form has an elementary antiderivative.
     In this context, "elementary" means a sum of rational functions and 
     logarithms of rational functions.  (It can be shown that nothing
     more is needed.)

     To state Risch's result, we need a definition.

     Defn: Suppose that z is a "good" local coordinate around the point P.
     Then 
          eta = (a_{\nu} z^{mu} + ... + a_{-1} z^{-1} + a0 + a1 z + ...) dz

     The number a_{-1}, which doesn't depend on the choice of local 
     coordinate, is called the *residue* of eta at P.

     It can be shown that sum_P residue_P(eta) = 0.

         In complex analysis, this is a consequence of the Stokes
         theorem.  It can also be proved purely algebraically (not
         a trivial job).

     Theorem (Risch, 1970):

         Let eta = R(x,y) dx/y 

         The residues of eta (on the elliptic curve E) generate a lattice
         L.  (That is, a discrete subgroup in some R^k.)

         Let r1, r2,..., rk be a basis for L, so that

                res_P(eta) = sum_{i=1}^k a_{i,P} r_i,   a_{i,P} in Z.

            [Note: Typically the residues will be in some field of
            algebraic numbers.  We are just expressing each residue
            as an integer linear combination of basic algebraic numbers.]

         Let D_i (a divisor on E) be sum_P a_{i,P} * P.

            [Note: we just collect, for the i-th coordinate, the contribution
            of eta's residues to that coordinate.]

         By the residue-sum theorem (above), D_i has degree 0.

         If eta = df, where f = sum of rational fns on E and logs of same,
         then:

               There are rational functions v_0,v_1,...,v_k
                     and integers               j_1,...,j_k

               such that
                          j_i D_i = (v_i)    (so D_i has finite order in 
                                              Pic(E))
               and

                 eta = d v0 + sum_i r_i/j_i  dv_i / v_i.

               (You should think of dv / v as d(log v).)

     The Risch theorem reduces the symbolic integration problem for
     elliptic functions to: a) computing orders in the Picard group,
     and b) finding rational antiderivatives.
 
     Problem a) boils down to testing the order of a point P
     on an elliptic curve.

     Problem b) is not that hard.  The idea is to consider, for each
     P on the curve, the so-called "principal part" of eta.  (This 
     is the part of the Laurent series that is not defined at P.)
     We may as well assume that all the residues (z^{-1} coefficients
     in the local variable z) are 0.  Then, integrate the principal
     parts locally and see if there is a global function with 
     those principal parts.

     Example (Davenport, p. 164)

        E is y^2 = x^3 - 3x^2 + x + 1 = (x-1)(x^2 - 2x - 1), plus O as usual.

           Roots of the cubic are at 1, 1 +-sqrt(2), numerically
           -0.414, 1, 2.414.

           [draw the picture for E(R)]

        The two affine points with x=0 are labelled P and Q.

        Let eta = x^{-1} dx/y = dx / (x sqrt(cubic))  

        The poles of eta are at P,Q, with residues +- 1.

           (It takes some thought to understand why the signs are
            different.  The reason is that when we go from P to Q,
            we are switching brances of the square root, so it
            flips in sign.)

        So L = Z, and k=1.

        D = P-Q = (P-O) - (Q-O).

        Then (using the isomorphism between Pic(E) and the group of
        points on the curve)

             jD = 0  in Pic(E)
        <==> 
             j(P-Q) = O in the group of E

        The line through P and Q is vertical, so P+Q = O, i.e. -Q = P.
        So this holds iff

             j(R) = O in the group of E,  R = 2P.

        So we need to test whether R has finite order or not.

        In 1977, Mazur proved that if the point X on E(Q) has
        finite order, the order is <= 12.  This gives us a test
        we can implement, since point addition is computable
        (just use the same formulas as we did for curves over
        GF(p)).

        Since R=2P, we can try 

           R, 2R, 3R, 4R, 5R, 6R

        and see that none of them are O.  Therefore, P has infinite
        order.

        Applying the Risch theorem, eta has no integral.  (It would
        have to have logarithmic terms to provide the simple poles at P,Q,
        but, as we have seen, no such terms can exist.)
          
        [Note: We can skip the multiples of R if we use the Lutz-Nagell
        theorem.  This theorem says that if E, given by y^2 = x^3 + ax + b
        with a,b integers, has a division point with rational coordinates
        (x,y), then these coordinates x and y are actually in Z.
        Our curve qualifies since we can shift the origin and write
        the cubic as (x-1)^3 + [Z-linear combo of x-1, 1].  Let P be
        the point with x=0 and y>0.  The tangent line at P is
        y = -x/2 + 1, so the x-coordinate of R (allegedly a point
        of finite order) satisfies 

           x^3 - 3x^2 + x + 1 = (-x/2 + 1)^2

        i.e. 

           x^3 - (11/4) x^2 + ... = 0.

        So the x-coordinate of R is 11/4, not an integer.  Applying
        Lutz-Nagell, R has infinite order in the group of E, and, 
        therefore, so does P and Q.]

     Is This Really an Algorithm?

        If we apply this procedure to an arbitrary differential form
        on E : y^2 = x^3 + ax + b (with rational coefficients), we may
        find that the poles are at points whose coefficients are 
        algebraic numbers.  To handle this case, we would need an
        analog of the Mazur theorem for extension fields of Q.  Such
        a result follows from work of L. Merel (Inv. Math., 1996)
        and P. Parent, (J. Reine & Angewandte Math., 1999), and gives
        a computable bound on the maximum order of any finite-order
        point on E(K), where K/Q has degree d.   The bound only involves
        d.
